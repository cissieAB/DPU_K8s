{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hello, FABRIC: Create your first FABRIC slice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup the Experiment\n",
    "\n",
    "#### Import the FABRIC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7dd3a tr:nth-child(even) {\n",
       "  background: #dbf3ff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7dd3a tr:nth-child(odd) {\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7dd3a caption {\n",
       "  text-align: center;\n",
       "  font-size: 150%;\n",
       "}\n",
       "#T_7dd3a_row0_col0, #T_7dd3a_row0_col1, #T_7dd3a_row1_col0, #T_7dd3a_row1_col1, #T_7dd3a_row2_col0, #T_7dd3a_row2_col1, #T_7dd3a_row3_col0, #T_7dd3a_row3_col1, #T_7dd3a_row4_col0, #T_7dd3a_row4_col1, #T_7dd3a_row5_col0, #T_7dd3a_row5_col1, #T_7dd3a_row6_col0, #T_7dd3a_row6_col1, #T_7dd3a_row7_col0, #T_7dd3a_row7_col1, #T_7dd3a_row8_col0, #T_7dd3a_row8_col1, #T_7dd3a_row9_col0, #T_7dd3a_row9_col1, #T_7dd3a_row10_col0, #T_7dd3a_row10_col1, #T_7dd3a_row11_col0, #T_7dd3a_row11_col1, #T_7dd3a_row12_col0, #T_7dd3a_row12_col1, #T_7dd3a_row13_col0, #T_7dd3a_row13_col1, #T_7dd3a_row14_col0, #T_7dd3a_row14_col1, #T_7dd3a_row15_col0, #T_7dd3a_row15_col1, #T_7dd3a_row16_col0, #T_7dd3a_row16_col1, #T_7dd3a_row17_col0, #T_7dd3a_row17_col1, #T_7dd3a_row18_col0, #T_7dd3a_row18_col1 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7dd3a\">\n",
       "  <caption>FABlib Config</caption>\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row0_col0\" class=\"data row0 col0\" >Log Level</td>\n",
       "      <td id=\"T_7dd3a_row0_col1\" class=\"data row0 col1\" >INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row1_col0\" class=\"data row1 col0\" >Log File</td>\n",
       "      <td id=\"T_7dd3a_row1_col1\" class=\"data row1 col1\" >/tmp/fablib/fablib.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row2_col0\" class=\"data row2 col0\" >Data directory</td>\n",
       "      <td id=\"T_7dd3a_row2_col1\" class=\"data row2 col1\" >/tmp/fablib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row3_col0\" class=\"data row3 col0\" >SSH Command Line</td>\n",
       "      <td id=\"T_7dd3a_row3_col1\" class=\"data row3 col1\" >ssh -i {{ _self_.private_ssh_key_file }} -F /home/vscode/work/fabric_config/ssh_config {{ _self_.username }}@{{ _self_.management_ip }}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row4_col0\" class=\"data row4 col0\" >Orchestrator</td>\n",
       "      <td id=\"T_7dd3a_row4_col1\" class=\"data row4 col1\" >orchestrator.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row5_col0\" class=\"data row5 col0\" >Credential Manager</td>\n",
       "      <td id=\"T_7dd3a_row5_col1\" class=\"data row5 col1\" >cm.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row6_col0\" class=\"data row6 col0\" >Core API</td>\n",
       "      <td id=\"T_7dd3a_row6_col1\" class=\"data row6 col1\" >uis.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row7_col0\" class=\"data row7 col0\" >Artifact Manager</td>\n",
       "      <td id=\"T_7dd3a_row7_col1\" class=\"data row7 col1\" >artifacts.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row8_col0\" class=\"data row8 col0\" >Token File</td>\n",
       "      <td id=\"T_7dd3a_row8_col1\" class=\"data row8 col1\" >/home/vscode/.tokens.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row9_col0\" class=\"data row9 col0\" >Project ID</td>\n",
       "      <td id=\"T_7dd3a_row9_col1\" class=\"data row9 col1\" >bbe0d94c-736b-477a-a2e6-fef9fe7ac9ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row10_col0\" class=\"data row10 col0\" >Bastion Host</td>\n",
       "      <td id=\"T_7dd3a_row10_col1\" class=\"data row10 col1\" >bastion.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row11_col0\" class=\"data row11 col0\" >Bastion Username</td>\n",
       "      <td id=\"T_7dd3a_row11_col1\" class=\"data row11 col1\" >tsai_0000103277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row12_col0\" class=\"data row12 col0\" >Bastion Private Key File</td>\n",
       "      <td id=\"T_7dd3a_row12_col1\" class=\"data row12 col1\" >/home/vscode/.ssh/fabric_local/bastion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row13_col0\" class=\"data row13 col0\" >Version</td>\n",
       "      <td id=\"T_7dd3a_row13_col1\" class=\"data row13 col1\" >1.8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row14_col0\" class=\"data row14 col0\" >Slice Public Key File</td>\n",
       "      <td id=\"T_7dd3a_row14_col1\" class=\"data row14 col1\" >/home/vscode/.ssh/fabric_local/silver.pub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row15_col0\" class=\"data row15 col0\" >Slice Private Key File</td>\n",
       "      <td id=\"T_7dd3a_row15_col1\" class=\"data row15 col1\" >/home/vscode/.ssh/fabric_local/silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row16_col0\" class=\"data row16 col0\" >Sites to avoid</td>\n",
       "      <td id=\"T_7dd3a_row16_col1\" class=\"data row16 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row17_col0\" class=\"data row17 col0\" >Bastion SSH Config File</td>\n",
       "      <td id=\"T_7dd3a_row17_col1\" class=\"data row17 col1\" >/home/vscode/.ssh/fabric_local/fabric_config</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row18_col0\" class=\"data row18 col0\" >Project Name</td>\n",
       "      <td id=\"T_7dd3a_row18_col1\" class=\"data row18 col1\" >EJFAT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbbcc3745e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7dd3a tr:nth-child(even) {\n",
       "  background: #dbf3ff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7dd3a tr:nth-child(odd) {\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7dd3a caption {\n",
       "  text-align: center;\n",
       "  font-size: 150%;\n",
       "}\n",
       "#T_7dd3a_row0_col0, #T_7dd3a_row0_col1, #T_7dd3a_row1_col0, #T_7dd3a_row1_col1, #T_7dd3a_row2_col0, #T_7dd3a_row2_col1, #T_7dd3a_row3_col0, #T_7dd3a_row3_col1, #T_7dd3a_row4_col0, #T_7dd3a_row4_col1, #T_7dd3a_row5_col0, #T_7dd3a_row5_col1, #T_7dd3a_row6_col0, #T_7dd3a_row6_col1, #T_7dd3a_row7_col0, #T_7dd3a_row7_col1, #T_7dd3a_row8_col0, #T_7dd3a_row8_col1, #T_7dd3a_row9_col0, #T_7dd3a_row9_col1, #T_7dd3a_row10_col0, #T_7dd3a_row10_col1, #T_7dd3a_row11_col0, #T_7dd3a_row11_col1, #T_7dd3a_row12_col0, #T_7dd3a_row12_col1, #T_7dd3a_row13_col0, #T_7dd3a_row13_col1, #T_7dd3a_row14_col0, #T_7dd3a_row14_col1, #T_7dd3a_row15_col0, #T_7dd3a_row15_col1, #T_7dd3a_row16_col0, #T_7dd3a_row16_col1, #T_7dd3a_row17_col0, #T_7dd3a_row17_col1, #T_7dd3a_row18_col0, #T_7dd3a_row18_col1 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7dd3a\">\n",
       "  <caption>FABlib Config</caption>\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row0_col0\" class=\"data row0 col0\" >Log Level</td>\n",
       "      <td id=\"T_7dd3a_row0_col1\" class=\"data row0 col1\" >INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row1_col0\" class=\"data row1 col0\" >Log File</td>\n",
       "      <td id=\"T_7dd3a_row1_col1\" class=\"data row1 col1\" >/tmp/fablib/fablib.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row2_col0\" class=\"data row2 col0\" >Data directory</td>\n",
       "      <td id=\"T_7dd3a_row2_col1\" class=\"data row2 col1\" >/tmp/fablib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row3_col0\" class=\"data row3 col0\" >SSH Command Line</td>\n",
       "      <td id=\"T_7dd3a_row3_col1\" class=\"data row3 col1\" >ssh -i {{ _self_.private_ssh_key_file }} -F /home/vscode/work/fabric_config/ssh_config {{ _self_.username }}@{{ _self_.management_ip }}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row4_col0\" class=\"data row4 col0\" >Orchestrator</td>\n",
       "      <td id=\"T_7dd3a_row4_col1\" class=\"data row4 col1\" >orchestrator.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row5_col0\" class=\"data row5 col0\" >Credential Manager</td>\n",
       "      <td id=\"T_7dd3a_row5_col1\" class=\"data row5 col1\" >cm.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row6_col0\" class=\"data row6 col0\" >Core API</td>\n",
       "      <td id=\"T_7dd3a_row6_col1\" class=\"data row6 col1\" >uis.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row7_col0\" class=\"data row7 col0\" >Artifact Manager</td>\n",
       "      <td id=\"T_7dd3a_row7_col1\" class=\"data row7 col1\" >artifacts.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row8_col0\" class=\"data row8 col0\" >Token File</td>\n",
       "      <td id=\"T_7dd3a_row8_col1\" class=\"data row8 col1\" >/home/vscode/.tokens.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row9_col0\" class=\"data row9 col0\" >Project ID</td>\n",
       "      <td id=\"T_7dd3a_row9_col1\" class=\"data row9 col1\" >bbe0d94c-736b-477a-a2e6-fef9fe7ac9ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row10_col0\" class=\"data row10 col0\" >Bastion Host</td>\n",
       "      <td id=\"T_7dd3a_row10_col1\" class=\"data row10 col1\" >bastion.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row11_col0\" class=\"data row11 col0\" >Bastion Username</td>\n",
       "      <td id=\"T_7dd3a_row11_col1\" class=\"data row11 col1\" >tsai_0000103277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row12_col0\" class=\"data row12 col0\" >Bastion Private Key File</td>\n",
       "      <td id=\"T_7dd3a_row12_col1\" class=\"data row12 col1\" >/home/vscode/.ssh/fabric_local/bastion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row13_col0\" class=\"data row13 col0\" >Version</td>\n",
       "      <td id=\"T_7dd3a_row13_col1\" class=\"data row13 col1\" >1.8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row14_col0\" class=\"data row14 col0\" >Slice Public Key File</td>\n",
       "      <td id=\"T_7dd3a_row14_col1\" class=\"data row14 col1\" >/home/vscode/.ssh/fabric_local/silver.pub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row15_col0\" class=\"data row15 col0\" >Slice Private Key File</td>\n",
       "      <td id=\"T_7dd3a_row15_col1\" class=\"data row15 col1\" >/home/vscode/.ssh/fabric_local/silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row16_col0\" class=\"data row16 col0\" >Sites to avoid</td>\n",
       "      <td id=\"T_7dd3a_row16_col1\" class=\"data row16 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row17_col0\" class=\"data row17 col0\" >Bastion SSH Config File</td>\n",
       "      <td id=\"T_7dd3a_row17_col1\" class=\"data row17 col1\" >/home/vscode/.ssh/fabric_local/fabric_config</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7dd3a_row18_col0\" class=\"data row18 col0\" >Project Name</td>\n",
       "      <td id=\"T_7dd3a_row18_col1\" class=\"data row18 col1\" >EJFAT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbbcc3745e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import traceback\n",
    "\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "\n",
    "fablib = fablib_manager()\n",
    "                     \n",
    "fablib.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (Optional) Query Available Resources\n",
    "\n",
    "This optional command queries the FABRIC services to find the available resources. It may be useful for finding a site with available capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     available_resources \u001b[38;5;241m=\u001b[39m \u001b[43mfablib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_available_resources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable Resources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_resources\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fabrictestbed_extensions/fablib/fablib.py:1955\u001b[0m, in \u001b[0;36mFablibManager.get_available_resources\u001b[0;34m(self, update, force_refresh, start, end, avoid, includes)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfabrictestbed_extensions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfablib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresources\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Resources\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources \u001b[38;5;241m=\u001b[39m \u001b[43mResources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_refresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mavoid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavoid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincludes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincludes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m update:\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   1965\u001b[0m         force_refresh\u001b[38;5;241m=\u001b[39mforce_refresh,\n\u001b[1;32m   1966\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         includes\u001b[38;5;241m=\u001b[39mincludes,\n\u001b[1;32m   1970\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fabrictestbed_extensions/fablib/resources.py:90\u001b[0m, in \u001b[0;36mResources.__init__\u001b[0;34m(self, fablib_manager, force_refresh, start, end, avoid, includes)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopology \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msites \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_refresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mincludes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincludes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavoid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavoid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fabrictestbed_extensions/fablib/resources.py:624\u001b[0m, in \u001b[0;36mResources.update\u001b[0;34m(self, force_refresh, start, end, avoid, includes)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopology \u001b[38;5;241m=\u001b[39m topology\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m site_name, site \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopology\u001b[38;5;241m.\u001b[39msites\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 624\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mSite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfablib_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fablib_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msites[site_name] \u001b[38;5;241m=\u001b[39m s\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fabrictestbed_extensions/fablib/site.py:679\u001b[0m, in \u001b[0;36mSite.__init__\u001b[0;34m(self, site, fablib_manager)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswitches \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite_info \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 679\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fabrictestbed_extensions/fablib/site.py:697\u001b[0m, in \u001b[0;36mSite.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03mLoad information about the site.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m:return: None\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load_hosts()\n\u001b[0;32m--> 697\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load_site_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fabrictestbed_extensions/fablib/site.py:948\u001b[0m, in \u001b[0;36mSite.__load_site_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite_info[Constants\u001b[38;5;241m.\u001b[39mDISK\u001b[38;5;241m.\u001b[39mlower()] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    943\u001b[0m     Constants\u001b[38;5;241m.\u001b[39mCAPACITY\u001b[38;5;241m.\u001b[39mlower(): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_disk_capacity(),\n\u001b[1;32m    944\u001b[0m     Constants\u001b[38;5;241m.\u001b[39mALLOCATED\u001b[38;5;241m.\u001b[39mlower(): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_disk_allocated(),\n\u001b[1;32m    945\u001b[0m }\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhosts\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m component_model_name, c \u001b[38;5;129;01min\u001b[39;00m h\u001b[38;5;241m.\u001b[39mget_components()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    950\u001b[0m             comp_cap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite_info\u001b[38;5;241m.\u001b[39msetdefault(\n\u001b[1;32m    951\u001b[0m                 component_model_name\u001b[38;5;241m.\u001b[39mlower(), {}\n\u001b[1;32m    952\u001b[0m             )\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fabrictestbed_extensions/fablib/site.py:350\u001b[0m, in \u001b[0;36mHost.get_components\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03mGet the components associated with the host.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m:return: Dictionary-like view of the components associated with the host.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03m:rtype: ViewOnlyDict\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fim/user/node.py:185\u001b[0m, in \u001b[0;36mNode.components\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcomponents\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__list_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fim/user/node.py:447\u001b[0m, in \u001b[0;36mNode.__list_components\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__list_components\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ViewOnlyDict:\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    List all Components children of a node in the topology as a dictionary\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    organized by component name. Modifying the dictionary will not affect\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m    the underlying model, but modifying Components in the dictionary will.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     node_id_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_network_node_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_node_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m nid \u001b[38;5;129;01min\u001b[39;00m node_id_list:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fim/graph/resources/abc_bqm.py:66\u001b[0m, in \u001b[0;36mABCBQMPropertyGraph.get_all_network_node_components\u001b[0;34m(self, parent_node_id)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_all_network_node_components\u001b[39m(\u001b[38;5;28mself\u001b[39m, parent_node_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Return a list of components, children of a prent (presumably composite node)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    :param parent_node_id:\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_node_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_node_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABCPropertyGraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NetworkNode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fim/graph/resources/abc_bqm.py:45\u001b[0m, in \u001b[0;36mABCBQMPropertyGraph.__get_node_components\u001b[0;34m(self, parent_node_id, claz)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m parent_node_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# check that parent is a NetworkNode\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m labels, parent_props \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_node_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_node_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m claz \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PropertyGraphQueryException(graph_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_id, node_id\u001b[38;5;241m=\u001b[39mparent_node_id,\n\u001b[1;32m     48\u001b[0m                                       msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParent node type is not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclaz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fim/graph/networkx_property_graph.py:96\u001b[0m, in \u001b[0;36mNetworkXPropertyGraph.get_node_properties\u001b[0;34m(self, node_id)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m node_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     node_props \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mget_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_id)\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PropertyGraphQueryException(graph_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_id,\n\u001b[1;32m     99\u001b[0m                                       node_id\u001b[38;5;241m=\u001b[39mnode_id, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/fim/graph/networkx_mixin.py:58\u001b[0m, in \u001b[0;36mNetworkXMixin._find_node\u001b[0;34m(self, node_id, graph_id)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     use_graph_id \u001b[38;5;241m=\u001b[39m graph_id\n\u001b[0;32m---> 58\u001b[0m query_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnxq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_graph_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mABCPropertyGraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNODE_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mABCPropertyGraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGRAPH_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_graph_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_match) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PropertyGraphQueryException(graph_id\u001b[38;5;241m=\u001b[39muse_graph_id,\n\u001b[1;32m     66\u001b[0m                                       node_id\u001b[38;5;241m=\u001b[39mnode_id, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/networkx_query/query.py:29\u001b[0m, in \u001b[0;36msearch_nodes.<locals>.<lambda>\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Search nodes in specified graph which match query.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _predicate \u001b[38;5;241m=\u001b[39m prepare_query(query)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(get_first_item, \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m node: \u001b[43m_predicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, graph\u001b[38;5;241m.\u001b[39mnodes(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/networkx_query/definition.py:38\u001b[0m, in \u001b[0;36moperator_factory.<locals>.<lambda>\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moperator_factory\u001b[39m(op_function: Callable, \u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Evaluator:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add context parameter to operator function.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m context: \u001b[43mop_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/networkx_query/operator.py:111\u001b[0m, in \u001b[0;36mop_and\u001b[0;34m(context, *filters)\u001b[0m\n\u001b[1;32m    107\u001b[0m     (match, _current_value) \u001b[38;5;241m=\u001b[39m lookup_path(context, path)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m match \u001b[38;5;129;01mand\u001b[39;00m _current_value \u001b[38;5;129;01min\u001b[39;00m value\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;129m@register_operator\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand\u001b[39m\u001b[38;5;124m\"\u001b[39m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&&\u001b[39m\u001b[38;5;124m\"\u001b[39m, arity\u001b[38;5;241m=\u001b[39mOperatoryArity\u001b[38;5;241m.\u001b[39mNARY, combinator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, profile\u001b[38;5;241m=\u001b[39m[Any, Evaluator])\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mop_and\u001b[39m(context: Any, \u001b[38;5;241m*\u001b[39mfilters: Evaluator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Define And operator.\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(f(context) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filters)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    available_resources = fablib.get_available_resources()\n",
    "    print(f\"Available Resources: {available_resources}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Experiment Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry: 10, Time: 259 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a534e tr:nth-child(even) {\n",
       "  background: #dbf3ff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_a534e tr:nth-child(odd) {\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_a534e caption {\n",
       "  text-align: center;\n",
       "  font-size: 150%;\n",
       "}\n",
       "#T_a534e_row0_col0, #T_a534e_row0_col1, #T_a534e_row1_col0, #T_a534e_row1_col1, #T_a534e_row2_col0, #T_a534e_row2_col1, #T_a534e_row3_col0, #T_a534e_row3_col1, #T_a534e_row4_col0, #T_a534e_row4_col1, #T_a534e_row5_col0 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "  background-color: ;\n",
       "}\n",
       "#T_a534e_row5_col1 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "  background-color: #c3ffc4;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a534e\">\n",
       "  <caption>Slice</caption>\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_a534e_row0_col0\" class=\"data row0 col0\" >ID</td>\n",
       "      <td id=\"T_a534e_row0_col1\" class=\"data row0 col1\" >f45ef733-64be-4c1d-9398-0fa7fced6a3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a534e_row1_col0\" class=\"data row1 col0\" >Name</td>\n",
       "      <td id=\"T_a534e_row1_col1\" class=\"data row1 col1\" >K8s_on_FABRIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a534e_row2_col0\" class=\"data row2 col0\" >Lease Expiration (UTC)</td>\n",
       "      <td id=\"T_a534e_row2_col1\" class=\"data row2 col1\" >2025-03-14 16:38:21 +0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a534e_row3_col0\" class=\"data row3 col0\" >Lease Start (UTC)</td>\n",
       "      <td id=\"T_a534e_row3_col1\" class=\"data row3 col1\" >2025-03-13 16:38:21 +0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a534e_row4_col0\" class=\"data row4 col0\" >Project ID</td>\n",
       "      <td id=\"T_a534e_row4_col1\" class=\"data row4 col1\" >bbe0d94c-736b-477a-a2e6-fef9fe7ac9ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a534e_row5_col0\" class=\"data row5 col0\" >State</td>\n",
       "      <td id=\"T_a534e_row5_col1\" class=\"data row5 col1\" >StableOK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbbcf143eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f0f05 caption {\n",
       "  text-align: center;\n",
       "  font-size: 150%;\n",
       "  caption-side: top;\n",
       "}\n",
       "#T_f0f05 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f0f05 tr:nth-child(even) {\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_f0f05 tr:nth-child(odd) {\n",
       "  background: #dbf3ff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_f0f05 .level0 {\n",
       "  border: 1px black solid !important;\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_f0f05_row0_col0, #T_f0f05_row0_col1, #T_f0f05_row0_col2, #T_f0f05_row0_col3, #T_f0f05_row0_col4, #T_f0f05_row0_col5, #T_f0f05_row0_col6, #T_f0f05_row0_col7, #T_f0f05_row0_col8, #T_f0f05_row0_col9, #T_f0f05_row0_col10, #T_f0f05_row0_col13, #T_f0f05_row0_col14, #T_f0f05_row0_col15, #T_f0f05_row1_col0, #T_f0f05_row1_col1, #T_f0f05_row1_col2, #T_f0f05_row1_col3, #T_f0f05_row1_col4, #T_f0f05_row1_col5, #T_f0f05_row1_col6, #T_f0f05_row1_col7, #T_f0f05_row1_col8, #T_f0f05_row1_col9, #T_f0f05_row1_col10, #T_f0f05_row1_col13, #T_f0f05_row1_col14, #T_f0f05_row1_col15, #T_f0f05_row2_col0, #T_f0f05_row2_col1, #T_f0f05_row2_col2, #T_f0f05_row2_col3, #T_f0f05_row2_col4, #T_f0f05_row2_col5, #T_f0f05_row2_col6, #T_f0f05_row2_col7, #T_f0f05_row2_col8, #T_f0f05_row2_col9, #T_f0f05_row2_col10, #T_f0f05_row2_col13, #T_f0f05_row2_col14, #T_f0f05_row2_col15 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "}\n",
       "#T_f0f05_row0_col11, #T_f0f05_row1_col11, #T_f0f05_row2_col11 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "  background-color: #c3ffc4;\n",
       "}\n",
       "#T_f0f05_row0_col12, #T_f0f05_row1_col12, #T_f0f05_row2_col12 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "  background-color: ;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f0f05\">\n",
       "  <caption>Nodes</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_f0f05_level0_col0\" class=\"col_heading level0 col0\" >ID</th>\n",
       "      <th id=\"T_f0f05_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_f0f05_level0_col2\" class=\"col_heading level0 col2\" >Cores</th>\n",
       "      <th id=\"T_f0f05_level0_col3\" class=\"col_heading level0 col3\" >RAM</th>\n",
       "      <th id=\"T_f0f05_level0_col4\" class=\"col_heading level0 col4\" >Disk</th>\n",
       "      <th id=\"T_f0f05_level0_col5\" class=\"col_heading level0 col5\" >Image</th>\n",
       "      <th id=\"T_f0f05_level0_col6\" class=\"col_heading level0 col6\" >Image Type</th>\n",
       "      <th id=\"T_f0f05_level0_col7\" class=\"col_heading level0 col7\" >Host</th>\n",
       "      <th id=\"T_f0f05_level0_col8\" class=\"col_heading level0 col8\" >Site</th>\n",
       "      <th id=\"T_f0f05_level0_col9\" class=\"col_heading level0 col9\" >Username</th>\n",
       "      <th id=\"T_f0f05_level0_col10\" class=\"col_heading level0 col10\" >Management IP</th>\n",
       "      <th id=\"T_f0f05_level0_col11\" class=\"col_heading level0 col11\" >State</th>\n",
       "      <th id=\"T_f0f05_level0_col12\" class=\"col_heading level0 col12\" >Error</th>\n",
       "      <th id=\"T_f0f05_level0_col13\" class=\"col_heading level0 col13\" >SSH Command</th>\n",
       "      <th id=\"T_f0f05_level0_col14\" class=\"col_heading level0 col14\" >Public SSH Key File</th>\n",
       "      <th id=\"T_f0f05_level0_col15\" class=\"col_heading level0 col15\" >Private SSH Key File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_f0f05_row0_col0\" class=\"data row0 col0\" >6132751a-83b4-4daf-be14-7ba0ded7737c</td>\n",
       "      <td id=\"T_f0f05_row0_col1\" class=\"data row0 col1\" >cpnode</td>\n",
       "      <td id=\"T_f0f05_row0_col2\" class=\"data row0 col2\" >2</td>\n",
       "      <td id=\"T_f0f05_row0_col3\" class=\"data row0 col3\" >8</td>\n",
       "      <td id=\"T_f0f05_row0_col4\" class=\"data row0 col4\" >100</td>\n",
       "      <td id=\"T_f0f05_row0_col5\" class=\"data row0 col5\" >default_ubuntu_20</td>\n",
       "      <td id=\"T_f0f05_row0_col6\" class=\"data row0 col6\" >qcow2</td>\n",
       "      <td id=\"T_f0f05_row0_col7\" class=\"data row0 col7\" >ucsd-w1.fabric-testbed.net</td>\n",
       "      <td id=\"T_f0f05_row0_col8\" class=\"data row0 col8\" >UCSD</td>\n",
       "      <td id=\"T_f0f05_row0_col9\" class=\"data row0 col9\" >ubuntu</td>\n",
       "      <td id=\"T_f0f05_row0_col10\" class=\"data row0 col10\" >132.249.252.151</td>\n",
       "      <td id=\"T_f0f05_row0_col11\" class=\"data row0 col11\" >Active</td>\n",
       "      <td id=\"T_f0f05_row0_col12\" class=\"data row0 col12\" ></td>\n",
       "      <td id=\"T_f0f05_row0_col13\" class=\"data row0 col13\" >ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.151</td>\n",
       "      <td id=\"T_f0f05_row0_col14\" class=\"data row0 col14\" >/home/vscode/.ssh/fabric_local/silver.pub</td>\n",
       "      <td id=\"T_f0f05_row0_col15\" class=\"data row0 col15\" >/home/vscode/.ssh/fabric_local/silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f0f05_row1_col0\" class=\"data row1 col0\" >9efad2f0-6399-488b-b005-888e2ec5aa62</td>\n",
       "      <td id=\"T_f0f05_row1_col1\" class=\"data row1 col1\" >wknode1</td>\n",
       "      <td id=\"T_f0f05_row1_col2\" class=\"data row1 col2\" >2</td>\n",
       "      <td id=\"T_f0f05_row1_col3\" class=\"data row1 col3\" >8</td>\n",
       "      <td id=\"T_f0f05_row1_col4\" class=\"data row1 col4\" >100</td>\n",
       "      <td id=\"T_f0f05_row1_col5\" class=\"data row1 col5\" >default_ubuntu_20</td>\n",
       "      <td id=\"T_f0f05_row1_col6\" class=\"data row1 col6\" >qcow2</td>\n",
       "      <td id=\"T_f0f05_row1_col7\" class=\"data row1 col7\" >ucsd-w1.fabric-testbed.net</td>\n",
       "      <td id=\"T_f0f05_row1_col8\" class=\"data row1 col8\" >UCSD</td>\n",
       "      <td id=\"T_f0f05_row1_col9\" class=\"data row1 col9\" >ubuntu</td>\n",
       "      <td id=\"T_f0f05_row1_col10\" class=\"data row1 col10\" >132.249.252.165</td>\n",
       "      <td id=\"T_f0f05_row1_col11\" class=\"data row1 col11\" >Active</td>\n",
       "      <td id=\"T_f0f05_row1_col12\" class=\"data row1 col12\" ></td>\n",
       "      <td id=\"T_f0f05_row1_col13\" class=\"data row1 col13\" >ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.165</td>\n",
       "      <td id=\"T_f0f05_row1_col14\" class=\"data row1 col14\" >/home/vscode/.ssh/fabric_local/silver.pub</td>\n",
       "      <td id=\"T_f0f05_row1_col15\" class=\"data row1 col15\" >/home/vscode/.ssh/fabric_local/silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f0f05_row2_col0\" class=\"data row2 col0\" >df11f0e8-f471-4fc1-b15e-9b8aa9004be3</td>\n",
       "      <td id=\"T_f0f05_row2_col1\" class=\"data row2 col1\" >wknode2</td>\n",
       "      <td id=\"T_f0f05_row2_col2\" class=\"data row2 col2\" >2</td>\n",
       "      <td id=\"T_f0f05_row2_col3\" class=\"data row2 col3\" >8</td>\n",
       "      <td id=\"T_f0f05_row2_col4\" class=\"data row2 col4\" >100</td>\n",
       "      <td id=\"T_f0f05_row2_col5\" class=\"data row2 col5\" >default_ubuntu_20</td>\n",
       "      <td id=\"T_f0f05_row2_col6\" class=\"data row2 col6\" >qcow2</td>\n",
       "      <td id=\"T_f0f05_row2_col7\" class=\"data row2 col7\" >ucsd-w1.fabric-testbed.net</td>\n",
       "      <td id=\"T_f0f05_row2_col8\" class=\"data row2 col8\" >UCSD</td>\n",
       "      <td id=\"T_f0f05_row2_col9\" class=\"data row2 col9\" >ubuntu</td>\n",
       "      <td id=\"T_f0f05_row2_col10\" class=\"data row2 col10\" >132.249.252.182</td>\n",
       "      <td id=\"T_f0f05_row2_col11\" class=\"data row2 col11\" >Active</td>\n",
       "      <td id=\"T_f0f05_row2_col12\" class=\"data row2 col12\" ></td>\n",
       "      <td id=\"T_f0f05_row2_col13\" class=\"data row2 col13\" >ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.182</td>\n",
       "      <td id=\"T_f0f05_row2_col14\" class=\"data row2 col14\" >/home/vscode/.ssh/fabric_local/silver.pub</td>\n",
       "      <td id=\"T_f0f05_row2_col15\" class=\"data row2 col15\" >/home/vscode/.ssh/fabric_local/silver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbbcefc8850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7d746 caption {\n",
       "  text-align: center;\n",
       "  font-size: 150%;\n",
       "  caption-side: top;\n",
       "}\n",
       "#T_7d746 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_7d746 tr:nth-child(even) {\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7d746 tr:nth-child(odd) {\n",
       "  background: #dbf3ff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7d746 .level0 {\n",
       "  border: 1px black solid !important;\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7d746_row0_col0, #T_7d746_row0_col1, #T_7d746_row0_col2, #T_7d746_row0_col3, #T_7d746_row0_col4, #T_7d746_row0_col5, #T_7d746_row0_col6 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "}\n",
       "#T_7d746_row0_col7 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "  background-color: #c3ffc4;\n",
       "}\n",
       "#T_7d746_row0_col8 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "  background-color: ;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7d746\">\n",
       "  <caption>Networks</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7d746_level0_col0\" class=\"col_heading level0 col0\" >ID</th>\n",
       "      <th id=\"T_7d746_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_7d746_level0_col2\" class=\"col_heading level0 col2\" >Layer</th>\n",
       "      <th id=\"T_7d746_level0_col3\" class=\"col_heading level0 col3\" >Type</th>\n",
       "      <th id=\"T_7d746_level0_col4\" class=\"col_heading level0 col4\" >Site</th>\n",
       "      <th id=\"T_7d746_level0_col5\" class=\"col_heading level0 col5\" >Subnet</th>\n",
       "      <th id=\"T_7d746_level0_col6\" class=\"col_heading level0 col6\" >Gateway</th>\n",
       "      <th id=\"T_7d746_level0_col7\" class=\"col_heading level0 col7\" >State</th>\n",
       "      <th id=\"T_7d746_level0_col8\" class=\"col_heading level0 col8\" >Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7d746_row0_col0\" class=\"data row0 col0\" >5363e492-babe-420c-bf9b-0ab51a0ebe99</td>\n",
       "      <td id=\"T_7d746_row0_col1\" class=\"data row0 col1\" >NET1</td>\n",
       "      <td id=\"T_7d746_row0_col2\" class=\"data row0 col2\" >L3</td>\n",
       "      <td id=\"T_7d746_row0_col3\" class=\"data row0 col3\" >FABNetv4</td>\n",
       "      <td id=\"T_7d746_row0_col4\" class=\"data row0 col4\" >UCSD</td>\n",
       "      <td id=\"T_7d746_row0_col5\" class=\"data row0 col5\" >10.134.132.0/24</td>\n",
       "      <td id=\"T_7d746_row0_col6\" class=\"data row0 col6\" >10.134.132.1</td>\n",
       "      <td id=\"T_7d746_row0_col7\" class=\"data row0 col7\" >Active</td>\n",
       "      <td id=\"T_7d746_row0_col8\" class=\"data row0 col8\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbbcf13a1c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0cbe9 caption {\n",
       "  text-align: center;\n",
       "  font-size: 150%;\n",
       "  caption-side: top;\n",
       "}\n",
       "#T_0cbe9 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0cbe9 tr:nth-child(even) {\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_0cbe9 tr:nth-child(odd) {\n",
       "  background: #dbf3ff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_0cbe9 .level0 {\n",
       "  border: 1px black solid !important;\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_0cbe9_row0_col0, #T_0cbe9_row0_col1, #T_0cbe9_row0_col2, #T_0cbe9_row0_col3, #T_0cbe9_row0_col4, #T_0cbe9_row0_col5, #T_0cbe9_row0_col6, #T_0cbe9_row0_col7, #T_0cbe9_row0_col8, #T_0cbe9_row0_col9, #T_0cbe9_row0_col10, #T_0cbe9_row0_col11, #T_0cbe9_row0_col12, #T_0cbe9_row1_col0, #T_0cbe9_row1_col1, #T_0cbe9_row1_col2, #T_0cbe9_row1_col3, #T_0cbe9_row1_col4, #T_0cbe9_row1_col5, #T_0cbe9_row1_col6, #T_0cbe9_row1_col7, #T_0cbe9_row1_col8, #T_0cbe9_row1_col9, #T_0cbe9_row1_col10, #T_0cbe9_row1_col11, #T_0cbe9_row1_col12, #T_0cbe9_row2_col0, #T_0cbe9_row2_col1, #T_0cbe9_row2_col2, #T_0cbe9_row2_col3, #T_0cbe9_row2_col4, #T_0cbe9_row2_col5, #T_0cbe9_row2_col6, #T_0cbe9_row2_col7, #T_0cbe9_row2_col8, #T_0cbe9_row2_col9, #T_0cbe9_row2_col10, #T_0cbe9_row2_col11, #T_0cbe9_row2_col12 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0cbe9\">\n",
       "  <caption>Interfaces</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_0cbe9_level0_col0\" class=\"col_heading level0 col0\" >Name</th>\n",
       "      <th id=\"T_0cbe9_level0_col1\" class=\"col_heading level0 col1\" >Short Name</th>\n",
       "      <th id=\"T_0cbe9_level0_col2\" class=\"col_heading level0 col2\" >Node</th>\n",
       "      <th id=\"T_0cbe9_level0_col3\" class=\"col_heading level0 col3\" >Network</th>\n",
       "      <th id=\"T_0cbe9_level0_col4\" class=\"col_heading level0 col4\" >Bandwidth</th>\n",
       "      <th id=\"T_0cbe9_level0_col5\" class=\"col_heading level0 col5\" >Mode</th>\n",
       "      <th id=\"T_0cbe9_level0_col6\" class=\"col_heading level0 col6\" >VLAN</th>\n",
       "      <th id=\"T_0cbe9_level0_col7\" class=\"col_heading level0 col7\" >MAC</th>\n",
       "      <th id=\"T_0cbe9_level0_col8\" class=\"col_heading level0 col8\" >Physical Device</th>\n",
       "      <th id=\"T_0cbe9_level0_col9\" class=\"col_heading level0 col9\" >Device</th>\n",
       "      <th id=\"T_0cbe9_level0_col10\" class=\"col_heading level0 col10\" >IP Address</th>\n",
       "      <th id=\"T_0cbe9_level0_col11\" class=\"col_heading level0 col11\" >Numa Node</th>\n",
       "      <th id=\"T_0cbe9_level0_col12\" class=\"col_heading level0 col12\" >Switch Port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_0cbe9_row0_col0\" class=\"data row0 col0\" >cpnode-NIC1-p1</td>\n",
       "      <td id=\"T_0cbe9_row0_col1\" class=\"data row0 col1\" >p1</td>\n",
       "      <td id=\"T_0cbe9_row0_col2\" class=\"data row0 col2\" >cpnode</td>\n",
       "      <td id=\"T_0cbe9_row0_col3\" class=\"data row0 col3\" >NET1</td>\n",
       "      <td id=\"T_0cbe9_row0_col4\" class=\"data row0 col4\" >100</td>\n",
       "      <td id=\"T_0cbe9_row0_col5\" class=\"data row0 col5\" >config</td>\n",
       "      <td id=\"T_0cbe9_row0_col6\" class=\"data row0 col6\" ></td>\n",
       "      <td id=\"T_0cbe9_row0_col7\" class=\"data row0 col7\" >02:34:D8:94:66:FD</td>\n",
       "      <td id=\"T_0cbe9_row0_col8\" class=\"data row0 col8\" >enp7s0</td>\n",
       "      <td id=\"T_0cbe9_row0_col9\" class=\"data row0 col9\" >enp7s0</td>\n",
       "      <td id=\"T_0cbe9_row0_col10\" class=\"data row0 col10\" >fe80::34:d8ff:fe94:66fd</td>\n",
       "      <td id=\"T_0cbe9_row0_col11\" class=\"data row0 col11\" >6</td>\n",
       "      <td id=\"T_0cbe9_row0_col12\" class=\"data row0 col12\" >HundredGigE0/0/0/5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0cbe9_row1_col0\" class=\"data row1 col0\" >wknode1-NIC2-p1</td>\n",
       "      <td id=\"T_0cbe9_row1_col1\" class=\"data row1 col1\" >p1</td>\n",
       "      <td id=\"T_0cbe9_row1_col2\" class=\"data row1 col2\" >wknode1</td>\n",
       "      <td id=\"T_0cbe9_row1_col3\" class=\"data row1 col3\" >NET1</td>\n",
       "      <td id=\"T_0cbe9_row1_col4\" class=\"data row1 col4\" >100</td>\n",
       "      <td id=\"T_0cbe9_row1_col5\" class=\"data row1 col5\" >config</td>\n",
       "      <td id=\"T_0cbe9_row1_col6\" class=\"data row1 col6\" ></td>\n",
       "      <td id=\"T_0cbe9_row1_col7\" class=\"data row1 col7\" >02:74:7A:C2:0A:63</td>\n",
       "      <td id=\"T_0cbe9_row1_col8\" class=\"data row1 col8\" >enp7s0</td>\n",
       "      <td id=\"T_0cbe9_row1_col9\" class=\"data row1 col9\" >enp7s0</td>\n",
       "      <td id=\"T_0cbe9_row1_col10\" class=\"data row1 col10\" >fe80::74:7aff:fec2:a63</td>\n",
       "      <td id=\"T_0cbe9_row1_col11\" class=\"data row1 col11\" >6</td>\n",
       "      <td id=\"T_0cbe9_row1_col12\" class=\"data row1 col12\" >HundredGigE0/0/0/5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0cbe9_row2_col0\" class=\"data row2 col0\" >wknode2-NIC3-p1</td>\n",
       "      <td id=\"T_0cbe9_row2_col1\" class=\"data row2 col1\" >p1</td>\n",
       "      <td id=\"T_0cbe9_row2_col2\" class=\"data row2 col2\" >wknode2</td>\n",
       "      <td id=\"T_0cbe9_row2_col3\" class=\"data row2 col3\" >NET1</td>\n",
       "      <td id=\"T_0cbe9_row2_col4\" class=\"data row2 col4\" >100</td>\n",
       "      <td id=\"T_0cbe9_row2_col5\" class=\"data row2 col5\" >config</td>\n",
       "      <td id=\"T_0cbe9_row2_col6\" class=\"data row2 col6\" ></td>\n",
       "      <td id=\"T_0cbe9_row2_col7\" class=\"data row2 col7\" >0A:19:42:43:96:03</td>\n",
       "      <td id=\"T_0cbe9_row2_col8\" class=\"data row2 col8\" >enp7s0</td>\n",
       "      <td id=\"T_0cbe9_row2_col9\" class=\"data row2 col9\" >enp7s0</td>\n",
       "      <td id=\"T_0cbe9_row2_col10\" class=\"data row2 col10\" >fe80::819:42ff:fe43:9603</td>\n",
       "      <td id=\"T_0cbe9_row2_col11\" class=\"data row2 col11\" >6</td>\n",
       "      <td id=\"T_0cbe9_row2_col12\" class=\"data row2 col12\" >HundredGigE0/0/0/5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbbcf2d83d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to print interfaces 270 seconds\n"
     ]
    }
   ],
   "source": [
    "# Configuration variables\n",
    "slice_name = 'K8s_on_FABRIC'\n",
    "site = \"UCSD\"\n",
    "image = 'default_ubuntu_20'\n",
    "network_name = 'NET1'\n",
    "\n",
    "# Create lists for node configurations\n",
    "nic_names, node_names = [], []\n",
    "for i in range(1, 4):\n",
    "    if i == 1:\n",
    "        node_names.append(f\"cpnode\")\n",
    "        nic_names.append(f\"NIC1\")\n",
    "    else:\n",
    "        node_names.append(f\"wknode{i-1}\")\n",
    "        nic_names.append(f\"NIC{i}\")\n",
    "\n",
    "print(f\"Site: {site}\")\n",
    "\n",
    "try:\n",
    "    # Create Slice\n",
    "    slice = fablib.new_slice(slice_name)\n",
    "    \n",
    "    # Add nodes and interfaces\n",
    "    nodes = []\n",
    "    ifaces = []\n",
    "    for i in range(len(node_names)):\n",
    "        node = slice.add_node(\n",
    "            name=node_names[i],\n",
    "            site=site,\n",
    "            image=image,\n",
    "            disk=100\n",
    "        )\n",
    "        nodes.append(node)\n",
    "        iface = node.add_component(\n",
    "            model='NIC_Basic', \n",
    "            name=nic_names[i]\n",
    "        ).get_interfaces()[0]\n",
    "        ifaces.append(iface)\n",
    "    \n",
    "    # Network\n",
    "    net1 = slice.add_l3network(\n",
    "        name=network_name, \n",
    "        interfaces=ifaces, \n",
    "        type='IPv4'\n",
    "    )\n",
    "    \n",
    "    # Submit Slice Request\n",
    "    slice_id = slice.submit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print the Node's Attributes\n",
    "\n",
    "Each node in the slice has a set of get functions that return the node's attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpnode: -----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "ID                 6132751a-83b4-4daf-be14-7ba0ded7737c\n",
      "Name               cpnode\n",
      "Cores              2\n",
      "RAM                8\n",
      "Disk               100\n",
      "Image              default_ubuntu_20\n",
      "Image Type         qcow2\n",
      "Host               ucsd-w1.fabric-testbed.net\n",
      "Site               UCSD\n",
      "Management IP      132.249.252.151\n",
      "Reservation State  Active\n",
      "Error Message\n",
      "SSH Command        ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.151\n",
      "-----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "wknode1: -----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "ID                 9efad2f0-6399-488b-b005-888e2ec5aa62\n",
      "Name               wknode1\n",
      "Cores              2\n",
      "RAM                8\n",
      "Disk               100\n",
      "Image              default_ubuntu_20\n",
      "Image Type         qcow2\n",
      "Host               ucsd-w1.fabric-testbed.net\n",
      "Site               UCSD\n",
      "Management IP      132.249.252.165\n",
      "Reservation State  Active\n",
      "Error Message\n",
      "SSH Command        ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.165\n",
      "-----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "wknode2: -----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "ID                 df11f0e8-f471-4fc1-b15e-9b8aa9004be3\n",
      "Name               wknode2\n",
      "Cores              2\n",
      "RAM                8\n",
      "Disk               100\n",
      "Image              default_ubuntu_20\n",
      "Image Type         qcow2\n",
      "Host               ucsd-w1.fabric-testbed.net\n",
      "Site               UCSD\n",
      "Management IP      132.249.252.182\n",
      "Reservation State  Active\n",
      "Error Message\n",
      "SSH Command        ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.182\n",
      "-----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "Network: -------  ------------------------------------\n",
      "ID       5363e492-babe-420c-bf9b-0ab51a0ebe99\n",
      "Name     NET1\n",
      "Layer    L3\n",
      "Type     FABNetv4\n",
      "Site     UCSD\n",
      "Gateway  10.134.132.1\n",
      "Subnet   10.134.132.0/24\n",
      "State    Active\n",
      "Error\n",
      "-------  ------------------------------------\n",
      "\n",
      "Configuration for cpnode:\n",
      "Address assigned: 10.134.132.2\n",
      "3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 02:34:d8:94:66:fd brd ff:ff:ff:ff:ff:ff\n",
      "    inet 10.134.132.2/24 scope global enp7s0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::34:d8ff:fe94:66fd/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 02:34:d8:94:66:fd brd ff:ff:ff:ff:ff:ff\n",
      "    inet 10.134.132.2/24 scope global enp7s0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::34:d8ff:fe94:66fd/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "\n",
      "default via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.206 metric 100 \n",
      "10.20.4.0/23 dev enp3s0 proto kernel scope link src 10.20.4.206 metric 100 \n",
      "10.134.132.0/24 dev enp7s0 proto kernel scope link src 10.134.132.2 \n",
      "169.254.169.254 via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.206 metric 100 \n",
      "default via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.206 metric 100 \n",
      "10.20.4.0/23 dev enp3s0 proto kernel scope link src 10.20.4.206 metric 100 \n",
      "10.134.132.0/24 dev enp7s0 proto kernel scope link src 10.134.132.2 \n",
      "169.254.169.254 via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.206 metric 100 \n",
      "\n",
      "\n",
      "Configuration for wknode1:\n",
      "Address assigned: 10.134.132.3\n",
      "3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 02:74:7a:c2:0a:63 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 10.134.132.3/24 scope global enp7s0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::74:7aff:fec2:a63/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 02:74:7a:c2:0a:63 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 10.134.132.3/24 scope global enp7s0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::74:7aff:fec2:a63/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "\n",
      "default via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.5.18 metric 100 \n",
      "10.20.4.0/23 dev enp3s0 proto kernel scope link src 10.20.5.18 metric 100 \n",
      "10.134.132.0/24 dev enp7s0 proto kernel scope link src 10.134.132.3 \n",
      "169.254.169.254 via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.5.18 metric 100 \n",
      "default via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.5.18 metric 100 \n",
      "10.20.4.0/23 dev enp3s0 proto kernel scope link src 10.20.5.18 metric 100 \n",
      "10.134.132.0/24 dev enp7s0 proto kernel scope link src 10.134.132.3 \n",
      "169.254.169.254 via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.5.18 metric 100 \n",
      "\n",
      "\n",
      "Configuration for wknode2:\n",
      "Address assigned: 10.134.132.4\n",
      "3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 0a:19:42:43:96:03 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 10.134.132.4/24 scope global enp7s0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::819:42ff:fe43:9603/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 0a:19:42:43:96:03 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 10.134.132.4/24 scope global enp7s0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::819:42ff:fe43:9603/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "\n",
      "default via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.229 metric 100 \n",
      "10.20.4.0/23 dev enp3s0 proto kernel scope link src 10.20.4.229 metric 100 \n",
      "10.134.132.0/24 dev enp7s0 proto kernel scope link src 10.134.132.4 \n",
      "169.254.169.254 via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.229 metric 100 \n",
      "default via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.229 metric 100 \n",
      "10.20.4.0/23 dev enp3s0 proto kernel scope link src 10.20.4.229 metric 100 \n",
      "10.134.132.0/24 dev enp7s0 proto kernel scope link src 10.134.132.4 \n",
      "169.254.169.254 via 10.20.4.1 dev enp3s0 proto dhcp src 10.20.4.229 metric 100 \n",
      "\n",
      "\n",
      "Node and Address Summary:\n",
      "cpnode: -----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "ID                 6132751a-83b4-4daf-be14-7ba0ded7737c\n",
      "Name               cpnode\n",
      "Cores              2\n",
      "RAM                8\n",
      "Disk               100\n",
      "Image              default_ubuntu_20\n",
      "Image Type         qcow2\n",
      "Host               ucsd-w1.fabric-testbed.net\n",
      "Site               UCSD\n",
      "Management IP      132.249.252.151\n",
      "Reservation State  Active\n",
      "Error Message\n",
      "SSH Command        ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.151\n",
      "-----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "cpnode_address: 10.134.132.2\n",
      "wknode1: -----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "ID                 9efad2f0-6399-488b-b005-888e2ec5aa62\n",
      "Name               wknode1\n",
      "Cores              2\n",
      "RAM                8\n",
      "Disk               100\n",
      "Image              default_ubuntu_20\n",
      "Image Type         qcow2\n",
      "Host               ucsd-w1.fabric-testbed.net\n",
      "Site               UCSD\n",
      "Management IP      132.249.252.165\n",
      "Reservation State  Active\n",
      "Error Message\n",
      "SSH Command        ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.165\n",
      "-----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "wknode1_address: 10.134.132.3\n",
      "wknode2: -----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "ID                 df11f0e8-f471-4fc1-b15e-9b8aa9004be3\n",
      "Name               wknode2\n",
      "Cores              2\n",
      "RAM                8\n",
      "Disk               100\n",
      "Image              default_ubuntu_20\n",
      "Image Type         qcow2\n",
      "Host               ucsd-w1.fabric-testbed.net\n",
      "Site               UCSD\n",
      "Management IP      132.249.252.182\n",
      "Reservation State  Active\n",
      "Error Message\n",
      "SSH Command        ssh -i /home/vscode/.ssh/fabric_local/silver -F /home/vscode/work/fabric_config/ssh_config ubuntu@132.249.252.182\n",
      "-----------------  -----------------------------------------------------------------------------------------------------------------\n",
      "wknode2_address: 10.134.132.4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    slice = fablib.get_slice(slice_id=slice_id)\n",
    "    nodes = slice.get_nodes()\n",
    "    node_dict = {}\n",
    "    address_dict = {}\n",
    "    \n",
    "    # Create node and address variables\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        node_name = node_names[i-1]\n",
    "        node_dict[node_name] = node\n",
    "        globals()[node_name] = node\n",
    "        address_dict[f\"{node_name}_address\"] = None\n",
    "        globals()[f\"{node_name}_address\"] = None\n",
    "        print(f\"{node_name}: {node}\")\n",
    "\n",
    "    # Get network info\n",
    "    network = slice.get_network(name=network_name)\n",
    "    network_available_ips = network.get_available_ips()\n",
    "    print(f\"Network: {network}\")\n",
    "    \n",
    "    # Configure IPs for all nodes\n",
    "    for node_name in node_names:\n",
    "        node = node_dict[node_name]\n",
    "        node_iface = node.get_interface(network_name=network_name)\n",
    "        \n",
    "        # Assign and store IP address\n",
    "        node_address = network_available_ips.pop(0)\n",
    "        node_iface.ip_addr_add(addr=node_address, subnet=network.get_subnet())\n",
    "        address_dict[f\"{node_name}_address\"] = node_address\n",
    "        globals()[f\"{node_name}_address\"] = node_address\n",
    "        \n",
    "        # Print node config\n",
    "        print(f\"\\nConfiguration for {node_name}:\")\n",
    "        print(f\"Address assigned: {node_address}\")\n",
    "        stdout, _ = node.execute(f'ip addr show {node_iface.get_os_interface()}')\n",
    "        print(stdout)\n",
    "        stdout, _ = node.execute('ip route list')\n",
    "        print(stdout)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nNode and Address Summary:\")\n",
    "    for node_name in node_names:\n",
    "        print(f\"{node_name}: {globals()[node_name]}\")\n",
    "        print(f\"{node_name}_address: {globals()[f'{node_name}_address']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fail: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Start the control plane\n",
    "We follow the instructions that we have here: https://github.com/apache/openwhisk-deploy-kube/blob/master/docs/k8s-diy-ubuntu.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\n",
      "Get:3 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3431 kB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 Packages [8628 kB]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe Translation-en [5124 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [500 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3481 kB]\n",
      "Get:10 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 c-n-f Metadata [265 kB]\n",
      "Get:11 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [144 kB]\n",
      "Get:12 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse Translation-en [104 kB]\n",
      "Get:13 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 c-n-f Metadata [9136 B]\n",
      "Get:14 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3811 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [487 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1034 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [219 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [21.4 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [32.6 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [7040 B]\n",
      "Get:21 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [540 B]\n",
      "Get:22 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main Translation-en [580 kB]\n",
      "Get:23 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3637 kB]\n",
      "Get:24 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [508 kB]\n",
      "Get:25 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1257 kB]\n",
      "Get:26 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [302 kB]\n",
      "Get:27 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [28.3 kB]\n",
      "Get:28 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [35.6 kB]\n",
      "Get:29 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse Translation-en [8920 B]\n",
      "Get:30 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [612 B]\n",
      "Get:31 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [45.7 kB]\n",
      "Get:32 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main Translation-en [16.3 kB]\n",
      "Get:33 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 c-n-f Metadata [1420 B]\n",
      "Get:34 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/restricted amd64 c-n-f Metadata [116 B]\n",
      "Get:35 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [25.0 kB]\n",
      "Get:36 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe Translation-en [16.3 kB]\n",
      "Get:37 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 c-n-f Metadata [880 B]\n",
      "Get:38 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/multiverse amd64 c-n-f Metadata [116 B]\n",
      "Fetched 34.4 MB in 5s (6435 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.68.0-1ubuntu2.25).\n",
      "curl set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  bridge-utils containerd dns-root-data dnsmasq-base libidn11 pigz runc\n",
      "  ubuntu-fan\n",
      "Suggested packages:\n",
      "  ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-buildx\n",
      "  docker-compose-v2 docker-doc rinse zfs-fuse | zfsutils\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https bridge-utils containerd dns-root-data dnsmasq-base\n",
      "  docker.io libidn11 pigz runc ubuntu-fan\n",
      "0 upgraded, 10 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 71.6 MB of archives.\n",
      "After this operation, 301 MB of additional disk space will be used.\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.10 [1704 B]\n",
      "Get:3 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 bridge-utils amd64 1.6-2ubuntu1 [30.5 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 runc amd64 1.1.12-0ubuntu2~20.04.1 [8066 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 containerd amd64 1.7.24-0ubuntu1~20.04.1 [33.6 MB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dns-root-data all 2024071801~ubuntu0.20.04.1 [6128 B]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 libidn11 amd64 1.33-2.2ubuntu2 [46.2 kB]\n",
      "Get:8 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dnsmasq-base amd64 2.90-0ubuntu0.20.04.1 [350 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 docker.io amd64 26.1.3-0ubuntu1~20.04.1 [29.5 MB]\n",
      "Get:10 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 ubuntu-fan all 0.12.13ubuntu0.1 [34.4 kB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 71.6 MB in 4s (19.9 MB/s)\n",
      "Selecting previously unselected package pigz.\n",
      "(Reading database ... 64203 files and directories currently installed.)\n",
      "Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\n",
      "Unpacking pigz (2.4-1) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "Preparing to unpack .../1-apt-transport-https_2.0.10_all.deb ...\n",
      "Unpacking apt-transport-https (2.0.10) ...\n",
      "Selecting previously unselected package bridge-utils.\n",
      "Preparing to unpack .../2-bridge-utils_1.6-2ubuntu1_amd64.deb ...\n",
      "Unpacking bridge-utils (1.6-2ubuntu1) ...\n",
      "Selecting previously unselected package runc.\n",
      "Preparing to unpack .../3-runc_1.1.12-0ubuntu2~20.04.1_amd64.deb ...\n",
      "Unpacking runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Selecting previously unselected package containerd.\n",
      "Preparing to unpack .../4-containerd_1.7.24-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package dns-root-data.\n",
      "Preparing to unpack .../5-dns-root-data_2024071801~ubuntu0.20.04.1_all.deb ...\n",
      "Unpacking dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package libidn11:amd64.\n",
      "Preparing to unpack .../6-libidn11_1.33-2.2ubuntu2_amd64.deb ...\n",
      "Unpacking libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Selecting previously unselected package dnsmasq-base.\n",
      "Preparing to unpack .../7-dnsmasq-base_2.90-0ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package docker.io.\n",
      "Preparing to unpack .../8-docker.io_26.1.3-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package ubuntu-fan.\n",
      "Preparing to unpack .../9-ubuntu-fan_0.12.13ubuntu0.1_all.deb ...\n",
      "Unpacking ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Setting up apt-transport-https (2.0.10) ...\n",
      "Setting up runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Setting up dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Setting up libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Setting up bridge-utils (1.6-2ubuntu1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up pigz (2.4-1) ...\n",
      "Setting up containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /lib/systemd/system/containerd.service.\n",
      "Setting up docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Adding group `docker' (GID 120) ...\n",
      "Done.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service  /lib/systemd/system/docker.service.\n",
      "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket  /lib/systemd/system/docker.socket.\n",
      "Setting up dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Setting up ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service  /lib/systemd/system/ubuntu-fan.service.\n",
      "Processing triggers for systemd (245.4-4ubuntu3.24) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.16) ...\n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "OK\n",
      "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease [1189 B]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  Packages [12.2 kB]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3814 kB]\n",
      "Get:8 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main Translation-en [581 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3641 kB]\n",
      "Get:10 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [510 kB]\n",
      "Get:11 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1257 kB]\n",
      "Fetched 10.3 MB in 2s (4178 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  conntrack cri-tools kubernetes-cni\n",
      "Suggested packages:\n",
      "  nftables\n",
      "The following NEW packages will be installed:\n",
      "  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni\n",
      "0 upgraded, 6 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 87.7 MB of archives.\n",
      "After this operation, 315 MB of additional disk space will be used.\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 conntrack amd64 1:1.4.5-2 [30.3 kB]\n",
      "Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  cri-tools 1.31.1-1.1 [15.7 MB]\n",
      "Get:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubeadm 1.31.7-1.1 [11.5 MB]\n",
      "Get:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubectl 1.31.7-1.1 [11.3 MB]\n",
      "Get:5 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubernetes-cni 1.5.1-1.1 [33.9 MB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubelet 1.31.7-1.1 [15.3 MB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 87.7 MB in 3s (28.1 MB/s)\n",
      "Selecting previously unselected package conntrack.\n",
      "(Reading database ... 64576 files and directories currently installed.)\n",
      "Preparing to unpack .../0-conntrack_1%3a1.4.5-2_amd64.deb ...\n",
      "Unpacking conntrack (1:1.4.5-2) ...\n",
      "Selecting previously unselected package cri-tools.\n",
      "Preparing to unpack .../1-cri-tools_1.31.1-1.1_amd64.deb ...\n",
      "Unpacking cri-tools (1.31.1-1.1) ...\n",
      "Selecting previously unselected package kubeadm.\n",
      "Preparing to unpack .../2-kubeadm_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubeadm (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubectl.\n",
      "Preparing to unpack .../3-kubectl_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubectl (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubernetes-cni.\n",
      "Preparing to unpack .../4-kubernetes-cni_1.5.1-1.1_amd64.deb ...\n",
      "Unpacking kubernetes-cni (1.5.1-1.1) ...\n",
      "Selecting previously unselected package kubelet.\n",
      "Preparing to unpack .../5-kubelet_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubelet (1.31.7-1.1) ...\n",
      "Setting up conntrack (1:1.4.5-2) ...\n",
      "Setting up kubectl (1.31.7-1.1) ...\n",
      "Setting up cri-tools (1.31.1-1.1) ...\n",
      "Setting up kubernetes-cni (1.5.1-1.1) ...\n",
      "Setting up kubeadm (1.31.7-1.1) ...\n",
      "Setting up kubelet (1.31.7-1.1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "kubelet set on hold.\n",
      "kubeadm set on hold.\n",
      "kubectl set on hold.\n",
      "W0313 16:44:30.841800    5579 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n",
      "[reset] Are you sure you want to proceed? [y/N]: [preflight] Running pre-flight checks\n",
      "W0313 16:44:30.841900    5579 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n",
      "[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n",
      "[reset] Stopping the kubelet service\n",
      "[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n",
      "[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\n",
      "[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n",
      "\n",
      "The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n",
      "\n",
      "The reset process does not reset or clean up iptables rules or IPVS tables.\n",
      "If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n",
      "\n",
      "If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n",
      "to reset your system's IPVS tables.\n",
      "\n",
      "The reset process does not clean your kubeconfig files and you must remove them manually.\n",
      "Please, check the contents of the $HOME/.kube/config file.\n",
      "I0313 16:44:31.470657    5605 version.go:261] remote version is much newer: v1.32.3; falling back to: stable-1.31\n",
      "[init] Using Kubernetes version: v1.31.7\n",
      "[preflight] Running pre-flight checks\n",
      "[preflight] Pulling images required for setting up a Kubernetes cluster\n",
      "[preflight] This might take a minute or two, depending on the speed of your internet connection\n",
      "[preflight] You can also perform this action beforehand using 'kubeadm config images pull'\n",
      "W0313 16:44:31.973757    5605 checks.go:846] detected that the sandbox image \"registry.k8s.io/pause:3.8\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.10\" as the CRI sandbox image.\n",
      "[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n",
      "[certs] Generating \"ca\" certificate and key\n",
      "[certs] Generating \"apiserver\" certificate and key\n",
      "[certs] apiserver serving cert is signed for DNS names [cpnode kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.134.132.2]\n",
      "[certs] Generating \"apiserver-kubelet-client\" certificate and key\n",
      "[certs] Generating \"front-proxy-ca\" certificate and key\n",
      "[certs] Generating \"front-proxy-client\" certificate and key\n",
      "[certs] Generating \"etcd/ca\" certificate and key\n",
      "[certs] Generating \"etcd/server\" certificate and key\n",
      "[certs] etcd/server serving cert is signed for DNS names [cpnode localhost] and IPs [10.134.132.2 127.0.0.1 ::1]\n",
      "[certs] Generating \"etcd/peer\" certificate and key\n",
      "[certs] etcd/peer serving cert is signed for DNS names [cpnode localhost] and IPs [10.134.132.2 127.0.0.1 ::1]\n",
      "[certs] Generating \"etcd/healthcheck-client\" certificate and key\n",
      "[certs] Generating \"apiserver-etcd-client\" certificate and key\n",
      "[certs] Generating \"sa\" key and public key\n",
      "[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n",
      "[kubeconfig] Writing \"admin.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n",
      "[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n",
      "[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n",
      "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "[kubelet-start] Starting the kubelet\n",
      "[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n",
      "[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n",
      "[kubelet-check] The kubelet is healthy after 501.14441ms\n",
      "[api-check] Waiting for a healthy API server. This can take up to 4m0s\n",
      "[api-check] The API server is healthy after 7.001828587s\n",
      "[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n",
      "[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n",
      "[upload-certs] Skipping phase. Please see --upload-certs\n",
      "[mark-control-plane] Marking the node cpnode as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n",
      "[mark-control-plane] Marking the node cpnode as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n",
      "[bootstrap-token] Using token: 90w1cf.doxb7r3u1gff3gld\n",
      "[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n",
      "[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n",
      "[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n",
      "[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n",
      "[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n",
      "[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n",
      "[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n",
      "[addons] Applied essential addon: CoreDNS\n",
      "[addons] Applied essential addon: kube-proxy\n",
      "\n",
      "Your Kubernetes control-plane has initialized successfully!\n",
      "\n",
      "To start using your cluster, you need to run the following as a regular user:\n",
      "\n",
      "  mkdir -p $HOME/.kube\n",
      "  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n",
      "  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n",
      "\n",
      "Alternatively, if you are the root user, you can run:\n",
      "\n",
      "  export KUBECONFIG=/etc/kubernetes/admin.conf\n",
      "\n",
      "You should now deploy a pod network to the cluster.\n",
      "Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n",
      "  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n",
      "\n",
      "Then you can join any number of worker nodes by running the following on each as root:\n",
      "\n",
      "kubeadm join 10.134.132.2:6443 --token 90w1cf.doxb7r3u1gff3gld \\\n",
      "\t--discovery-token-ca-cert-hash sha256:ac9278bfb9d6a12fc03f70e375d172035d6707afe0a8053f9731c9f362be5079 \n",
      "poddisruptionbudget.policy/calico-kube-controllers created\n",
      "serviceaccount/calico-kube-controllers created\n",
      "serviceaccount/calico-node created\n",
      "configmap/calico-config created\n",
      "customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\n",
      "clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\n",
      "clusterrole.rbac.authorization.k8s.io/calico-node created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/calico-node created\n",
      "daemonset.apps/calico-node created\n",
      "deployment.apps/calico-kube-controllers created\n",
      "Waiting for Calico pods to be ready...\n",
      "error: no matching resources found\n",
      "Installing calicoctl...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     00 --:--:-- --:--:-- --:--:--     0\n",
      "100 67.0M  100 67.0M    0     0  67.4M      0 --:--:-- --:--:-- --:--:-- 67.4M\n",
      "Configuring node-to-node mesh...\n",
      "Successfully applied 1 'BGPConfiguration' resource(s)\n",
      "Configuring Calico IPPool with larger CIDR...\n",
      "Successfully applied 1 'IPPool' resource(s)\n",
      "Verifying configurations...\n",
      "BGP Configuration:\n",
      "apiVersion: projectcalico.org/v3\n",
      "kind: BGPConfiguration\n",
      "metadata:\n",
      "  creationTimestamp: \"2025-03-13T16:45:10Z\"\n",
      "  name: default\n",
      "  resourceVersion: \"406\"\n",
      "  uid: 492ed7bf-d882-4677-aa0f-6cb431053bcf\n",
      "spec:\n",
      "  logSeverityScreen: Info\n",
      "  nodeToNodeMeshEnabled: true\n",
      "IPPool Configuration:\n",
      "apiVersion: projectcalico.org/v3\n",
      "items:\n",
      "- apiVersion: projectcalico.org/v3\n",
      "  kind: IPPool\n",
      "  metadata:\n",
      "    creationTimestamp: \"2025-03-13T16:45:10Z\"\n",
      "    name: default-ipv4-ippool\n",
      "    resourceVersion: \"410\"\n",
      "    uid: abaec54d-67b4-4161-bc8d-eb590bbfc77b\n",
      "  spec:\n",
      "    allowedUses:\n",
      "    - Workload\n",
      "    - Tunnel\n",
      "    blockSize: 30\n",
      "    cidr: 10.134.132.0/24\n",
      "    ipipMode: Always\n",
      "    natOutgoing: true\n",
      "    nodeSelector: all()\n",
      "    vxlanMode: Never\n",
      "kind: IPPoolList\n",
      "metadata:\n",
      "  resourceVersion: \"411\"\n",
      "Node Status:\n",
      "Need super user privileges: Operation not permitted\n",
      "NAME     STATUS     ROLES           AGE   VERSION\n",
      "cpnode   NotReady   control-plane   8s    v1.31.7\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    file_attributes = cpnode.upload_file(local_file_path=\"/workspaces/local-fabric/FabricPortal_tests/config_control_plane.sh\", remote_file_path=\"config_control_plane.sh\")\n",
    "    \n",
    "    stdout, stderr = cpnode.execute(f\"chmod +x config_control_plane.sh && ./config_control_plane.sh {network.get_subnet()} {cpnode_address}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the worker node\n",
    "Put the join command in the `join_cmd` variable. The command should look like this: ` kubeadm join <control-plane-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring wknode1...\n",
      "Uploading and executing config_worker_node.sh on wknode1...\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3431 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [500 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3481 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [487 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1034 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [219 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [21.4 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [32.6 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [7040 B]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [540 B]\n",
      "Get:15 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 Packages [8628 kB]\n",
      "Get:16 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe Translation-en [5124 kB]\n",
      "Get:17 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 c-n-f Metadata [265 kB]\n",
      "Get:18 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [144 kB]\n",
      "Get:19 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse Translation-en [104 kB]\n",
      "Get:20 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 c-n-f Metadata [9136 B]\n",
      "Get:21 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3814 kB]\n",
      "Get:22 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main Translation-en [581 kB]\n",
      "Get:23 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3641 kB]\n",
      "Get:24 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [510 kB]\n",
      "Get:25 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1257 kB]\n",
      "Get:26 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [302 kB]\n",
      "Get:27 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [28.3 kB]\n",
      "Get:28 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [35.6 kB]\n",
      "Get:29 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse Translation-en [8920 B]\n",
      "Get:30 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [612 B]\n",
      "Get:31 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [45.7 kB]\n",
      "Get:32 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main Translation-en [16.3 kB]\n",
      "Get:33 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 c-n-f Metadata [1420 B]\n",
      "Get:34 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/restricted amd64 c-n-f Metadata [116 B]\n",
      "Get:35 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [25.0 kB]\n",
      "Get:36 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe Translation-en [16.3 kB]\n",
      "Get:37 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 c-n-f Metadata [880 B]\n",
      "Get:38 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/multiverse amd64 c-n-f Metadata [116 B]\n",
      "Fetched 34.4 MB in 5s (6482 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.68.0-1ubuntu2.25).\n",
      "curl set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  bridge-utils containerd dns-root-data dnsmasq-base libidn11 pigz runc\n",
      "  ubuntu-fan\n",
      "Suggested packages:\n",
      "  ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-buildx\n",
      "  docker-compose-v2 docker-doc rinse zfs-fuse | zfsutils\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https bridge-utils containerd dns-root-data dnsmasq-base\n",
      "  docker.io libidn11 pigz runc ubuntu-fan\n",
      "0 upgraded, 10 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 71.6 MB of archives.\n",
      "After this operation, 301 MB of additional disk space will be used.\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.10 [1704 B]\n",
      "Get:3 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 bridge-utils amd64 1.6-2ubuntu1 [30.5 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 runc amd64 1.1.12-0ubuntu2~20.04.1 [8066 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 containerd amd64 1.7.24-0ubuntu1~20.04.1 [33.6 MB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dns-root-data all 2024071801~ubuntu0.20.04.1 [6128 B]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 libidn11 amd64 1.33-2.2ubuntu2 [46.2 kB]\n",
      "Get:8 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dnsmasq-base amd64 2.90-0ubuntu0.20.04.1 [350 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 docker.io amd64 26.1.3-0ubuntu1~20.04.1 [29.5 MB]\n",
      "Get:10 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 ubuntu-fan all 0.12.13ubuntu0.1 [34.4 kB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 71.6 MB in 3s (28.2 MB/s)\n",
      "Selecting previously unselected package pigz.\n",
      "(Reading database ... 64203 files and directories currently installed.)\n",
      "Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\n",
      "Unpacking pigz (2.4-1) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "Preparing to unpack .../1-apt-transport-https_2.0.10_all.deb ...\n",
      "Unpacking apt-transport-https (2.0.10) ...\n",
      "Selecting previously unselected package bridge-utils.\n",
      "Preparing to unpack .../2-bridge-utils_1.6-2ubuntu1_amd64.deb ...\n",
      "Unpacking bridge-utils (1.6-2ubuntu1) ...\n",
      "Selecting previously unselected package runc.\n",
      "Preparing to unpack .../3-runc_1.1.12-0ubuntu2~20.04.1_amd64.deb ...\n",
      "Unpacking runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Selecting previously unselected package containerd.\n",
      "Preparing to unpack .../4-containerd_1.7.24-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package dns-root-data.\n",
      "Preparing to unpack .../5-dns-root-data_2024071801~ubuntu0.20.04.1_all.deb ...\n",
      "Unpacking dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package libidn11:amd64.\n",
      "Preparing to unpack .../6-libidn11_1.33-2.2ubuntu2_amd64.deb ...\n",
      "Unpacking libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Selecting previously unselected package dnsmasq-base.\n",
      "Preparing to unpack .../7-dnsmasq-base_2.90-0ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package docker.io.\n",
      "Preparing to unpack .../8-docker.io_26.1.3-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package ubuntu-fan.\n",
      "Preparing to unpack .../9-ubuntu-fan_0.12.13ubuntu0.1_all.deb ...\n",
      "Unpacking ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Setting up apt-transport-https (2.0.10) ...\n",
      "Setting up runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Setting up dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Setting up libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Setting up bridge-utils (1.6-2ubuntu1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up pigz (2.4-1) ...\n",
      "Setting up containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /lib/systemd/system/containerd.service.\n",
      "Setting up docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Adding group `docker' (GID 120) ...\n",
      "Done.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service  /lib/systemd/system/docker.service.\n",
      "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket  /lib/systemd/system/docker.socket.\n",
      "Setting up dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Setting up ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service  /lib/systemd/system/ubuntu-fan.service.\n",
      "Processing triggers for systemd (245.4-4ubuntu3.24) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.16) ...\n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "OK\n",
      "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease [1189 B]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  Packages [12.2 kB]\n",
      "Fetched 534 kB in 1s (644 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  conntrack cri-tools kubernetes-cni\n",
      "Suggested packages:\n",
      "  nftables\n",
      "The following NEW packages will be installed:\n",
      "  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni\n",
      "0 upgraded, 6 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 87.7 MB of archives.\n",
      "After this operation, 315 MB of additional disk space will be used.\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 conntrack amd64 1:1.4.5-2 [30.3 kB]\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  cri-tools 1.31.1-1.1 [15.7 MB]\n",
      "Get:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubeadm 1.31.7-1.1 [11.5 MB]\n",
      "Get:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubectl 1.31.7-1.1 [11.3 MB]\n",
      "Get:5 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubernetes-cni 1.5.1-1.1 [33.9 MB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubelet 1.31.7-1.1 [15.3 MB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 87.7 MB in 3s (30.9 MB/s)\n",
      "Selecting previously unselected package conntrack.\n",
      "(Reading database ... 64576 files and directories currently installed.)\n",
      "Preparing to unpack .../0-conntrack_1%3a1.4.5-2_amd64.deb ...\n",
      "Unpacking conntrack (1:1.4.5-2) ...\n",
      "Selecting previously unselected package cri-tools.\n",
      "Preparing to unpack .../1-cri-tools_1.31.1-1.1_amd64.deb ...\n",
      "Unpacking cri-tools (1.31.1-1.1) ...\n",
      "Selecting previously unselected package kubeadm.\n",
      "Preparing to unpack .../2-kubeadm_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubeadm (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubectl.\n",
      "Preparing to unpack .../3-kubectl_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubectl (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubernetes-cni.\n",
      "Preparing to unpack .../4-kubernetes-cni_1.5.1-1.1_amd64.deb ...\n",
      "Unpacking kubernetes-cni (1.5.1-1.1) ...\n",
      "Selecting previously unselected package kubelet.\n",
      "Preparing to unpack .../5-kubelet_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubelet (1.31.7-1.1) ...\n",
      "Setting up conntrack (1:1.4.5-2) ...\n",
      "Setting up kubectl (1.31.7-1.1) ...\n",
      "Setting up cri-tools (1.31.1-1.1) ...\n",
      "Setting up kubernetes-cni (1.5.1-1.1) ...\n",
      "Setting up kubeadm (1.31.7-1.1) ...\n",
      "Setting up kubelet (1.31.7-1.1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "kubelet set on hold.\n",
      "kubeadm set on hold.\n",
      "kubectl set on hold.\n",
      "Resetting any previous kubeadm configuration...\n",
      "W0313 16:47:18.252377    5644 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n",
      "[reset] Are you sure you want to proceed? [y/N]: [preflight] Running pre-flight checks\n",
      "W0313 16:47:18.252790    5644 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n",
      "[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n",
      "[reset] Stopping the kubelet service\n",
      "[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n",
      "[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\n",
      "[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n",
      "\n",
      "The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n",
      "\n",
      "The reset process does not reset or clean up iptables rules or IPVS tables.\n",
      "If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n",
      "\n",
      "If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n",
      "to reset your system's IPVS tables.\n",
      "\n",
      "The reset process does not clean your kubeconfig files and you must remove them manually.\n",
      "Please, check the contents of the $HOME/.kube/config file.\n",
      "Executing join command: sudo kubeadm join 10.134.132.2:6443 --token 90w1cf.doxb7r3u1gff3gld --discovery-token-ca-cert-hash sha256:ac9278bfb9d6a12fc03f70e375d172035d6707afe0a8053f9731c9f362be5079\n",
      "[preflight] Running pre-flight checks\n",
      "[preflight] Reading configuration from the cluster...\n",
      "[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n",
      "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "[kubelet-start] Starting the kubelet\n",
      "[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n",
      "[kubelet-check] The kubelet is healthy after 501.384715ms\n",
      "[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap\n",
      "\n",
      "This node has joined the cluster:\n",
      "* Certificate signing request was sent to apiserver and a response was received.\n",
      "* The Kubelet was informed of the new secure connection details.\n",
      "\n",
      "Run 'kubectl get nodes' on the control-plane to see this node join the cluster.\n",
      "\n",
      "Config output for wknode1: \n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3431 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [500 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3481 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [487 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1034 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [219 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [21.4 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [32.6 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [7040 B]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [540 B]\n",
      "Get:15 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 Packages [8628 kB]\n",
      "Get:16 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe Translation-en [5124 kB]\n",
      "Get:17 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 c-n-f Metadata [265 kB]\n",
      "Get:18 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [144 kB]\n",
      "Get:19 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse Translation-en [104 kB]\n",
      "Get:20 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 c-n-f Metadata [9136 B]\n",
      "Get:21 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3814 kB]\n",
      "Get:22 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main Translation-en [581 kB]\n",
      "Get:23 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3641 kB]\n",
      "Get:24 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [510 kB]\n",
      "Get:25 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1257 kB]\n",
      "Get:26 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [302 kB]\n",
      "Get:27 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [28.3 kB]\n",
      "Get:28 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [35.6 kB]\n",
      "Get:29 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse Translation-en [8920 B]\n",
      "Get:30 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [612 B]\n",
      "Get:31 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [45.7 kB]\n",
      "Get:32 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main Translation-en [16.3 kB]\n",
      "Get:33 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 c-n-f Metadata [1420 B]\n",
      "Get:34 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/restricted amd64 c-n-f Metadata [116 B]\n",
      "Get:35 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [25.0 kB]\n",
      "Get:36 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe Translation-en [16.3 kB]\n",
      "Get:37 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 c-n-f Metadata [880 B]\n",
      "Get:38 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/multiverse amd64 c-n-f Metadata [116 B]\n",
      "Fetched 34.4 MB in 5s (6482 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.68.0-1ubuntu2.25).\n",
      "curl set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  bridge-utils containerd dns-root-data dnsmasq-base libidn11 pigz runc\n",
      "  ubuntu-fan\n",
      "Suggested packages:\n",
      "  ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-buildx\n",
      "  docker-compose-v2 docker-doc rinse zfs-fuse | zfsutils\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https bridge-utils containerd dns-root-data dnsmasq-base\n",
      "  docker.io libidn11 pigz runc ubuntu-fan\n",
      "0 upgraded, 10 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 71.6 MB of archives.\n",
      "After this operation, 301 MB of additional disk space will be used.\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.10 [1704 B]\n",
      "Get:3 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 bridge-utils amd64 1.6-2ubuntu1 [30.5 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 runc amd64 1.1.12-0ubuntu2~20.04.1 [8066 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 containerd amd64 1.7.24-0ubuntu1~20.04.1 [33.6 MB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dns-root-data all 2024071801~ubuntu0.20.04.1 [6128 B]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 libidn11 amd64 1.33-2.2ubuntu2 [46.2 kB]\n",
      "Get:8 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dnsmasq-base amd64 2.90-0ubuntu0.20.04.1 [350 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 docker.io amd64 26.1.3-0ubuntu1~20.04.1 [29.5 MB]\n",
      "Get:10 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 ubuntu-fan all 0.12.13ubuntu0.1 [34.4 kB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 71.6 MB in 3s (28.2 MB/s)\n",
      "Selecting previously unselected package pigz.\n",
      "(Reading database ... 64203 files and directories currently installed.)\n",
      "Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\n",
      "Unpacking pigz (2.4-1) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "Preparing to unpack .../1-apt-transport-https_2.0.10_all.deb ...\n",
      "Unpacking apt-transport-https (2.0.10) ...\n",
      "Selecting previously unselected package bridge-utils.\n",
      "Preparing to unpack .../2-bridge-utils_1.6-2ubuntu1_amd64.deb ...\n",
      "Unpacking bridge-utils (1.6-2ubuntu1) ...\n",
      "Selecting previously unselected package runc.\n",
      "Preparing to unpack .../3-runc_1.1.12-0ubuntu2~20.04.1_amd64.deb ...\n",
      "Unpacking runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Selecting previously unselected package containerd.\n",
      "Preparing to unpack .../4-containerd_1.7.24-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package dns-root-data.\n",
      "Preparing to unpack .../5-dns-root-data_2024071801~ubuntu0.20.04.1_all.deb ...\n",
      "Unpacking dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package libidn11:amd64.\n",
      "Preparing to unpack .../6-libidn11_1.33-2.2ubuntu2_amd64.deb ...\n",
      "Unpacking libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Selecting previously unselected package dnsmasq-base.\n",
      "Preparing to unpack .../7-dnsmasq-base_2.90-0ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package docker.io.\n",
      "Preparing to unpack .../8-docker.io_26.1.3-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package ubuntu-fan.\n",
      "Preparing to unpack .../9-ubuntu-fan_0.12.13ubuntu0.1_all.deb ...\n",
      "Unpacking ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Setting up apt-transport-https (2.0.10) ...\n",
      "Setting up runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Setting up dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Setting up libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Setting up bridge-utils (1.6-2ubuntu1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up pigz (2.4-1) ...\n",
      "Setting up containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /lib/systemd/system/containerd.service.\n",
      "Setting up docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Adding group `docker' (GID 120) ...\n",
      "Done.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service  /lib/systemd/system/docker.service.\n",
      "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket  /lib/systemd/system/docker.socket.\n",
      "Setting up dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Setting up ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service  /lib/systemd/system/ubuntu-fan.service.\n",
      "Processing triggers for systemd (245.4-4ubuntu3.24) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.16) ...\n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "OK\n",
      "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease [1189 B]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  Packages [12.2 kB]\n",
      "Fetched 534 kB in 1s (644 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  conntrack cri-tools kubernetes-cni\n",
      "Suggested packages:\n",
      "  nftables\n",
      "The following NEW packages will be installed:\n",
      "  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni\n",
      "0 upgraded, 6 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 87.7 MB of archives.\n",
      "After this operation, 315 MB of additional disk space will be used.\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 conntrack amd64 1:1.4.5-2 [30.3 kB]\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  cri-tools 1.31.1-1.1 [15.7 MB]\n",
      "Get:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubeadm 1.31.7-1.1 [11.5 MB]\n",
      "Get:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubectl 1.31.7-1.1 [11.3 MB]\n",
      "Get:5 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubernetes-cni 1.5.1-1.1 [33.9 MB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubelet 1.31.7-1.1 [15.3 MB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 87.7 MB in 3s (30.9 MB/s)\n",
      "Selecting previously unselected package conntrack.\n",
      "(Reading database ... 64576 files and directories currently installed.)\n",
      "Preparing to unpack .../0-conntrack_1%3a1.4.5-2_amd64.deb ...\n",
      "Unpacking conntrack (1:1.4.5-2) ...\n",
      "Selecting previously unselected package cri-tools.\n",
      "Preparing to unpack .../1-cri-tools_1.31.1-1.1_amd64.deb ...\n",
      "Unpacking cri-tools (1.31.1-1.1) ...\n",
      "Selecting previously unselected package kubeadm.\n",
      "Preparing to unpack .../2-kubeadm_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubeadm (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubectl.\n",
      "Preparing to unpack .../3-kubectl_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubectl (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubernetes-cni.\n",
      "Preparing to unpack .../4-kubernetes-cni_1.5.1-1.1_amd64.deb ...\n",
      "Unpacking kubernetes-cni (1.5.1-1.1) ...\n",
      "Selecting previously unselected package kubelet.\n",
      "Preparing to unpack .../5-kubelet_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubelet (1.31.7-1.1) ...\n",
      "Setting up conntrack (1:1.4.5-2) ...\n",
      "Setting up kubectl (1.31.7-1.1) ...\n",
      "Setting up cri-tools (1.31.1-1.1) ...\n",
      "Setting up kubernetes-cni (1.5.1-1.1) ...\n",
      "Setting up kubeadm (1.31.7-1.1) ...\n",
      "Setting up kubelet (1.31.7-1.1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "kubelet set on hold.\n",
      "kubeadm set on hold.\n",
      "kubectl set on hold.\n",
      "Resetting any previous kubeadm configuration...\n",
      "W0313 16:47:18.252377    5644 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n",
      "[reset] Are you sure you want to proceed? [y/N]: [preflight] Running pre-flight checks\n",
      "W0313 16:47:18.252790    5644 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n",
      "[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n",
      "[reset] Stopping the kubelet service\n",
      "[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n",
      "[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\n",
      "[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n",
      "\n",
      "The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n",
      "\n",
      "The reset process does not reset or clean up iptables rules or IPVS tables.\n",
      "If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n",
      "\n",
      "If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n",
      "to reset your system's IPVS tables.\n",
      "\n",
      "The reset process does not clean your kubeconfig files and you must remove them manually.\n",
      "Please, check the contents of the $HOME/.kube/config file.\n",
      "Executing join command: sudo kubeadm join 10.134.132.2:6443 --token 90w1cf.doxb7r3u1gff3gld --discovery-token-ca-cert-hash sha256:ac9278bfb9d6a12fc03f70e375d172035d6707afe0a8053f9731c9f362be5079\n",
      "[preflight] Running pre-flight checks\n",
      "[preflight] Reading configuration from the cluster...\n",
      "[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n",
      "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "[kubelet-start] Starting the kubelet\n",
      "[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n",
      "[kubelet-check] The kubelet is healthy after 501.384715ms\n",
      "[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap\n",
      "\n",
      "This node has joined the cluster:\n",
      "* Certificate signing request was sent to apiserver and a response was received.\n",
      "* The Kubelet was informed of the new secure connection details.\n",
      "\n",
      "Run 'kubectl get nodes' on the control-plane to see this node join the cluster.\n",
      "\n",
      "\n",
      "\n",
      "Configuring wknode2...\n",
      "Uploading and executing config_worker_node.sh on wknode2...\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3431 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [500 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3481 kB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [487 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1034 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [219 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [21.4 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [32.6 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [7040 B]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [540 B]\n",
      "Get:14 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:15 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 Packages [8628 kB]\n",
      "Get:16 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe Translation-en [5124 kB]\n",
      "Get:17 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 c-n-f Metadata [265 kB]\n",
      "Get:18 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [144 kB]\n",
      "Get:19 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse Translation-en [104 kB]\n",
      "Get:20 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 c-n-f Metadata [9136 B]\n",
      "Get:21 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3814 kB]\n",
      "Get:22 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main Translation-en [581 kB]\n",
      "Get:23 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3641 kB]\n",
      "Get:24 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [510 kB]\n",
      "Get:25 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1257 kB]\n",
      "Get:26 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [302 kB]\n",
      "Get:27 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [28.3 kB]\n",
      "Get:28 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [35.6 kB]\n",
      "Get:29 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse Translation-en [8920 B]\n",
      "Get:30 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [612 B]\n",
      "Get:31 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [45.7 kB]\n",
      "Get:32 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main Translation-en [16.3 kB]\n",
      "Get:33 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 c-n-f Metadata [1420 B]\n",
      "Get:34 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/restricted amd64 c-n-f Metadata [116 B]\n",
      "Get:35 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [25.0 kB]\n",
      "Get:36 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe Translation-en [16.3 kB]\n",
      "Get:37 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 c-n-f Metadata [880 B]\n",
      "Get:38 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/multiverse amd64 c-n-f Metadata [116 B]\n",
      "Fetched 34.4 MB in 5s (6622 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.68.0-1ubuntu2.25).\n",
      "curl set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  bridge-utils containerd dns-root-data dnsmasq-base libidn11 pigz runc\n",
      "  ubuntu-fan\n",
      "Suggested packages:\n",
      "  ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-buildx\n",
      "  docker-compose-v2 docker-doc rinse zfs-fuse | zfsutils\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https bridge-utils containerd dns-root-data dnsmasq-base\n",
      "  docker.io libidn11 pigz runc ubuntu-fan\n",
      "0 upgraded, 10 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 71.6 MB of archives.\n",
      "After this operation, 301 MB of additional disk space will be used.\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.10 [1704 B]\n",
      "Get:3 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 bridge-utils amd64 1.6-2ubuntu1 [30.5 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 runc amd64 1.1.12-0ubuntu2~20.04.1 [8066 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 containerd amd64 1.7.24-0ubuntu1~20.04.1 [33.6 MB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dns-root-data all 2024071801~ubuntu0.20.04.1 [6128 B]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 libidn11 amd64 1.33-2.2ubuntu2 [46.2 kB]\n",
      "Get:8 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dnsmasq-base amd64 2.90-0ubuntu0.20.04.1 [350 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 docker.io amd64 26.1.3-0ubuntu1~20.04.1 [29.5 MB]\n",
      "Get:10 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 ubuntu-fan all 0.12.13ubuntu0.1 [34.4 kB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 71.6 MB in 2s (29.1 MB/s)\n",
      "Selecting previously unselected package pigz.\n",
      "(Reading database ... 64203 files and directories currently installed.)\n",
      "Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\n",
      "Unpacking pigz (2.4-1) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "Preparing to unpack .../1-apt-transport-https_2.0.10_all.deb ...\n",
      "Unpacking apt-transport-https (2.0.10) ...\n",
      "Selecting previously unselected package bridge-utils.\n",
      "Preparing to unpack .../2-bridge-utils_1.6-2ubuntu1_amd64.deb ...\n",
      "Unpacking bridge-utils (1.6-2ubuntu1) ...\n",
      "Selecting previously unselected package runc.\n",
      "Preparing to unpack .../3-runc_1.1.12-0ubuntu2~20.04.1_amd64.deb ...\n",
      "Unpacking runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Selecting previously unselected package containerd.\n",
      "Preparing to unpack .../4-containerd_1.7.24-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package dns-root-data.\n",
      "Preparing to unpack .../5-dns-root-data_2024071801~ubuntu0.20.04.1_all.deb ...\n",
      "Unpacking dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package libidn11:amd64.\n",
      "Preparing to unpack .../6-libidn11_1.33-2.2ubuntu2_amd64.deb ...\n",
      "Unpacking libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Selecting previously unselected package dnsmasq-base.\n",
      "Preparing to unpack .../7-dnsmasq-base_2.90-0ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package docker.io.\n",
      "Preparing to unpack .../8-docker.io_26.1.3-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package ubuntu-fan.\n",
      "Preparing to unpack .../9-ubuntu-fan_0.12.13ubuntu0.1_all.deb ...\n",
      "Unpacking ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Setting up apt-transport-https (2.0.10) ...\n",
      "Setting up runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Setting up dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Setting up libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Setting up bridge-utils (1.6-2ubuntu1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up pigz (2.4-1) ...\n",
      "Setting up containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /lib/systemd/system/containerd.service.\n",
      "Setting up docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Adding group `docker' (GID 120) ...\n",
      "Done.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service  /lib/systemd/system/docker.service.\n",
      "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket  /lib/systemd/system/docker.socket.\n",
      "Setting up dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Setting up ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service  /lib/systemd/system/ubuntu-fan.service.\n",
      "Processing triggers for systemd (245.4-4ubuntu3.24) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.16) ...\n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "OK\n",
      "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease [1189 B]\n",
      "Get:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  Packages [12.2 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Fetched 534 kB in 1s (376 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  conntrack cri-tools kubernetes-cni\n",
      "Suggested packages:\n",
      "  nftables\n",
      "The following NEW packages will be installed:\n",
      "  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni\n",
      "0 upgraded, 6 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 87.7 MB of archives.\n",
      "After this operation, 315 MB of additional disk space will be used.\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 conntrack amd64 1:1.4.5-2 [30.3 kB]\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  cri-tools 1.31.1-1.1 [15.7 MB]\n",
      "Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubeadm 1.31.7-1.1 [11.5 MB]\n",
      "Get:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubectl 1.31.7-1.1 [11.3 MB]\n",
      "Get:5 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubernetes-cni 1.5.1-1.1 [33.9 MB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubelet 1.31.7-1.1 [15.3 MB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 87.7 MB in 3s (32.2 MB/s)\n",
      "Selecting previously unselected package conntrack.\n",
      "(Reading database ... 64576 files and directories currently installed.)\n",
      "Preparing to unpack .../0-conntrack_1%3a1.4.5-2_amd64.deb ...\n",
      "Unpacking conntrack (1:1.4.5-2) ...\n",
      "Selecting previously unselected package cri-tools.\n",
      "Preparing to unpack .../1-cri-tools_1.31.1-1.1_amd64.deb ...\n",
      "Unpacking cri-tools (1.31.1-1.1) ...\n",
      "Selecting previously unselected package kubeadm.\n",
      "Preparing to unpack .../2-kubeadm_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubeadm (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubectl.\n",
      "Preparing to unpack .../3-kubectl_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubectl (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubernetes-cni.\n",
      "Preparing to unpack .../4-kubernetes-cni_1.5.1-1.1_amd64.deb ...\n",
      "Unpacking kubernetes-cni (1.5.1-1.1) ...\n",
      "Selecting previously unselected package kubelet.\n",
      "Preparing to unpack .../5-kubelet_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubelet (1.31.7-1.1) ...\n",
      "Setting up conntrack (1:1.4.5-2) ...\n",
      "Setting up kubectl (1.31.7-1.1) ...\n",
      "Setting up cri-tools (1.31.1-1.1) ...\n",
      "Setting up kubernetes-cni (1.5.1-1.1) ...\n",
      "Setting up kubeadm (1.31.7-1.1) ...\n",
      "Setting up kubelet (1.31.7-1.1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "kubelet set on hold.\n",
      "kubeadm set on hold.\n",
      "kubectl set on hold.\n",
      "Resetting any previous kubeadm configuration...\n",
      "W0313 16:48:10.122932    5628 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n",
      "[reset] Are you sure you want to proceed? [y/N]: [preflight] Running pre-flight checks\n",
      "W0313 16:48:10.122987    5628 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n",
      "[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n",
      "[reset] Stopping the kubelet service\n",
      "[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n",
      "[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\n",
      "[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n",
      "\n",
      "The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n",
      "\n",
      "The reset process does not reset or clean up iptables rules or IPVS tables.\n",
      "If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n",
      "\n",
      "If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n",
      "to reset your system's IPVS tables.\n",
      "\n",
      "The reset process does not clean your kubeconfig files and you must remove them manually.\n",
      "Please, check the contents of the $HOME/.kube/config file.\n",
      "Executing join command: sudo kubeadm join 10.134.132.2:6443 --token 90w1cf.doxb7r3u1gff3gld --discovery-token-ca-cert-hash sha256:ac9278bfb9d6a12fc03f70e375d172035d6707afe0a8053f9731c9f362be5079\n",
      "[preflight] Running pre-flight checks\n",
      "[preflight] Reading configuration from the cluster...\n",
      "[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n",
      "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "[kubelet-start] Starting the kubelet\n",
      "[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n",
      "[kubelet-check] The kubelet is healthy after 501.477712ms\n",
      "[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap\n",
      "\n",
      "This node has joined the cluster:\n",
      "* Certificate signing request was sent to apiserver and a response was received.\n",
      "* The Kubelet was informed of the new secure connection details.\n",
      "\n",
      "Run 'kubectl get nodes' on the control-plane to see this node join the cluster.\n",
      "\n",
      "Config output for wknode2: \n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3431 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [500 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3481 kB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [487 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1034 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [219 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [21.4 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [32.6 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [7040 B]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [540 B]\n",
      "Get:14 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:15 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 Packages [8628 kB]\n",
      "Get:16 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe Translation-en [5124 kB]\n",
      "Get:17 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 c-n-f Metadata [265 kB]\n",
      "Get:18 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [144 kB]\n",
      "Get:19 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse Translation-en [104 kB]\n",
      "Get:20 http://nova.clouds.archive.ubuntu.com/ubuntu focal/multiverse amd64 c-n-f Metadata [9136 B]\n",
      "Get:21 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3814 kB]\n",
      "Get:22 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main Translation-en [581 kB]\n",
      "Get:23 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3641 kB]\n",
      "Get:24 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [510 kB]\n",
      "Get:25 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1257 kB]\n",
      "Get:26 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [302 kB]\n",
      "Get:27 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [28.3 kB]\n",
      "Get:28 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [35.6 kB]\n",
      "Get:29 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse Translation-en [8920 B]\n",
      "Get:30 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [612 B]\n",
      "Get:31 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [45.7 kB]\n",
      "Get:32 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main Translation-en [16.3 kB]\n",
      "Get:33 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/main amd64 c-n-f Metadata [1420 B]\n",
      "Get:34 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/restricted amd64 c-n-f Metadata [116 B]\n",
      "Get:35 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [25.0 kB]\n",
      "Get:36 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe Translation-en [16.3 kB]\n",
      "Get:37 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/universe amd64 c-n-f Metadata [880 B]\n",
      "Get:38 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports/multiverse amd64 c-n-f Metadata [116 B]\n",
      "Fetched 34.4 MB in 5s (6622 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.68.0-1ubuntu2.25).\n",
      "curl set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  bridge-utils containerd dns-root-data dnsmasq-base libidn11 pigz runc\n",
      "  ubuntu-fan\n",
      "Suggested packages:\n",
      "  ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-buildx\n",
      "  docker-compose-v2 docker-doc rinse zfs-fuse | zfsutils\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https bridge-utils containerd dns-root-data dnsmasq-base\n",
      "  docker.io libidn11 pigz runc ubuntu-fan\n",
      "0 upgraded, 10 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 71.6 MB of archives.\n",
      "After this operation, 301 MB of additional disk space will be used.\n",
      "Get:1 http://nova.clouds.archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.10 [1704 B]\n",
      "Get:3 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 bridge-utils amd64 1.6-2ubuntu1 [30.5 kB]\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 runc amd64 1.1.12-0ubuntu2~20.04.1 [8066 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 containerd amd64 1.7.24-0ubuntu1~20.04.1 [33.6 MB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dns-root-data all 2024071801~ubuntu0.20.04.1 [6128 B]\n",
      "Get:7 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 libidn11 amd64 1.33-2.2ubuntu2 [46.2 kB]\n",
      "Get:8 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 dnsmasq-base amd64 2.90-0ubuntu0.20.04.1 [350 kB]\n",
      "Get:9 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/universe amd64 docker.io amd64 26.1.3-0ubuntu1~20.04.1 [29.5 MB]\n",
      "Get:10 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates/main amd64 ubuntu-fan all 0.12.13ubuntu0.1 [34.4 kB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 71.6 MB in 2s (29.1 MB/s)\n",
      "Selecting previously unselected package pigz.\n",
      "(Reading database ... 64203 files and directories currently installed.)\n",
      "Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\n",
      "Unpacking pigz (2.4-1) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "Preparing to unpack .../1-apt-transport-https_2.0.10_all.deb ...\n",
      "Unpacking apt-transport-https (2.0.10) ...\n",
      "Selecting previously unselected package bridge-utils.\n",
      "Preparing to unpack .../2-bridge-utils_1.6-2ubuntu1_amd64.deb ...\n",
      "Unpacking bridge-utils (1.6-2ubuntu1) ...\n",
      "Selecting previously unselected package runc.\n",
      "Preparing to unpack .../3-runc_1.1.12-0ubuntu2~20.04.1_amd64.deb ...\n",
      "Unpacking runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Selecting previously unselected package containerd.\n",
      "Preparing to unpack .../4-containerd_1.7.24-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package dns-root-data.\n",
      "Preparing to unpack .../5-dns-root-data_2024071801~ubuntu0.20.04.1_all.deb ...\n",
      "Unpacking dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package libidn11:amd64.\n",
      "Preparing to unpack .../6-libidn11_1.33-2.2ubuntu2_amd64.deb ...\n",
      "Unpacking libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Selecting previously unselected package dnsmasq-base.\n",
      "Preparing to unpack .../7-dnsmasq-base_2.90-0ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package docker.io.\n",
      "Preparing to unpack .../8-docker.io_26.1.3-0ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package ubuntu-fan.\n",
      "Preparing to unpack .../9-ubuntu-fan_0.12.13ubuntu0.1_all.deb ...\n",
      "Unpacking ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Setting up apt-transport-https (2.0.10) ...\n",
      "Setting up runc (1.1.12-0ubuntu2~20.04.1) ...\n",
      "Setting up dns-root-data (2024071801~ubuntu0.20.04.1) ...\n",
      "Setting up libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
      "Setting up bridge-utils (1.6-2ubuntu1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up pigz (2.4-1) ...\n",
      "Setting up containerd (1.7.24-0ubuntu1~20.04.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /lib/systemd/system/containerd.service.\n",
      "Setting up docker.io (26.1.3-0ubuntu1~20.04.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Adding group `docker' (GID 120) ...\n",
      "Done.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service  /lib/systemd/system/docker.service.\n",
      "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket  /lib/systemd/system/docker.socket.\n",
      "Setting up dnsmasq-base (2.90-0ubuntu0.20.04.1) ...\n",
      "Setting up ubuntu-fan (0.12.13ubuntu0.1) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service  /lib/systemd/system/ubuntu-fan.service.\n",
      "Processing triggers for systemd (245.4-4ubuntu3.24) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.16) ...\n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "OK\n",
      "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Get:2 http://nova.clouds.archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease [1189 B]\n",
      "Get:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  Packages [12.2 kB]\n",
      "Get:5 http://nova.clouds.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:6 http://nova.clouds.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Fetched 534 kB in 1s (376 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  conntrack cri-tools kubernetes-cni\n",
      "Suggested packages:\n",
      "  nftables\n",
      "The following NEW packages will be installed:\n",
      "  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni\n",
      "0 upgraded, 6 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 87.7 MB of archives.\n",
      "After this operation, 315 MB of additional disk space will be used.\n",
      "Get:4 http://nova.clouds.archive.ubuntu.com/ubuntu focal/main amd64 conntrack amd64 1:1.4.5-2 [30.3 kB]\n",
      "Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  cri-tools 1.31.1-1.1 [15.7 MB]\n",
      "Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubeadm 1.31.7-1.1 [11.5 MB]\n",
      "Get:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubectl 1.31.7-1.1 [11.3 MB]\n",
      "Get:5 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubernetes-cni 1.5.1-1.1 [33.9 MB]\n",
      "Get:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubelet 1.31.7-1.1 [15.3 MB]\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Fetched 87.7 MB in 3s (32.2 MB/s)\n",
      "Selecting previously unselected package conntrack.\n",
      "(Reading database ... 64576 files and directories currently installed.)\n",
      "Preparing to unpack .../0-conntrack_1%3a1.4.5-2_amd64.deb ...\n",
      "Unpacking conntrack (1:1.4.5-2) ...\n",
      "Selecting previously unselected package cri-tools.\n",
      "Preparing to unpack .../1-cri-tools_1.31.1-1.1_amd64.deb ...\n",
      "Unpacking cri-tools (1.31.1-1.1) ...\n",
      "Selecting previously unselected package kubeadm.\n",
      "Preparing to unpack .../2-kubeadm_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubeadm (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubectl.\n",
      "Preparing to unpack .../3-kubectl_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubectl (1.31.7-1.1) ...\n",
      "Selecting previously unselected package kubernetes-cni.\n",
      "Preparing to unpack .../4-kubernetes-cni_1.5.1-1.1_amd64.deb ...\n",
      "Unpacking kubernetes-cni (1.5.1-1.1) ...\n",
      "Selecting previously unselected package kubelet.\n",
      "Preparing to unpack .../5-kubelet_1.31.7-1.1_amd64.deb ...\n",
      "Unpacking kubelet (1.31.7-1.1) ...\n",
      "Setting up conntrack (1:1.4.5-2) ...\n",
      "Setting up kubectl (1.31.7-1.1) ...\n",
      "Setting up cri-tools (1.31.1-1.1) ...\n",
      "Setting up kubernetes-cni (1.5.1-1.1) ...\n",
      "Setting up kubeadm (1.31.7-1.1) ...\n",
      "Setting up kubelet (1.31.7-1.1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "kubelet set on hold.\n",
      "kubeadm set on hold.\n",
      "kubectl set on hold.\n",
      "Resetting any previous kubeadm configuration...\n",
      "W0313 16:48:10.122932    5628 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n",
      "[reset] Are you sure you want to proceed? [y/N]: [preflight] Running pre-flight checks\n",
      "W0313 16:48:10.122987    5628 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n",
      "[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n",
      "[reset] Stopping the kubelet service\n",
      "[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n",
      "[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\n",
      "[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n",
      "\n",
      "The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n",
      "\n",
      "The reset process does not reset or clean up iptables rules or IPVS tables.\n",
      "If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n",
      "\n",
      "If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n",
      "to reset your system's IPVS tables.\n",
      "\n",
      "The reset process does not clean your kubeconfig files and you must remove them manually.\n",
      "Please, check the contents of the $HOME/.kube/config file.\n",
      "Executing join command: sudo kubeadm join 10.134.132.2:6443 --token 90w1cf.doxb7r3u1gff3gld --discovery-token-ca-cert-hash sha256:ac9278bfb9d6a12fc03f70e375d172035d6707afe0a8053f9731c9f362be5079\n",
      "[preflight] Running pre-flight checks\n",
      "[preflight] Reading configuration from the cluster...\n",
      "[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n",
      "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "[kubelet-start] Starting the kubelet\n",
      "[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n",
      "[kubelet-check] The kubelet is healthy after 501.477712ms\n",
      "[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap\n",
      "\n",
      "This node has joined the cluster:\n",
      "* Certificate signing request was sent to apiserver and a response was received.\n",
      "* The Kubelet was informed of the new secure connection details.\n",
      "\n",
      "Run 'kubectl get nodes' on the control-plane to see this node join the cluster.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_cmd = \"sudo kubeadm join 10.134.132.2:6443 --token 90w1cf.doxb7r3u1gff3gld \\\n",
    "\t--discovery-token-ca-cert-hash sha256:ac9278bfb9d6a12fc03f70e375d172035d6707afe0a8053f9731c9f362be5079\"\n",
    "try:\n",
    "    # Loop through worker nodes in node_dict\n",
    "    for node_name, node in node_dict.items():\n",
    "        if node_name == 'cpnode':  # Skip control plane node\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nConfiguring {node_name}...\")\n",
    "        \n",
    "        # Upload and execute config script\n",
    "        try:\n",
    "            print(f\"Uploading and executing config_worker_node.sh on {node_name}...\")\n",
    "            file_attributes = node.upload_file(\n",
    "                local_file_path=\"/workspaces/local-fabric/FabricPortal_tests/config_worker_node.sh\", \n",
    "                remote_file_path=\"config_worker_node.sh\"\n",
    "            )\n",
    "            exec_cmd = f\"chmod +x config_worker_node.sh && ./config_worker_node.sh {join_cmd}\"\n",
    "            stdout, stderr = node.execute(exec_cmd)\n",
    "            print(f\"Config output for {node_name}:\", stdout)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to configure {node_name}: {e}\")\n",
    "            continue  # Skip to next node if configuration fails\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Main exception: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting nodes...\n",
      "NAME      STATUS     ROLES           AGE     VERSION\n",
      "cpnode    Ready      control-plane   2m41s   v1.31.7\n",
      "wknode1   Ready      <none>          72s     v1.31.7\n",
      "wknode2   NotReady   <none>          20s     v1.31.7\n",
      "Nodes:\n",
      "NAME      STATUS     ROLES           AGE     VERSION\n",
      "cpnode    Ready      control-plane   2m41s   v1.31.7\n",
      "wknode1   Ready      <none>          72s     v1.31.7\n",
      "wknode2   NotReady   <none>          20s     v1.31.7\n",
      "\n",
      "Getting all kubernetes resources...\n",
      "NAMESPACE     NAME                                           READY   STATUS     RESTARTS   AGE\n",
      "kube-system   pod/calico-kube-controllers-6879d4fcdc-tzcth   1/1     Running    0          2m35s\n",
      "kube-system   pod/calico-node-bf4b2                          0/1     Init:1/3   0          21s\n",
      "kube-system   pod/calico-node-d8djr                          1/1     Running    0          2m35s\n",
      "kube-system   pod/calico-node-rxxmr                          1/1     Running    0          74s\n",
      "kube-system   pod/coredns-7c65d6cfc9-dxjjh                   1/1     Running    0          2m35s\n",
      "kube-system   pod/coredns-7c65d6cfc9-rqhx7                   1/1     Running    0          2m35s\n",
      "kube-system   pod/etcd-cpnode                                1/1     Running    0          2m40s\n",
      "kube-system   pod/kube-apiserver-cpnode                      1/1     Running    0          2m40s\n",
      "kube-system   pod/kube-controller-manager-cpnode             1/1     Running    0          2m42s\n",
      "kube-system   pod/kube-proxy-7wsg4                           1/1     Running    0          74s\n",
      "kube-system   pod/kube-proxy-hlvhr                           1/1     Running    0          21s\n",
      "kube-system   pod/kube-proxy-q7vsz                           1/1     Running    0          2m35s\n",
      "kube-system   pod/kube-scheduler-cpnode                      1/1     Running    0          2m40s\n",
      "\n",
      "NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\n",
      "default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  2m42s\n",
      "kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   2m41s\n",
      "\n",
      "NAMESPACE     NAME                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "kube-system   daemonset.apps/calico-node   3         3         2       3            2           kubernetes.io/os=linux   2m38s\n",
      "kube-system   daemonset.apps/kube-proxy    3         3         3       3            3           kubernetes.io/os=linux   2m41s\n",
      "\n",
      "NAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "kube-system   deployment.apps/calico-kube-controllers   1/1     1            1           2m38s\n",
      "kube-system   deployment.apps/coredns                   2/2     2            2           2m41s\n",
      "\n",
      "NAMESPACE     NAME                                                 DESIRED   CURRENT   READY   AGE\n",
      "kube-system   replicaset.apps/calico-kube-controllers-6879d4fcdc   1         1         1       2m35s\n",
      "kube-system   replicaset.apps/coredns-7c65d6cfc9                   2         2         2       2m35s\n",
      "All resources:\n",
      "NAMESPACE     NAME                                           READY   STATUS     RESTARTS   AGE\n",
      "kube-system   pod/calico-kube-controllers-6879d4fcdc-tzcth   1/1     Running    0          2m35s\n",
      "kube-system   pod/calico-node-bf4b2                          0/1     Init:1/3   0          21s\n",
      "kube-system   pod/calico-node-d8djr                          1/1     Running    0          2m35s\n",
      "kube-system   pod/calico-node-rxxmr                          1/1     Running    0          74s\n",
      "kube-system   pod/coredns-7c65d6cfc9-dxjjh                   1/1     Running    0          2m35s\n",
      "kube-system   pod/coredns-7c65d6cfc9-rqhx7                   1/1     Running    0          2m35s\n",
      "kube-system   pod/etcd-cpnode                                1/1     Running    0          2m40s\n",
      "kube-system   pod/kube-apiserver-cpnode                      1/1     Running    0          2m40s\n",
      "kube-system   pod/kube-controller-manager-cpnode             1/1     Running    0          2m42s\n",
      "kube-system   pod/kube-proxy-7wsg4                           1/1     Running    0          74s\n",
      "kube-system   pod/kube-proxy-hlvhr                           1/1     Running    0          21s\n",
      "kube-system   pod/kube-proxy-q7vsz                           1/1     Running    0          2m35s\n",
      "kube-system   pod/kube-scheduler-cpnode                      1/1     Running    0          2m40s\n",
      "\n",
      "NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\n",
      "default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  2m42s\n",
      "kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   2m41s\n",
      "\n",
      "NAMESPACE     NAME                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "kube-system   daemonset.apps/calico-node   3         3         2       3            2           kubernetes.io/os=linux   2m38s\n",
      "kube-system   daemonset.apps/kube-proxy    3         3         3       3            3           kubernetes.io/os=linux   2m41s\n",
      "\n",
      "NAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "kube-system   deployment.apps/calico-kube-controllers   1/1     1            1           2m38s\n",
      "kube-system   deployment.apps/coredns                   2/2     2            2           2m41s\n",
      "\n",
      "NAMESPACE     NAME                                                 DESIRED   CURRENT   READY   AGE\n",
      "kube-system   replicaset.apps/calico-kube-controllers-6879d4fcdc   1         1         1       2m35s\n",
      "kube-system   replicaset.apps/coredns-7c65d6cfc9                   2         2         2       2m35s\n",
      "\n",
      "\n",
      "Getting detailed pods status across all namespaces...\n",
      "NAMESPACE     NAME                                       READY   STATUS     RESTARTS   AGE     IP               NODE      NOMINATED NODE   READINESS GATES\n",
      "kube-system   calico-kube-controllers-6879d4fcdc-tzcth   1/1     Running    0          2m36s   10.134.132.231   cpnode    <none>           <none>\n",
      "kube-system   calico-node-bf4b2                          0/1     Init:1/3   0          22s     10.20.4.135      wknode2   <none>           <none>\n",
      "kube-system   calico-node-d8djr                          1/1     Running    0          2m36s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   calico-node-rxxmr                          1/1     Running    0          75s     10.20.4.189      wknode1   <none>           <none>\n",
      "kube-system   coredns-7c65d6cfc9-dxjjh                   1/1     Running    0          2m36s   10.134.132.229   cpnode    <none>           <none>\n",
      "kube-system   coredns-7c65d6cfc9-rqhx7                   1/1     Running    0          2m36s   10.134.132.230   cpnode    <none>           <none>\n",
      "kube-system   etcd-cpnode                                1/1     Running    0          2m41s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-apiserver-cpnode                      1/1     Running    0          2m41s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-controller-manager-cpnode             1/1     Running    0          2m43s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-proxy-7wsg4                           1/1     Running    0          75s     10.20.4.189      wknode1   <none>           <none>\n",
      "kube-system   kube-proxy-hlvhr                           1/1     Running    0          22s     10.20.4.135      wknode2   <none>           <none>\n",
      "kube-system   kube-proxy-q7vsz                           1/1     Running    0          2m36s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-scheduler-cpnode                      1/1     Running    0          2m41s   10.20.5.188      cpnode    <none>           <none>\n",
      "Detailed pods status:\n",
      "NAMESPACE     NAME                                       READY   STATUS     RESTARTS   AGE     IP               NODE      NOMINATED NODE   READINESS GATES\n",
      "kube-system   calico-kube-controllers-6879d4fcdc-tzcth   1/1     Running    0          2m36s   10.134.132.231   cpnode    <none>           <none>\n",
      "kube-system   calico-node-bf4b2                          0/1     Init:1/3   0          22s     10.20.4.135      wknode2   <none>           <none>\n",
      "kube-system   calico-node-d8djr                          1/1     Running    0          2m36s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   calico-node-rxxmr                          1/1     Running    0          75s     10.20.4.189      wknode1   <none>           <none>\n",
      "kube-system   coredns-7c65d6cfc9-dxjjh                   1/1     Running    0          2m36s   10.134.132.229   cpnode    <none>           <none>\n",
      "kube-system   coredns-7c65d6cfc9-rqhx7                   1/1     Running    0          2m36s   10.134.132.230   cpnode    <none>           <none>\n",
      "kube-system   etcd-cpnode                                1/1     Running    0          2m41s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-apiserver-cpnode                      1/1     Running    0          2m41s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-controller-manager-cpnode             1/1     Running    0          2m43s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-proxy-7wsg4                           1/1     Running    0          75s     10.20.4.189      wknode1   <none>           <none>\n",
      "kube-system   kube-proxy-hlvhr                           1/1     Running    0          22s     10.20.4.135      wknode2   <none>           <none>\n",
      "kube-system   kube-proxy-q7vsz                           1/1     Running    0          2m36s   10.20.5.188      cpnode    <none>           <none>\n",
      "kube-system   kube-scheduler-cpnode                      1/1     Running    0          2m41s   10.20.5.188      cpnode    <none>           <none>\n",
      "\n",
      "\n",
      "Describing kube-system pods...\n",
      "Name:                 calico-kube-controllers-6879d4fcdc-tzcth\n",
      "Namespace:            kube-system\n",
      "Priority:             2000000000\n",
      "Priority Class Name:  system-cluster-critical\n",
      "Service Account:      calico-kube-controllers\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:35 +0000\n",
      "Labels:               k8s-app=calico-kube-controllers\n",
      "                      pod-template-hash=6879d4fcdc\n",
      "Annotations:          cni.projectcalico.org/containerID: f6e907900d79c586b0c881d560ed226064cbe2a03b50d45f713cd208695a0590\n",
      "                      cni.projectcalico.org/podIP: 10.134.132.231/32\n",
      "                      cni.projectcalico.org/podIPs: 10.134.132.231/32\n",
      "Status:               Running\n",
      "IP:                   10.134.132.231\n",
      "IPs:\n",
      "  IP:           10.134.132.231\n",
      "Controlled By:  ReplicaSet/calico-kube-controllers-6879d4fcdc\n",
      "Containers:\n",
      "  calico-kube-controllers:\n",
      "    Container ID:   containerd://d0f7dbe3f163ee66b1df5986b08e861f9bc206f2cf36414c9aed957255662a12\n",
      "    Image:          docker.io/calico/kube-controllers:v3.25.0\n",
      "    Image ID:       docker.io/calico/kube-controllers@sha256:c45af3a9692d87a527451cf544557138fedf86f92b6e39bf2003e2fdb848dce3\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:49 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Liveness:       exec [/usr/bin/check-status -l] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:      exec [/usr/bin/check-status -r] delay=0s timeout=1s period=10s #success=1 #failure=3\n",
      "    Environment:\n",
      "      ENABLED_CONTROLLERS:  node\n",
      "      DATASTORE_TYPE:       kubernetes\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xkm6v (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-api-access-xkm6v:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 CriticalAddonsOnly op=Exists\n",
      "                             node-role.kubernetes.io/control-plane:NoSchedule\n",
      "                             node-role.kubernetes.io/master:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason                  Age                    From               Message\n",
      "  ----     ------                  ----                   ----               -------\n",
      "  Warning  FailedScheduling        2m38s                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.\n",
      "  Normal   Scheduled               2m30s                  default-scheduler  Successfully assigned kube-system/calico-kube-controllers-6879d4fcdc-tzcth to cpnode\n",
      "  Warning  FailedCreatePodSandBox  2m29s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"15b18c3ffaa1a3ff406107d2138f6a1af7e3c64aba271ad899049acb08d3daad\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m28s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"07e93cb09bb4a5b35ca95fc231cdf73760998c5ff14e2742af82301db54ac8fc\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m27s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"f90af45fa73fe25bdfb2334c88db6b58cc9ab585acf7ea724903a37753b5fe7a\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m26s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"9ede3d079c3a9abc9b916b674d9a9d36eaf04a0ce90563a9625b6ff331466914\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m25s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"f85b79e945001b84a62af67f7614775efd1a361813aa30f97941f647ef715468\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m24s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"0fc181a139cdc629e5d8906ac8db143fe1ba328003998e93137d944030d054c3\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m23s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"81bc234ffef10fc44e1b2f13f12cbe9e17de12b6528b63ee63062078a373f907\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m22s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"db02642c21906ac53ca5471084fbf464e9083c5e411cbd3c095c2ca0d784aa77\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m21s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"9dc9a6d4e348bd39c60105fc0529b15bf997f2bcca50bd63378bcd1e38570797\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Normal   SandboxChanged          2m20s (x9 over 2m29s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n",
      "  Normal   Pulling                 2m20s                  kubelet            Pulling image \"docker.io/calico/kube-controllers:v3.25.0\"\n",
      "  Normal   Pulled                  2m16s                  kubelet            Successfully pulled image \"docker.io/calico/kube-controllers:v3.25.0\" in 3.935s (3.935s including waiting). Image size: 31271800 bytes.\n",
      "  Normal   Created                 2m16s                  kubelet            Created container: calico-kube-controllers\n",
      "  Normal   Started                 2m16s                  kubelet            Started container calico-kube-controllers\n",
      "\n",
      "\n",
      "Name:                 calico-node-bf4b2\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      calico-node\n",
      "Node:                 wknode2/10.20.4.135\n",
      "Start Time:           Thu, 13 Mar 2025 16:05:42 +0000\n",
      "Labels:               controller-revision-hash=645686658b\n",
      "                      k8s-app=calico-node\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Pending\n",
      "IP:                   10.20.4.135\n",
      "IPs:\n",
      "  IP:           10.20.4.135\n",
      "Controlled By:  DaemonSet/calico-node\n",
      "Init Containers:\n",
      "  upgrade-ipam:\n",
      "    Container ID:  containerd://df5deabbeab259f7ad989cb1365bfe7db9daba3427228ae3a024a0d487850fcc\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/calico-ipam\n",
      "      -upgrade\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:05:50 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:05:50 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "    Mounts:\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/lib/cni/networks from host-local-net-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "  install-cni:\n",
      "    Container ID:  containerd://6b81772386d1b675d67dba19a776e8eeaab002d42ab3d7c52892939c343fbb00\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/install\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:51 +0000\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      CNI_CONF_NAME:         10-calico.conflist\n",
      "      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false\n",
      "      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)\n",
      "      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      SLEEP:                 false\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "  mount-bpffs:\n",
      "    Container ID:  \n",
      "    Image:         docker.io/calico/node:v3.25.0\n",
      "    Image ID:      \n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      calico-node\n",
      "      -init\n",
      "      -best-effort\n",
      "    State:          Waiting\n",
      "      Reason:       PodInitializing\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /nodeproc from nodeproc (ro)\n",
      "      /sys/fs from sys-fs (rw)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "Containers:\n",
      "  calico-node:\n",
      "    Container ID:   \n",
      "    Image:          docker.io/calico/node:v3.25.0\n",
      "    Image ID:       \n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Waiting\n",
      "      Reason:       PodInitializing\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:      250m\n",
      "    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=10s period=10s #success=1 #failure=3\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      DATASTORE_TYPE:                     kubernetes\n",
      "      WAIT_FOR_DATASTORE:                 true\n",
      "      NODENAME:                            (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "      CLUSTER_TYPE:                       k8s,bgp\n",
      "      IP:                                 autodetect\n",
      "      CALICO_IPV4POOL_IPIP:               Always\n",
      "      CALICO_IPV4POOL_VXLAN:              Never\n",
      "      CALICO_IPV6POOL_VXLAN:              Never\n",
      "      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_VXLANMTU:                     <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_WIREGUARDMTU:                 <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      CALICO_DISABLE_FILE_LOGGING:        true\n",
      "      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\n",
      "      FELIX_IPV6SUPPORT:                  false\n",
      "      FELIX_HEALTHENABLED:                true\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /sys/fs/bpf from bpffs (rw)\n",
      "      /var/lib/calico from var-lib-calico (rw)\n",
      "      /var/log/calico/cni from cni-log-dir (ro)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/nodeagent from policysync (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 False \n",
      "  Ready                       False \n",
      "  ContainersReady             False \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  var-run-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/calico\n",
      "    HostPathType:  \n",
      "  var-lib-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/calico\n",
      "    HostPathType:  \n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  sys-fs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  bpffs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/bpf\n",
      "    HostPathType:  Directory\n",
      "  nodeproc:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /proc\n",
      "    HostPathType:  \n",
      "  cni-bin-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/cni/bin\n",
      "    HostPathType:  \n",
      "  cni-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/cni/net.d\n",
      "    HostPathType:  \n",
      "  cni-log-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/calico/cni\n",
      "    HostPathType:  \n",
      "  host-local-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/cni/networks\n",
      "    HostPathType:  \n",
      "  policysync:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/nodeagent\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kube-api-access-bpmxc:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 :NoSchedule op=Exists\n",
      "                             :NoExecute op=Exists\n",
      "                             CriticalAddonsOnly op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  24s   default-scheduler  Successfully assigned kube-system/calico-node-bf4b2 to wknode2\n",
      "  Normal  Pulling    20s   kubelet            Pulling image \"docker.io/calico/cni:v3.25.0\"\n",
      "  Normal  Pulled     15s   kubelet            Successfully pulled image \"docker.io/calico/cni:v3.25.0\" in 5.35s (5.35s including waiting). Image size: 87984941 bytes.\n",
      "  Normal  Created    15s   kubelet            Created container: upgrade-ipam\n",
      "  Normal  Started    15s   kubelet            Started container upgrade-ipam\n",
      "  Normal  Pulled     14s   kubelet            Container image \"docker.io/calico/cni:v3.25.0\" already present on machine\n",
      "  Normal  Created    14s   kubelet            Created container: install-cni\n",
      "  Normal  Started    14s   kubelet            Started container install-cni\n",
      "\n",
      "\n",
      "Name:                 calico-node-d8djr\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      calico-node\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:27 +0000\n",
      "Labels:               controller-revision-hash=645686658b\n",
      "                      k8s-app=calico-node\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  DaemonSet/calico-node\n",
      "Init Containers:\n",
      "  upgrade-ipam:\n",
      "    Container ID:  containerd://7ac7ed2ba8c47c1ee2fc5930887e2c152c2f0a0ef02916f52ce157f1f13faed1\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/calico-ipam\n",
      "      -upgrade\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:03:34 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:03:34 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "    Mounts:\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/lib/cni/networks from host-local-net-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "  install-cni:\n",
      "    Container ID:  containerd://20efa0adf0a2ba45ff353d081ba6ab70838207f3937e7d15c5c8c927ca6a8acd\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/install\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:03:35 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:03:35 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      CNI_CONF_NAME:         10-calico.conflist\n",
      "      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false\n",
      "      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)\n",
      "      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      SLEEP:                 false\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "  mount-bpffs:\n",
      "    Container ID:  containerd://5425209a79963668b686636784dd2e968e006f7bdfabf22e6083b254ee7eb958\n",
      "    Image:         docker.io/calico/node:v3.25.0\n",
      "    Image ID:      docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      calico-node\n",
      "      -init\n",
      "      -best-effort\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:03:42 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:03:42 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /nodeproc from nodeproc (ro)\n",
      "      /sys/fs from sys-fs (rw)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "Containers:\n",
      "  calico-node:\n",
      "    Container ID:   containerd://663c496629fe772cb0d3b8ce41706c21e9fbb16dd5e4f016a5dc8a17c3a085d6\n",
      "    Image:          docker.io/calico/node:v3.25.0\n",
      "    Image ID:       docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:44 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:      250m\n",
      "    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=10s period=10s #success=1 #failure=3\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      DATASTORE_TYPE:                     kubernetes\n",
      "      WAIT_FOR_DATASTORE:                 true\n",
      "      NODENAME:                            (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "      CLUSTER_TYPE:                       k8s,bgp\n",
      "      IP:                                 autodetect\n",
      "      CALICO_IPV4POOL_IPIP:               Always\n",
      "      CALICO_IPV4POOL_VXLAN:              Never\n",
      "      CALICO_IPV6POOL_VXLAN:              Never\n",
      "      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_VXLANMTU:                     <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_WIREGUARDMTU:                 <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      CALICO_DISABLE_FILE_LOGGING:        true\n",
      "      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\n",
      "      FELIX_IPV6SUPPORT:                  false\n",
      "      FELIX_HEALTHENABLED:                true\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /sys/fs/bpf from bpffs (rw)\n",
      "      /var/lib/calico from var-lib-calico (rw)\n",
      "      /var/log/calico/cni from cni-log-dir (ro)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/nodeagent from policysync (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  var-run-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/calico\n",
      "    HostPathType:  \n",
      "  var-lib-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/calico\n",
      "    HostPathType:  \n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  sys-fs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  bpffs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/bpf\n",
      "    HostPathType:  Directory\n",
      "  nodeproc:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /proc\n",
      "    HostPathType:  \n",
      "  cni-bin-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/cni/bin\n",
      "    HostPathType:  \n",
      "  cni-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/cni/net.d\n",
      "    HostPathType:  \n",
      "  cni-log-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/calico/cni\n",
      "    HostPathType:  \n",
      "  host-local-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/cni/networks\n",
      "    HostPathType:  \n",
      "  policysync:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/nodeagent\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kube-api-access-qwtfk:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 :NoSchedule op=Exists\n",
      "                             :NoExecute op=Exists\n",
      "                             CriticalAddonsOnly op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type     Reason     Age                    From               Message\n",
      "  ----     ------     ----                   ----               -------\n",
      "  Normal   Scheduled  2m38s                  default-scheduler  Successfully assigned kube-system/calico-node-d8djr to cpnode\n",
      "  Normal   Pulling    2m38s                  kubelet            Pulling image \"docker.io/calico/cni:v3.25.0\"\n",
      "  Normal   Pulled     2m31s                  kubelet            Successfully pulled image \"docker.io/calico/cni:v3.25.0\" in 6.257s (6.257s including waiting). Image size: 87984941 bytes.\n",
      "  Normal   Created    2m31s                  kubelet            Created container: upgrade-ipam\n",
      "  Normal   Started    2m31s                  kubelet            Started container upgrade-ipam\n",
      "  Normal   Pulled     2m31s                  kubelet            Container image \"docker.io/calico/cni:v3.25.0\" already present on machine\n",
      "  Normal   Created    2m30s                  kubelet            Created container: install-cni\n",
      "  Normal   Started    2m30s                  kubelet            Started container install-cni\n",
      "  Normal   Pulling    2m30s                  kubelet            Pulling image \"docker.io/calico/node:v3.25.0\"\n",
      "  Normal   Pulled     2m23s                  kubelet            Successfully pulled image \"docker.io/calico/node:v3.25.0\" in 6.796s (6.796s including waiting). Image size: 87185935 bytes.\n",
      "  Normal   Created    2m23s                  kubelet            Created container: mount-bpffs\n",
      "  Normal   Started    2m23s                  kubelet            Started container mount-bpffs\n",
      "  Normal   Pulled     2m21s                  kubelet            Container image \"docker.io/calico/node:v3.25.0\" already present on machine\n",
      "  Normal   Created    2m21s                  kubelet            Created container: calico-node\n",
      "  Normal   Started    2m21s                  kubelet            Started container calico-node\n",
      "  Warning  Unhealthy  2m20s                  kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/bird/bird.ctl: connect: no such file or directory\n",
      "  Warning  Unhealthy  2m18s (x2 over 2m19s)  kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused\n",
      "  Warning  Unhealthy  28s                    kubelet            Readiness probe failed: 2025-03-13 16:05:37.855 [INFO][579] confd/health.go 180: Number of node(s) with BGP peering established = 0\n",
      "calico/node is not ready: BIRD is not ready: BGP not established with 10.134.132.3\n",
      "\n",
      "\n",
      "Name:                 calico-node-rxxmr\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      calico-node\n",
      "Node:                 wknode1/10.20.4.189\n",
      "Start Time:           Thu, 13 Mar 2025 16:04:48 +0000\n",
      "Labels:               controller-revision-hash=645686658b\n",
      "                      k8s-app=calico-node\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.4.189\n",
      "IPs:\n",
      "  IP:           10.20.4.189\n",
      "Controlled By:  DaemonSet/calico-node\n",
      "Init Containers:\n",
      "  upgrade-ipam:\n",
      "    Container ID:  containerd://e19eccbb0116d2afdb450fe4c7cd9e7d33e364cd981a19ad712b2092f042ad27\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/calico-ipam\n",
      "      -upgrade\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:04:57 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:04:57 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "    Mounts:\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/lib/cni/networks from host-local-net-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "  install-cni:\n",
      "    Container ID:  containerd://fcd96b5477d31e258fa29c0646141741cd6c6bab976e60c7be91ea8dc44a13fd\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/install\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:05:28 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:05:29 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  1\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      CNI_CONF_NAME:         10-calico.conflist\n",
      "      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false\n",
      "      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)\n",
      "      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      SLEEP:                 false\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "  mount-bpffs:\n",
      "    Container ID:  containerd://8dfcded7a69bbe2e32517cf97e875fcdf89e7e85f8a3a30648e954fce8f8fed2\n",
      "    Image:         docker.io/calico/node:v3.25.0\n",
      "    Image ID:      docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      calico-node\n",
      "      -init\n",
      "      -best-effort\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:05:34 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:05:34 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /nodeproc from nodeproc (ro)\n",
      "      /sys/fs from sys-fs (rw)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "Containers:\n",
      "  calico-node:\n",
      "    Container ID:   containerd://2d1aa7e98ad57a3a0e8d8e3a676392c52bd67bc75816adddea1fcab53bc8f374\n",
      "    Image:          docker.io/calico/node:v3.25.0\n",
      "    Image ID:       docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:35 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:      250m\n",
      "    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=10s period=10s #success=1 #failure=3\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      DATASTORE_TYPE:                     kubernetes\n",
      "      WAIT_FOR_DATASTORE:                 true\n",
      "      NODENAME:                            (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "      CLUSTER_TYPE:                       k8s,bgp\n",
      "      IP:                                 autodetect\n",
      "      CALICO_IPV4POOL_IPIP:               Always\n",
      "      CALICO_IPV4POOL_VXLAN:              Never\n",
      "      CALICO_IPV6POOL_VXLAN:              Never\n",
      "      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_VXLANMTU:                     <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_WIREGUARDMTU:                 <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      CALICO_DISABLE_FILE_LOGGING:        true\n",
      "      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\n",
      "      FELIX_IPV6SUPPORT:                  false\n",
      "      FELIX_HEALTHENABLED:                true\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /sys/fs/bpf from bpffs (rw)\n",
      "      /var/lib/calico from var-lib-calico (rw)\n",
      "      /var/log/calico/cni from cni-log-dir (ro)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/nodeagent from policysync (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  var-run-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/calico\n",
      "    HostPathType:  \n",
      "  var-lib-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/calico\n",
      "    HostPathType:  \n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  sys-fs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  bpffs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/bpf\n",
      "    HostPathType:  Directory\n",
      "  nodeproc:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /proc\n",
      "    HostPathType:  \n",
      "  cni-bin-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/cni/bin\n",
      "    HostPathType:  \n",
      "  cni-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/cni/net.d\n",
      "    HostPathType:  \n",
      "  cni-log-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/calico/cni\n",
      "    HostPathType:  \n",
      "  host-local-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/cni/networks\n",
      "    HostPathType:  \n",
      "  policysync:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/nodeagent\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kube-api-access-gbsbx:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 :NoSchedule op=Exists\n",
      "                             :NoExecute op=Exists\n",
      "                             CriticalAddonsOnly op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type     Reason     Age                From               Message\n",
      "  ----     ------     ----               ----               -------\n",
      "  Normal   Scheduled  77s                default-scheduler  Successfully assigned kube-system/calico-node-rxxmr to wknode1\n",
      "  Normal   Pulling    74s                kubelet            Pulling image \"docker.io/calico/cni:v3.25.0\"\n",
      "  Normal   Pulled     69s                kubelet            Successfully pulled image \"docker.io/calico/cni:v3.25.0\" in 5.746s (5.747s including waiting). Image size: 87984941 bytes.\n",
      "  Normal   Created    69s                kubelet            Created container: upgrade-ipam\n",
      "  Normal   Started    68s                kubelet            Started container upgrade-ipam\n",
      "  Normal   Pulled     37s (x2 over 68s)  kubelet            Container image \"docker.io/calico/cni:v3.25.0\" already present on machine\n",
      "  Normal   Created    37s (x2 over 68s)  kubelet            Created container: install-cni\n",
      "  Normal   Started    37s (x2 over 68s)  kubelet            Started container install-cni\n",
      "  Normal   Pulling    36s                kubelet            Pulling image \"docker.io/calico/node:v3.25.0\"\n",
      "  Normal   Pulled     31s                kubelet            Successfully pulled image \"docker.io/calico/node:v3.25.0\" in 4.886s (4.886s including waiting). Image size: 87185935 bytes.\n",
      "  Normal   Created    31s                kubelet            Created container: mount-bpffs\n",
      "  Normal   Started    31s                kubelet            Started container mount-bpffs\n",
      "  Normal   Pulled     30s                kubelet            Container image \"docker.io/calico/node:v3.25.0\" already present on machine\n",
      "  Normal   Created    30s                kubelet            Created container: calico-node\n",
      "  Normal   Started    30s                kubelet            Started container calico-node\n",
      "  Warning  Unhealthy  29s                kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/bird/bird.ctl: connect: no such file or directory\n",
      "  Warning  Unhealthy  28s                kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused\n",
      "\n",
      "\n",
      "Name:                 coredns-7c65d6cfc9-dxjjh\n",
      "Namespace:            kube-system\n",
      "Priority:             2000000000\n",
      "Priority Class Name:  system-cluster-critical\n",
      "Service Account:      coredns\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:35 +0000\n",
      "Labels:               k8s-app=kube-dns\n",
      "                      pod-template-hash=7c65d6cfc9\n",
      "Annotations:          cni.projectcalico.org/containerID: 514ed720c03e56244645385f8a3a8fa9c9e5330a0600b77039a6c5e4790da7e8\n",
      "                      cni.projectcalico.org/podIP: 10.134.132.229/32\n",
      "                      cni.projectcalico.org/podIPs: 10.134.132.229/32\n",
      "Status:               Running\n",
      "IP:                   10.134.132.229\n",
      "IPs:\n",
      "  IP:           10.134.132.229\n",
      "Controlled By:  ReplicaSet/coredns-7c65d6cfc9\n",
      "Containers:\n",
      "  coredns:\n",
      "    Container ID:  containerd://7132edf0eb7d5ce354657a493be26198f518e27e548a3e6e2ea8af336ea1b354\n",
      "    Image:         registry.k8s.io/coredns/coredns:v1.11.3\n",
      "    Image ID:      registry.k8s.io/coredns/coredns@sha256:9caabbf6238b189a65d0d6e6ac138de60d6a1c419e5a341fbbb7c78382559c6e\n",
      "    Ports:         53/UDP, 53/TCP, 9153/TCP\n",
      "    Host Ports:    0/UDP, 0/TCP, 0/TCP\n",
      "    Args:\n",
      "      -conf\n",
      "      /etc/coredns/Corefile\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:45 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      memory:  170Mi\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "      memory:     70Mi\n",
      "    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5\n",
      "    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/coredns from config-volume (ro)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mbttp (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  config-volume:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      coredns\n",
      "    Optional:  false\n",
      "  kube-api-access-mbttp:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 CriticalAddonsOnly op=Exists\n",
      "                             node-role.kubernetes.io/control-plane:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason                  Age                    From               Message\n",
      "  ----     ------                  ----                   ----               -------\n",
      "  Warning  FailedScheduling        2m38s                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.\n",
      "  Normal   Scheduled               2m30s                  default-scheduler  Successfully assigned kube-system/coredns-7c65d6cfc9-dxjjh to cpnode\n",
      "  Warning  FailedCreatePodSandBox  2m29s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"2564602d14ed77e242fbd23b9798a40070a89764956d8cfa9b62133b737617d8\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m28s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"6821b9a564697cc80f0a07926bc36fd0fc8fffcc67bc541b7b8d14c8e567e2c8\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m27s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"019300d47c477c16fbf9c9407b261a704ed0b489c6932d46995105d083ea1c49\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m26s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"29f92e5d714091403aaf8048b678bb8d9b15b01a32aed167caa96312f5b5903e\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m25s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"e128169e28761f9ffac661397b141e4973017d5db4e3e38526e62f20d9f9b3df\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m24s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"b8123982180e9d590539d80314908358410c319de9602908dfd6ffae5c047f24\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m23s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"77c2a2488744e0a671c4793a920ecc40c00f44060b0125f66aeadace370a0cfe\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m22s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"7558c9e6b408ff0b284412f24e0ef8b00cfa82e47953020d084a0714a023115b\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m21s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"1165f8a04c571ec6fb083eb0cbea45b229d925af7653c6ed0b112a2b0e890b3f\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Normal   SandboxChanged          2m20s (x9 over 2m29s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n",
      "  Normal   Pulled                  2m20s                  kubelet            Container image \"registry.k8s.io/coredns/coredns:v1.11.3\" already present on machine\n",
      "  Normal   Created                 2m20s                  kubelet            Created container: coredns\n",
      "  Normal   Started                 2m20s                  kubelet            Started container coredns\n",
      "\n",
      "\n",
      "Name:                 coredns-7c65d6cfc9-rqhx7\n",
      "Namespace:            kube-system\n",
      "Priority:             2000000000\n",
      "Priority Class Name:  system-cluster-critical\n",
      "Service Account:      coredns\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:35 +0000\n",
      "Labels:               k8s-app=kube-dns\n",
      "                      pod-template-hash=7c65d6cfc9\n",
      "Annotations:          cni.projectcalico.org/containerID: 1c1cc996f08f62825f9a49ec8f3b5332a7b2878afd7762c71629c96f0045bedc\n",
      "                      cni.projectcalico.org/podIP: 10.134.132.230/32\n",
      "                      cni.projectcalico.org/podIPs: 10.134.132.230/32\n",
      "Status:               Running\n",
      "IP:                   10.134.132.230\n",
      "IPs:\n",
      "  IP:           10.134.132.230\n",
      "Controlled By:  ReplicaSet/coredns-7c65d6cfc9\n",
      "Containers:\n",
      "  coredns:\n",
      "    Container ID:  containerd://bead03a4bab81c3384477cdf320a55b4d6731dde5f2924be97da5a2b5665fad6\n",
      "    Image:         registry.k8s.io/coredns/coredns:v1.11.3\n",
      "    Image ID:      registry.k8s.io/coredns/coredns@sha256:9caabbf6238b189a65d0d6e6ac138de60d6a1c419e5a341fbbb7c78382559c6e\n",
      "    Ports:         53/UDP, 53/TCP, 9153/TCP\n",
      "    Host Ports:    0/UDP, 0/TCP, 0/TCP\n",
      "    Args:\n",
      "      -conf\n",
      "      /etc/coredns/Corefile\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:46 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      memory:  170Mi\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "      memory:     70Mi\n",
      "    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5\n",
      "    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/coredns from config-volume (ro)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9qzxr (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  config-volume:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      coredns\n",
      "    Optional:  false\n",
      "  kube-api-access-9qzxr:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 CriticalAddonsOnly op=Exists\n",
      "                             node-role.kubernetes.io/control-plane:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason                  Age                    From               Message\n",
      "  ----     ------                  ----                   ----               -------\n",
      "  Warning  FailedScheduling        2m38s                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.\n",
      "  Normal   Scheduled               2m30s                  default-scheduler  Successfully assigned kube-system/coredns-7c65d6cfc9-rqhx7 to cpnode\n",
      "  Warning  FailedCreatePodSandBox  2m28s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"6a2269f4381eaf3e438585ca0dfd44d6d6b64e1e1eb5ab855fd53167b71ba1e0\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m27s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"9bdd3f38127a2416963655b6309dd5247ba639c43e6ef2f6a53be1a8de7c6f6a\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m26s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"57de4b2fda2ec39ab22256c6958728e894f84235151ad2ad9abf34b20b33fc76\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m25s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"1e37827adda58b4f0163779272a73d8dfc327e319b640e0e993bc54c00be4eaa\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m24s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"808507fd17b6ecc581ae607e648e9404df2a58caa7444195a48f9bda7b8f3c2e\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m23s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"e697b922400e02f18970186d672f0225420c46b53dbcb7272d89c3aec8c88b10\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m22s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"93f864c9511f906a0de9dbc62f3a315f86395d0352a5017020d6a3b680325a00\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m21s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"3005d6a8ba7db45c4056a7f297e3d68cdefb39937bbeea0e4ad69313baac607b\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Normal   SandboxChanged          2m20s (x8 over 2m27s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n",
      "  Normal   Pulled                  2m19s                  kubelet            Container image \"registry.k8s.io/coredns/coredns:v1.11.3\" already present on machine\n",
      "  Normal   Created                 2m19s                  kubelet            Created container: coredns\n",
      "  Normal   Started                 2m19s                  kubelet            Started container coredns\n",
      "\n",
      "\n",
      "Name:                 etcd-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=etcd\n",
      "                      tier=control-plane\n",
      "Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://10.134.132.2:2379\n",
      "                      kubernetes.io/config.hash: b38112e11521dcc07e79de28b7cb678c\n",
      "                      kubernetes.io/config.mirror: b38112e11521dcc07e79de28b7cb678c\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:21.893585826Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  etcd:\n",
      "    Container ID:  containerd://cd80ce8d1f803e13d99073f257afdfa1f2e46d3ce058de6110768fb2411f0344\n",
      "    Image:         registry.k8s.io/etcd:3.5.15-0\n",
      "    Image ID:      registry.k8s.io/etcd@sha256:a6dc63e6e8cfa0307d7851762fa6b629afb18f28d8aa3fab5a6e91b4af60026a\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      etcd\n",
      "      --advertise-client-urls=https://10.134.132.2:2379\n",
      "      --cert-file=/etc/kubernetes/pki/etcd/server.crt\n",
      "      --client-cert-auth=true\n",
      "      --data-dir=/var/lib/etcd\n",
      "      --experimental-initial-corrupt-check=true\n",
      "      --experimental-watch-progress-notify-interval=5s\n",
      "      --initial-advertise-peer-urls=https://10.134.132.2:2380\n",
      "      --initial-cluster=cpnode=https://10.134.132.2:2380\n",
      "      --key-file=/etc/kubernetes/pki/etcd/server.key\n",
      "      --listen-client-urls=https://127.0.0.1:2379,https://10.134.132.2:2379\n",
      "      --listen-metrics-urls=http://127.0.0.1:2381\n",
      "      --listen-peer-urls=https://10.134.132.2:2380\n",
      "      --name=cpnode\n",
      "      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n",
      "      --peer-client-cert-auth=true\n",
      "      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n",
      "      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n",
      "      --snapshot-count=10000\n",
      "      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "      memory:     100Mi\n",
      "    Liveness:     http-get http://127.0.0.1:2381/livez delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Readiness:    http-get http://127.0.0.1:2381/readyz delay=0s timeout=15s period=1s #success=1 #failure=3\n",
      "    Startup:      http-get http://127.0.0.1:2381/readyz delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/kubernetes/pki/etcd from etcd-certs (rw)\n",
      "      /var/lib/etcd from etcd-data (rw)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  etcd-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/pki/etcd\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  etcd-data:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/etcd\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/etcd:3.5.15-0\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: etcd\n",
      "  Normal  Started  2m48s  kubelet  Started container etcd\n",
      "\n",
      "\n",
      "Name:                 kube-apiserver-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=kube-apiserver\n",
      "                      tier=control-plane\n",
      "Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.134.132.2:6443\n",
      "                      kubernetes.io/config.hash: f807b57d31281f36c12eddb66344ee9e\n",
      "                      kubernetes.io/config.mirror: f807b57d31281f36c12eddb66344ee9e\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:21.893586628Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  kube-apiserver:\n",
      "    Container ID:  containerd://0cbeca4b8c1646c1bbeea6761503f0794ca8674da0523938d5dab7f7910d07ea\n",
      "    Image:         registry.k8s.io/kube-apiserver:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-apiserver@sha256:22c19cc70fe5806d0a2cb28a6b6b33fd34e6f9e50616bdf6d53649bcfafbc277\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      kube-apiserver\n",
      "      --advertise-address=10.134.132.2\n",
      "      --allow-privileged=true\n",
      "      --authorization-mode=Node,RBAC\n",
      "      --client-ca-file=/etc/kubernetes/pki/ca.crt\n",
      "      --enable-admission-plugins=NodeRestriction\n",
      "      --enable-bootstrap-token-auth=true\n",
      "      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n",
      "      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n",
      "      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n",
      "      --etcd-servers=https://127.0.0.1:2379\n",
      "      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n",
      "      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n",
      "      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n",
      "      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n",
      "      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n",
      "      --requestheader-allowed-names=front-proxy-client\n",
      "      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n",
      "      --requestheader-extra-headers-prefix=X-Remote-Extra-\n",
      "      --requestheader-group-headers=X-Remote-Group\n",
      "      --requestheader-username-headers=X-Remote-User\n",
      "      --secure-port=6443\n",
      "      --service-account-issuer=https://kubernetes.default.svc.cluster.local\n",
      "      --service-account-key-file=/etc/kubernetes/pki/sa.pub\n",
      "      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n",
      "      --service-cluster-ip-range=10.96.0.0/12\n",
      "      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n",
      "      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        250m\n",
      "    Liveness:     http-get https://10.134.132.2:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Readiness:    http-get https://10.134.132.2:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3\n",
      "    Startup:      http-get https://10.134.132.2:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/ca-certificates from etc-ca-certificates (ro)\n",
      "      /etc/kubernetes/pki from k8s-certs (ro)\n",
      "      /etc/ssl/certs from ca-certs (ro)\n",
      "      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)\n",
      "      /usr/share/ca-certificates from usr-share-ca-certificates (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  ca-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ssl/certs\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  etc-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  k8s-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/pki\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  usr-local-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/local/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  usr-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/kube-apiserver:v1.31.7\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: kube-apiserver\n",
      "  Normal  Started  2m48s  kubelet  Started container kube-apiserver\n",
      "\n",
      "\n",
      "Name:                 kube-controller-manager-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=kube-controller-manager\n",
      "                      tier=control-plane\n",
      "Annotations:          kubernetes.io/config.hash: aac482c30a24fd0335a50eaba5f1d92b\n",
      "                      kubernetes.io/config.mirror: aac482c30a24fd0335a50eaba5f1d92b\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:14.300503833Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  kube-controller-manager:\n",
      "    Container ID:  containerd://fa8a7922f2652fbd16fa7c5d8e010d628a3a3db30159fef0728c8487bbf55f64\n",
      "    Image:         registry.k8s.io/kube-controller-manager:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-controller-manager@sha256:6abe7a0accecf29db6ebab18a10f844678ffed693d79e2e51a18a6f2b4530cbb\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      kube-controller-manager\n",
      "      --allocate-node-cidrs=true\n",
      "      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf\n",
      "      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf\n",
      "      --bind-address=127.0.0.1\n",
      "      --client-ca-file=/etc/kubernetes/pki/ca.crt\n",
      "      --cluster-cidr=10.134.132.0/24\n",
      "      --cluster-name=kubernetes\n",
      "      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n",
      "      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n",
      "      --controllers=*,bootstrapsigner,tokencleaner\n",
      "      --kubeconfig=/etc/kubernetes/controller-manager.conf\n",
      "      --leader-elect=true\n",
      "      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n",
      "      --root-ca-file=/etc/kubernetes/pki/ca.crt\n",
      "      --service-account-private-key-file=/etc/kubernetes/pki/sa.key\n",
      "      --service-cluster-ip-range=10.96.0.0/12\n",
      "      --use-service-account-credentials=true\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        200m\n",
      "    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/ca-certificates from etc-ca-certificates (ro)\n",
      "      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)\n",
      "      /etc/kubernetes/pki from k8s-certs (ro)\n",
      "      /etc/ssl/certs from ca-certs (ro)\n",
      "      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)\n",
      "      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)\n",
      "      /usr/share/ca-certificates from usr-share-ca-certificates (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  ca-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ssl/certs\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  etc-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  flexvolume-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  k8s-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/pki\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kubeconfig:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/controller-manager.conf\n",
      "    HostPathType:  FileOrCreate\n",
      "  usr-local-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/local/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  usr-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/kube-controller-manager:v1.31.7\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: kube-controller-manager\n",
      "  Normal  Started  2m48s  kubelet  Started container kube-controller-manager\n",
      "\n",
      "\n",
      "Name:                 kube-proxy-7wsg4\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      kube-proxy\n",
      "Node:                 wknode1/10.20.4.189\n",
      "Start Time:           Thu, 13 Mar 2025 16:04:48 +0000\n",
      "Labels:               controller-revision-hash=67b77d7946\n",
      "                      k8s-app=kube-proxy\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.4.189\n",
      "IPs:\n",
      "  IP:           10.20.4.189\n",
      "Controlled By:  DaemonSet/kube-proxy\n",
      "Containers:\n",
      "  kube-proxy:\n",
      "    Container ID:  containerd://9be4d1f870bd8d2e097cce587a86d43ff2b802081ebeac2612a16c75314c5e16\n",
      "    Image:         registry.k8s.io/kube-proxy:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-proxy@sha256:e5839270c96c3ad1bea1dce4935126d3281297527f3655408d2970aa4b5cf178\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /usr/local/bin/kube-proxy\n",
      "      --config=/var/lib/kube-proxy/config.conf\n",
      "      --hostname-override=$(NODE_NAME)\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:00 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      NODE_NAME:   (v1:spec.nodeName)\n",
      "    Mounts:\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /var/lib/kube-proxy from kube-proxy (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-trpc7 (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-proxy:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      kube-proxy\n",
      "    Optional:  false\n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  kube-api-access-trpc7:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  77s   default-scheduler  Successfully assigned kube-system/kube-proxy-7wsg4 to wknode1\n",
      "  Normal  Pulling    74s   kubelet            Pulling image \"registry.k8s.io/kube-proxy:v1.31.7\"\n",
      "  Normal  Pulled     66s   kubelet            Successfully pulled image \"registry.k8s.io/kube-proxy:v1.31.7\" in 3.089s (8.826s including waiting). Image size: 30353649 bytes.\n",
      "  Normal  Created    65s   kubelet            Created container: kube-proxy\n",
      "  Normal  Started    65s   kubelet            Started container kube-proxy\n",
      "\n",
      "\n",
      "Name:                 kube-proxy-hlvhr\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      kube-proxy\n",
      "Node:                 wknode2/10.20.4.135\n",
      "Start Time:           Thu, 13 Mar 2025 16:05:42 +0000\n",
      "Labels:               controller-revision-hash=67b77d7946\n",
      "                      k8s-app=kube-proxy\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.4.135\n",
      "IPs:\n",
      "  IP:           10.20.4.135\n",
      "Controlled By:  DaemonSet/kube-proxy\n",
      "Containers:\n",
      "  kube-proxy:\n",
      "    Container ID:  containerd://0ae4b0d2480f7ef38b542256910748707c67a8471cd7954d8534624a5d286d34\n",
      "    Image:         registry.k8s.io/kube-proxy:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-proxy@sha256:e5839270c96c3ad1bea1dce4935126d3281297527f3655408d2970aa4b5cf178\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /usr/local/bin/kube-proxy\n",
      "      --config=/var/lib/kube-proxy/config.conf\n",
      "      --hostname-override=$(NODE_NAME)\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:53 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      NODE_NAME:   (v1:spec.nodeName)\n",
      "    Mounts:\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /var/lib/kube-proxy from kube-proxy (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7dwx4 (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-proxy:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      kube-proxy\n",
      "    Optional:  false\n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  kube-api-access-7dwx4:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  24s   default-scheduler  Successfully assigned kube-system/kube-proxy-hlvhr to wknode2\n",
      "  Normal  Pulling    20s   kubelet            Pulling image \"registry.k8s.io/kube-proxy:v1.31.7\"\n",
      "  Normal  Pulled     12s   kubelet            Successfully pulled image \"registry.k8s.io/kube-proxy:v1.31.7\" in 3.099s (8.447s including waiting). Image size: 30353649 bytes.\n",
      "  Normal  Created    12s   kubelet            Created container: kube-proxy\n",
      "  Normal  Started    12s   kubelet            Started container kube-proxy\n",
      "\n",
      "\n",
      "Name:                 kube-proxy-q7vsz\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      kube-proxy\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:27 +0000\n",
      "Labels:               controller-revision-hash=67b77d7946\n",
      "                      k8s-app=kube-proxy\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  DaemonSet/kube-proxy\n",
      "Containers:\n",
      "  kube-proxy:\n",
      "    Container ID:  containerd://f2474af30aa4f343d6abdb92a2f0e2f698a791a01e8922350f3055a5d61f002c\n",
      "    Image:         registry.k8s.io/kube-proxy:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-proxy@sha256:e5839270c96c3ad1bea1dce4935126d3281297527f3655408d2970aa4b5cf178\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /usr/local/bin/kube-proxy\n",
      "      --config=/var/lib/kube-proxy/config.conf\n",
      "      --hostname-override=$(NODE_NAME)\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:27 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      NODE_NAME:   (v1:spec.nodeName)\n",
      "    Mounts:\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /var/lib/kube-proxy from kube-proxy (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-th9mp (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-proxy:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      kube-proxy\n",
      "    Optional:  false\n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  kube-api-access-th9mp:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age    From               Message\n",
      "  ----    ------     ----   ----               -------\n",
      "  Normal  Scheduled  2m38s  default-scheduler  Successfully assigned kube-system/kube-proxy-q7vsz to cpnode\n",
      "  Normal  Pulled     2m38s  kubelet            Container image \"registry.k8s.io/kube-proxy:v1.31.7\" already present on machine\n",
      "  Normal  Created    2m38s  kubelet            Created container: kube-proxy\n",
      "  Normal  Started    2m38s  kubelet            Started container kube-proxy\n",
      "\n",
      "\n",
      "Name:                 kube-scheduler-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=kube-scheduler\n",
      "                      tier=control-plane\n",
      "Annotations:          kubernetes.io/config.hash: 010b3393b5e001f315b370f02ea180dc\n",
      "                      kubernetes.io/config.mirror: 010b3393b5e001f315b370f02ea180dc\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:21.893584684Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  kube-scheduler:\n",
      "    Container ID:  containerd://bca4d8d2bb99a0358e77a3b70f3f1f181ebbad6aa51eac345a5298aca785eef0\n",
      "    Image:         registry.k8s.io/kube-scheduler:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-scheduler@sha256:fb80249bcb77ee72b1c9fa5b70bc28a83ed107c9ca71957841ad91db379963bf\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      kube-scheduler\n",
      "      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n",
      "      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n",
      "      --bind-address=127.0.0.1\n",
      "      --kubeconfig=/etc/kubernetes/scheduler.conf\n",
      "      --leader-elect=true\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/kubernetes/scheduler.conf from kubeconfig (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kubeconfig:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/scheduler.conf\n",
      "    HostPathType:  FileOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/kube-scheduler:v1.31.7\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: kube-scheduler\n",
      "  Normal  Started  2m48s  kubelet  Started container kube-scheduler\n",
      "Kube-system pods details:\n",
      "Name:                 calico-kube-controllers-6879d4fcdc-tzcth\n",
      "Namespace:            kube-system\n",
      "Priority:             2000000000\n",
      "Priority Class Name:  system-cluster-critical\n",
      "Service Account:      calico-kube-controllers\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:35 +0000\n",
      "Labels:               k8s-app=calico-kube-controllers\n",
      "                      pod-template-hash=6879d4fcdc\n",
      "Annotations:          cni.projectcalico.org/containerID: f6e907900d79c586b0c881d560ed226064cbe2a03b50d45f713cd208695a0590\n",
      "                      cni.projectcalico.org/podIP: 10.134.132.231/32\n",
      "                      cni.projectcalico.org/podIPs: 10.134.132.231/32\n",
      "Status:               Running\n",
      "IP:                   10.134.132.231\n",
      "IPs:\n",
      "  IP:           10.134.132.231\n",
      "Controlled By:  ReplicaSet/calico-kube-controllers-6879d4fcdc\n",
      "Containers:\n",
      "  calico-kube-controllers:\n",
      "    Container ID:   containerd://d0f7dbe3f163ee66b1df5986b08e861f9bc206f2cf36414c9aed957255662a12\n",
      "    Image:          docker.io/calico/kube-controllers:v3.25.0\n",
      "    Image ID:       docker.io/calico/kube-controllers@sha256:c45af3a9692d87a527451cf544557138fedf86f92b6e39bf2003e2fdb848dce3\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:49 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Liveness:       exec [/usr/bin/check-status -l] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:      exec [/usr/bin/check-status -r] delay=0s timeout=1s period=10s #success=1 #failure=3\n",
      "    Environment:\n",
      "      ENABLED_CONTROLLERS:  node\n",
      "      DATASTORE_TYPE:       kubernetes\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xkm6v (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-api-access-xkm6v:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 CriticalAddonsOnly op=Exists\n",
      "                             node-role.kubernetes.io/control-plane:NoSchedule\n",
      "                             node-role.kubernetes.io/master:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason                  Age                    From               Message\n",
      "  ----     ------                  ----                   ----               -------\n",
      "  Warning  FailedScheduling        2m38s                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.\n",
      "  Normal   Scheduled               2m30s                  default-scheduler  Successfully assigned kube-system/calico-kube-controllers-6879d4fcdc-tzcth to cpnode\n",
      "  Warning  FailedCreatePodSandBox  2m29s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"15b18c3ffaa1a3ff406107d2138f6a1af7e3c64aba271ad899049acb08d3daad\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m28s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"07e93cb09bb4a5b35ca95fc231cdf73760998c5ff14e2742af82301db54ac8fc\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m27s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"f90af45fa73fe25bdfb2334c88db6b58cc9ab585acf7ea724903a37753b5fe7a\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m26s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"9ede3d079c3a9abc9b916b674d9a9d36eaf04a0ce90563a9625b6ff331466914\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m25s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"f85b79e945001b84a62af67f7614775efd1a361813aa30f97941f647ef715468\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m24s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"0fc181a139cdc629e5d8906ac8db143fe1ba328003998e93137d944030d054c3\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m23s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"81bc234ffef10fc44e1b2f13f12cbe9e17de12b6528b63ee63062078a373f907\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m22s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"db02642c21906ac53ca5471084fbf464e9083c5e411cbd3c095c2ca0d784aa77\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m21s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"9dc9a6d4e348bd39c60105fc0529b15bf997f2bcca50bd63378bcd1e38570797\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Normal   SandboxChanged          2m20s (x9 over 2m29s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n",
      "  Normal   Pulling                 2m20s                  kubelet            Pulling image \"docker.io/calico/kube-controllers:v3.25.0\"\n",
      "  Normal   Pulled                  2m16s                  kubelet            Successfully pulled image \"docker.io/calico/kube-controllers:v3.25.0\" in 3.935s (3.935s including waiting). Image size: 31271800 bytes.\n",
      "  Normal   Created                 2m16s                  kubelet            Created container: calico-kube-controllers\n",
      "  Normal   Started                 2m16s                  kubelet            Started container calico-kube-controllers\n",
      "\n",
      "\n",
      "Name:                 calico-node-bf4b2\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      calico-node\n",
      "Node:                 wknode2/10.20.4.135\n",
      "Start Time:           Thu, 13 Mar 2025 16:05:42 +0000\n",
      "Labels:               controller-revision-hash=645686658b\n",
      "                      k8s-app=calico-node\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Pending\n",
      "IP:                   10.20.4.135\n",
      "IPs:\n",
      "  IP:           10.20.4.135\n",
      "Controlled By:  DaemonSet/calico-node\n",
      "Init Containers:\n",
      "  upgrade-ipam:\n",
      "    Container ID:  containerd://df5deabbeab259f7ad989cb1365bfe7db9daba3427228ae3a024a0d487850fcc\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/calico-ipam\n",
      "      -upgrade\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:05:50 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:05:50 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "    Mounts:\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/lib/cni/networks from host-local-net-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "  install-cni:\n",
      "    Container ID:  containerd://6b81772386d1b675d67dba19a776e8eeaab002d42ab3d7c52892939c343fbb00\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/install\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:51 +0000\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      CNI_CONF_NAME:         10-calico.conflist\n",
      "      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false\n",
      "      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)\n",
      "      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      SLEEP:                 false\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "  mount-bpffs:\n",
      "    Container ID:  \n",
      "    Image:         docker.io/calico/node:v3.25.0\n",
      "    Image ID:      \n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      calico-node\n",
      "      -init\n",
      "      -best-effort\n",
      "    State:          Waiting\n",
      "      Reason:       PodInitializing\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /nodeproc from nodeproc (ro)\n",
      "      /sys/fs from sys-fs (rw)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "Containers:\n",
      "  calico-node:\n",
      "    Container ID:   \n",
      "    Image:          docker.io/calico/node:v3.25.0\n",
      "    Image ID:       \n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Waiting\n",
      "      Reason:       PodInitializing\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:      250m\n",
      "    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=10s period=10s #success=1 #failure=3\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      DATASTORE_TYPE:                     kubernetes\n",
      "      WAIT_FOR_DATASTORE:                 true\n",
      "      NODENAME:                            (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "      CLUSTER_TYPE:                       k8s,bgp\n",
      "      IP:                                 autodetect\n",
      "      CALICO_IPV4POOL_IPIP:               Always\n",
      "      CALICO_IPV4POOL_VXLAN:              Never\n",
      "      CALICO_IPV6POOL_VXLAN:              Never\n",
      "      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_VXLANMTU:                     <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_WIREGUARDMTU:                 <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      CALICO_DISABLE_FILE_LOGGING:        true\n",
      "      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\n",
      "      FELIX_IPV6SUPPORT:                  false\n",
      "      FELIX_HEALTHENABLED:                true\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /sys/fs/bpf from bpffs (rw)\n",
      "      /var/lib/calico from var-lib-calico (rw)\n",
      "      /var/log/calico/cni from cni-log-dir (ro)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/nodeagent from policysync (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpmxc (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 False \n",
      "  Ready                       False \n",
      "  ContainersReady             False \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  var-run-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/calico\n",
      "    HostPathType:  \n",
      "  var-lib-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/calico\n",
      "    HostPathType:  \n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  sys-fs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  bpffs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/bpf\n",
      "    HostPathType:  Directory\n",
      "  nodeproc:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /proc\n",
      "    HostPathType:  \n",
      "  cni-bin-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/cni/bin\n",
      "    HostPathType:  \n",
      "  cni-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/cni/net.d\n",
      "    HostPathType:  \n",
      "  cni-log-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/calico/cni\n",
      "    HostPathType:  \n",
      "  host-local-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/cni/networks\n",
      "    HostPathType:  \n",
      "  policysync:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/nodeagent\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kube-api-access-bpmxc:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 :NoSchedule op=Exists\n",
      "                             :NoExecute op=Exists\n",
      "                             CriticalAddonsOnly op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  24s   default-scheduler  Successfully assigned kube-system/calico-node-bf4b2 to wknode2\n",
      "  Normal  Pulling    20s   kubelet            Pulling image \"docker.io/calico/cni:v3.25.0\"\n",
      "  Normal  Pulled     15s   kubelet            Successfully pulled image \"docker.io/calico/cni:v3.25.0\" in 5.35s (5.35s including waiting). Image size: 87984941 bytes.\n",
      "  Normal  Created    15s   kubelet            Created container: upgrade-ipam\n",
      "  Normal  Started    15s   kubelet            Started container upgrade-ipam\n",
      "  Normal  Pulled     14s   kubelet            Container image \"docker.io/calico/cni:v3.25.0\" already present on machine\n",
      "  Normal  Created    14s   kubelet            Created container: install-cni\n",
      "  Normal  Started    14s   kubelet            Started container install-cni\n",
      "\n",
      "\n",
      "Name:                 calico-node-d8djr\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      calico-node\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:27 +0000\n",
      "Labels:               controller-revision-hash=645686658b\n",
      "                      k8s-app=calico-node\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  DaemonSet/calico-node\n",
      "Init Containers:\n",
      "  upgrade-ipam:\n",
      "    Container ID:  containerd://7ac7ed2ba8c47c1ee2fc5930887e2c152c2f0a0ef02916f52ce157f1f13faed1\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/calico-ipam\n",
      "      -upgrade\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:03:34 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:03:34 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "    Mounts:\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/lib/cni/networks from host-local-net-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "  install-cni:\n",
      "    Container ID:  containerd://20efa0adf0a2ba45ff353d081ba6ab70838207f3937e7d15c5c8c927ca6a8acd\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/install\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:03:35 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:03:35 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      CNI_CONF_NAME:         10-calico.conflist\n",
      "      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false\n",
      "      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)\n",
      "      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      SLEEP:                 false\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "  mount-bpffs:\n",
      "    Container ID:  containerd://5425209a79963668b686636784dd2e968e006f7bdfabf22e6083b254ee7eb958\n",
      "    Image:         docker.io/calico/node:v3.25.0\n",
      "    Image ID:      docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      calico-node\n",
      "      -init\n",
      "      -best-effort\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:03:42 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:03:42 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /nodeproc from nodeproc (ro)\n",
      "      /sys/fs from sys-fs (rw)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "Containers:\n",
      "  calico-node:\n",
      "    Container ID:   containerd://663c496629fe772cb0d3b8ce41706c21e9fbb16dd5e4f016a5dc8a17c3a085d6\n",
      "    Image:          docker.io/calico/node:v3.25.0\n",
      "    Image ID:       docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:44 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:      250m\n",
      "    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=10s period=10s #success=1 #failure=3\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      DATASTORE_TYPE:                     kubernetes\n",
      "      WAIT_FOR_DATASTORE:                 true\n",
      "      NODENAME:                            (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "      CLUSTER_TYPE:                       k8s,bgp\n",
      "      IP:                                 autodetect\n",
      "      CALICO_IPV4POOL_IPIP:               Always\n",
      "      CALICO_IPV4POOL_VXLAN:              Never\n",
      "      CALICO_IPV6POOL_VXLAN:              Never\n",
      "      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_VXLANMTU:                     <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_WIREGUARDMTU:                 <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      CALICO_DISABLE_FILE_LOGGING:        true\n",
      "      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\n",
      "      FELIX_IPV6SUPPORT:                  false\n",
      "      FELIX_HEALTHENABLED:                true\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /sys/fs/bpf from bpffs (rw)\n",
      "      /var/lib/calico from var-lib-calico (rw)\n",
      "      /var/log/calico/cni from cni-log-dir (ro)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/nodeagent from policysync (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qwtfk (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  var-run-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/calico\n",
      "    HostPathType:  \n",
      "  var-lib-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/calico\n",
      "    HostPathType:  \n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  sys-fs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  bpffs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/bpf\n",
      "    HostPathType:  Directory\n",
      "  nodeproc:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /proc\n",
      "    HostPathType:  \n",
      "  cni-bin-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/cni/bin\n",
      "    HostPathType:  \n",
      "  cni-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/cni/net.d\n",
      "    HostPathType:  \n",
      "  cni-log-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/calico/cni\n",
      "    HostPathType:  \n",
      "  host-local-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/cni/networks\n",
      "    HostPathType:  \n",
      "  policysync:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/nodeagent\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kube-api-access-qwtfk:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 :NoSchedule op=Exists\n",
      "                             :NoExecute op=Exists\n",
      "                             CriticalAddonsOnly op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type     Reason     Age                    From               Message\n",
      "  ----     ------     ----                   ----               -------\n",
      "  Normal   Scheduled  2m38s                  default-scheduler  Successfully assigned kube-system/calico-node-d8djr to cpnode\n",
      "  Normal   Pulling    2m38s                  kubelet            Pulling image \"docker.io/calico/cni:v3.25.0\"\n",
      "  Normal   Pulled     2m31s                  kubelet            Successfully pulled image \"docker.io/calico/cni:v3.25.0\" in 6.257s (6.257s including waiting). Image size: 87984941 bytes.\n",
      "  Normal   Created    2m31s                  kubelet            Created container: upgrade-ipam\n",
      "  Normal   Started    2m31s                  kubelet            Started container upgrade-ipam\n",
      "  Normal   Pulled     2m31s                  kubelet            Container image \"docker.io/calico/cni:v3.25.0\" already present on machine\n",
      "  Normal   Created    2m30s                  kubelet            Created container: install-cni\n",
      "  Normal   Started    2m30s                  kubelet            Started container install-cni\n",
      "  Normal   Pulling    2m30s                  kubelet            Pulling image \"docker.io/calico/node:v3.25.0\"\n",
      "  Normal   Pulled     2m23s                  kubelet            Successfully pulled image \"docker.io/calico/node:v3.25.0\" in 6.796s (6.796s including waiting). Image size: 87185935 bytes.\n",
      "  Normal   Created    2m23s                  kubelet            Created container: mount-bpffs\n",
      "  Normal   Started    2m23s                  kubelet            Started container mount-bpffs\n",
      "  Normal   Pulled     2m21s                  kubelet            Container image \"docker.io/calico/node:v3.25.0\" already present on machine\n",
      "  Normal   Created    2m21s                  kubelet            Created container: calico-node\n",
      "  Normal   Started    2m21s                  kubelet            Started container calico-node\n",
      "  Warning  Unhealthy  2m20s                  kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/bird/bird.ctl: connect: no such file or directory\n",
      "  Warning  Unhealthy  2m18s (x2 over 2m19s)  kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused\n",
      "  Warning  Unhealthy  28s                    kubelet            Readiness probe failed: 2025-03-13 16:05:37.855 [INFO][579] confd/health.go 180: Number of node(s) with BGP peering established = 0\n",
      "calico/node is not ready: BIRD is not ready: BGP not established with 10.134.132.3\n",
      "\n",
      "\n",
      "Name:                 calico-node-rxxmr\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      calico-node\n",
      "Node:                 wknode1/10.20.4.189\n",
      "Start Time:           Thu, 13 Mar 2025 16:04:48 +0000\n",
      "Labels:               controller-revision-hash=645686658b\n",
      "                      k8s-app=calico-node\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.4.189\n",
      "IPs:\n",
      "  IP:           10.20.4.189\n",
      "Controlled By:  DaemonSet/calico-node\n",
      "Init Containers:\n",
      "  upgrade-ipam:\n",
      "    Container ID:  containerd://e19eccbb0116d2afdb450fe4c7cd9e7d33e364cd981a19ad712b2092f042ad27\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/calico-ipam\n",
      "      -upgrade\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:04:57 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:04:57 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "    Mounts:\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/lib/cni/networks from host-local-net-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "  install-cni:\n",
      "    Container ID:  containerd://fcd96b5477d31e258fa29c0646141741cd6c6bab976e60c7be91ea8dc44a13fd\n",
      "    Image:         docker.io/calico/cni:v3.25.0\n",
      "    Image ID:      docker.io/calico/cni@sha256:a38d53cb8688944eafede2f0eadc478b1b403cefeff7953da57fe9cd2d65e977\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /opt/cni/bin/install\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:05:28 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:05:29 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  1\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      CNI_CONF_NAME:         10-calico.conflist\n",
      "      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false\n",
      "      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)\n",
      "      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      SLEEP:                 false\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /host/opt/cni/bin from cni-bin-dir (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "  mount-bpffs:\n",
      "    Container ID:  containerd://8dfcded7a69bbe2e32517cf97e875fcdf89e7e85f8a3a30648e954fce8f8fed2\n",
      "    Image:         docker.io/calico/node:v3.25.0\n",
      "    Image ID:      docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      calico-node\n",
      "      -init\n",
      "      -best-effort\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Thu, 13 Mar 2025 16:05:34 +0000\n",
      "      Finished:     Thu, 13 Mar 2025 16:05:34 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /nodeproc from nodeproc (ro)\n",
      "      /sys/fs from sys-fs (rw)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "Containers:\n",
      "  calico-node:\n",
      "    Container ID:   containerd://2d1aa7e98ad57a3a0e8d8e3a676392c52bd67bc75816adddea1fcab53bc8f374\n",
      "    Image:          docker.io/calico/node:v3.25.0\n",
      "    Image ID:       docker.io/calico/node@sha256:a85123d1882832af6c45b5e289c6bb99820646cb7d4f6006f98095168808b1e6\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:35 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:      250m\n",
      "    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=10s period=10s #success=1 #failure=6\n",
      "    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=10s period=10s #success=1 #failure=3\n",
      "    Environment Variables from:\n",
      "      kubernetes-services-endpoint  ConfigMap  Optional: true\n",
      "    Environment:\n",
      "      DATASTORE_TYPE:                     kubernetes\n",
      "      WAIT_FOR_DATASTORE:                 true\n",
      "      NODENAME:                            (v1:spec.nodeName)\n",
      "      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false\n",
      "      CLUSTER_TYPE:                       k8s,bgp\n",
      "      IP:                                 autodetect\n",
      "      CALICO_IPV4POOL_IPIP:               Always\n",
      "      CALICO_IPV4POOL_VXLAN:              Never\n",
      "      CALICO_IPV6POOL_VXLAN:              Never\n",
      "      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_VXLANMTU:                     <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      FELIX_WIREGUARDMTU:                 <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false\n",
      "      CALICO_DISABLE_FILE_LOGGING:        true\n",
      "      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT\n",
      "      FELIX_IPV6SUPPORT:                  false\n",
      "      FELIX_HEALTHENABLED:                true\n",
      "    Mounts:\n",
      "      /host/etc/cni/net.d from cni-net-dir (rw)\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /sys/fs/bpf from bpffs (rw)\n",
      "      /var/lib/calico from var-lib-calico (rw)\n",
      "      /var/log/calico/cni from cni-log-dir (ro)\n",
      "      /var/run/calico from var-run-calico (rw)\n",
      "      /var/run/nodeagent from policysync (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbsbx (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  var-run-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/calico\n",
      "    HostPathType:  \n",
      "  var-lib-calico:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/calico\n",
      "    HostPathType:  \n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  sys-fs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  bpffs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/fs/bpf\n",
      "    HostPathType:  Directory\n",
      "  nodeproc:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /proc\n",
      "    HostPathType:  \n",
      "  cni-bin-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/cni/bin\n",
      "    HostPathType:  \n",
      "  cni-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/cni/net.d\n",
      "    HostPathType:  \n",
      "  cni-log-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/calico/cni\n",
      "    HostPathType:  \n",
      "  host-local-net-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/cni/networks\n",
      "    HostPathType:  \n",
      "  policysync:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/run/nodeagent\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kube-api-access-gbsbx:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 :NoSchedule op=Exists\n",
      "                             :NoExecute op=Exists\n",
      "                             CriticalAddonsOnly op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type     Reason     Age                From               Message\n",
      "  ----     ------     ----               ----               -------\n",
      "  Normal   Scheduled  77s                default-scheduler  Successfully assigned kube-system/calico-node-rxxmr to wknode1\n",
      "  Normal   Pulling    74s                kubelet            Pulling image \"docker.io/calico/cni:v3.25.0\"\n",
      "  Normal   Pulled     69s                kubelet            Successfully pulled image \"docker.io/calico/cni:v3.25.0\" in 5.746s (5.747s including waiting). Image size: 87984941 bytes.\n",
      "  Normal   Created    69s                kubelet            Created container: upgrade-ipam\n",
      "  Normal   Started    68s                kubelet            Started container upgrade-ipam\n",
      "  Normal   Pulled     37s (x2 over 68s)  kubelet            Container image \"docker.io/calico/cni:v3.25.0\" already present on machine\n",
      "  Normal   Created    37s (x2 over 68s)  kubelet            Created container: install-cni\n",
      "  Normal   Started    37s (x2 over 68s)  kubelet            Started container install-cni\n",
      "  Normal   Pulling    36s                kubelet            Pulling image \"docker.io/calico/node:v3.25.0\"\n",
      "  Normal   Pulled     31s                kubelet            Successfully pulled image \"docker.io/calico/node:v3.25.0\" in 4.886s (4.886s including waiting). Image size: 87185935 bytes.\n",
      "  Normal   Created    31s                kubelet            Created container: mount-bpffs\n",
      "  Normal   Started    31s                kubelet            Started container mount-bpffs\n",
      "  Normal   Pulled     30s                kubelet            Container image \"docker.io/calico/node:v3.25.0\" already present on machine\n",
      "  Normal   Created    30s                kubelet            Created container: calico-node\n",
      "  Normal   Started    30s                kubelet            Started container calico-node\n",
      "  Warning  Unhealthy  29s                kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/bird/bird.ctl: connect: no such file or directory\n",
      "  Warning  Unhealthy  28s                kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused\n",
      "\n",
      "\n",
      "Name:                 coredns-7c65d6cfc9-dxjjh\n",
      "Namespace:            kube-system\n",
      "Priority:             2000000000\n",
      "Priority Class Name:  system-cluster-critical\n",
      "Service Account:      coredns\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:35 +0000\n",
      "Labels:               k8s-app=kube-dns\n",
      "                      pod-template-hash=7c65d6cfc9\n",
      "Annotations:          cni.projectcalico.org/containerID: 514ed720c03e56244645385f8a3a8fa9c9e5330a0600b77039a6c5e4790da7e8\n",
      "                      cni.projectcalico.org/podIP: 10.134.132.229/32\n",
      "                      cni.projectcalico.org/podIPs: 10.134.132.229/32\n",
      "Status:               Running\n",
      "IP:                   10.134.132.229\n",
      "IPs:\n",
      "  IP:           10.134.132.229\n",
      "Controlled By:  ReplicaSet/coredns-7c65d6cfc9\n",
      "Containers:\n",
      "  coredns:\n",
      "    Container ID:  containerd://7132edf0eb7d5ce354657a493be26198f518e27e548a3e6e2ea8af336ea1b354\n",
      "    Image:         registry.k8s.io/coredns/coredns:v1.11.3\n",
      "    Image ID:      registry.k8s.io/coredns/coredns@sha256:9caabbf6238b189a65d0d6e6ac138de60d6a1c419e5a341fbbb7c78382559c6e\n",
      "    Ports:         53/UDP, 53/TCP, 9153/TCP\n",
      "    Host Ports:    0/UDP, 0/TCP, 0/TCP\n",
      "    Args:\n",
      "      -conf\n",
      "      /etc/coredns/Corefile\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:45 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      memory:  170Mi\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "      memory:     70Mi\n",
      "    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5\n",
      "    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/coredns from config-volume (ro)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mbttp (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  config-volume:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      coredns\n",
      "    Optional:  false\n",
      "  kube-api-access-mbttp:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 CriticalAddonsOnly op=Exists\n",
      "                             node-role.kubernetes.io/control-plane:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason                  Age                    From               Message\n",
      "  ----     ------                  ----                   ----               -------\n",
      "  Warning  FailedScheduling        2m38s                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.\n",
      "  Normal   Scheduled               2m30s                  default-scheduler  Successfully assigned kube-system/coredns-7c65d6cfc9-dxjjh to cpnode\n",
      "  Warning  FailedCreatePodSandBox  2m29s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"2564602d14ed77e242fbd23b9798a40070a89764956d8cfa9b62133b737617d8\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m28s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"6821b9a564697cc80f0a07926bc36fd0fc8fffcc67bc541b7b8d14c8e567e2c8\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m27s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"019300d47c477c16fbf9c9407b261a704ed0b489c6932d46995105d083ea1c49\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m26s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"29f92e5d714091403aaf8048b678bb8d9b15b01a32aed167caa96312f5b5903e\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m25s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"e128169e28761f9ffac661397b141e4973017d5db4e3e38526e62f20d9f9b3df\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m24s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"b8123982180e9d590539d80314908358410c319de9602908dfd6ffae5c047f24\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m23s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"77c2a2488744e0a671c4793a920ecc40c00f44060b0125f66aeadace370a0cfe\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m22s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"7558c9e6b408ff0b284412f24e0ef8b00cfa82e47953020d084a0714a023115b\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m21s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"1165f8a04c571ec6fb083eb0cbea45b229d925af7653c6ed0b112a2b0e890b3f\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Normal   SandboxChanged          2m20s (x9 over 2m29s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n",
      "  Normal   Pulled                  2m20s                  kubelet            Container image \"registry.k8s.io/coredns/coredns:v1.11.3\" already present on machine\n",
      "  Normal   Created                 2m20s                  kubelet            Created container: coredns\n",
      "  Normal   Started                 2m20s                  kubelet            Started container coredns\n",
      "\n",
      "\n",
      "Name:                 coredns-7c65d6cfc9-rqhx7\n",
      "Namespace:            kube-system\n",
      "Priority:             2000000000\n",
      "Priority Class Name:  system-cluster-critical\n",
      "Service Account:      coredns\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:35 +0000\n",
      "Labels:               k8s-app=kube-dns\n",
      "                      pod-template-hash=7c65d6cfc9\n",
      "Annotations:          cni.projectcalico.org/containerID: 1c1cc996f08f62825f9a49ec8f3b5332a7b2878afd7762c71629c96f0045bedc\n",
      "                      cni.projectcalico.org/podIP: 10.134.132.230/32\n",
      "                      cni.projectcalico.org/podIPs: 10.134.132.230/32\n",
      "Status:               Running\n",
      "IP:                   10.134.132.230\n",
      "IPs:\n",
      "  IP:           10.134.132.230\n",
      "Controlled By:  ReplicaSet/coredns-7c65d6cfc9\n",
      "Containers:\n",
      "  coredns:\n",
      "    Container ID:  containerd://bead03a4bab81c3384477cdf320a55b4d6731dde5f2924be97da5a2b5665fad6\n",
      "    Image:         registry.k8s.io/coredns/coredns:v1.11.3\n",
      "    Image ID:      registry.k8s.io/coredns/coredns@sha256:9caabbf6238b189a65d0d6e6ac138de60d6a1c419e5a341fbbb7c78382559c6e\n",
      "    Ports:         53/UDP, 53/TCP, 9153/TCP\n",
      "    Host Ports:    0/UDP, 0/TCP, 0/TCP\n",
      "    Args:\n",
      "      -conf\n",
      "      /etc/coredns/Corefile\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:46 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      memory:  170Mi\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "      memory:     70Mi\n",
      "    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5\n",
      "    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/coredns from config-volume (ro)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9qzxr (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  config-volume:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      coredns\n",
      "    Optional:  false\n",
      "  kube-api-access-9qzxr:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 CriticalAddonsOnly op=Exists\n",
      "                             node-role.kubernetes.io/control-plane:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason                  Age                    From               Message\n",
      "  ----     ------                  ----                   ----               -------\n",
      "  Warning  FailedScheduling        2m38s                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.\n",
      "  Normal   Scheduled               2m30s                  default-scheduler  Successfully assigned kube-system/coredns-7c65d6cfc9-rqhx7 to cpnode\n",
      "  Warning  FailedCreatePodSandBox  2m28s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"6a2269f4381eaf3e438585ca0dfd44d6d6b64e1e1eb5ab855fd53167b71ba1e0\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m27s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"9bdd3f38127a2416963655b6309dd5247ba639c43e6ef2f6a53be1a8de7c6f6a\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m26s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"57de4b2fda2ec39ab22256c6958728e894f84235151ad2ad9abf34b20b33fc76\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m25s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"1e37827adda58b4f0163779272a73d8dfc327e319b640e0e993bc54c00be4eaa\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m24s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"808507fd17b6ecc581ae607e648e9404df2a58caa7444195a48f9bda7b8f3c2e\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m23s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"e697b922400e02f18970186d672f0225420c46b53dbcb7272d89c3aec8c88b10\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m22s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"93f864c9511f906a0de9dbc62f3a315f86395d0352a5017020d6a3b680325a00\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Warning  FailedCreatePodSandBox  2m21s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"3005d6a8ba7db45c4056a7f297e3d68cdefb39937bbeea0e4ad69313baac607b\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n",
      "  Normal   SandboxChanged          2m20s (x8 over 2m27s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n",
      "  Normal   Pulled                  2m19s                  kubelet            Container image \"registry.k8s.io/coredns/coredns:v1.11.3\" already present on machine\n",
      "  Normal   Created                 2m19s                  kubelet            Created container: coredns\n",
      "  Normal   Started                 2m19s                  kubelet            Started container coredns\n",
      "\n",
      "\n",
      "Name:                 etcd-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=etcd\n",
      "                      tier=control-plane\n",
      "Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://10.134.132.2:2379\n",
      "                      kubernetes.io/config.hash: b38112e11521dcc07e79de28b7cb678c\n",
      "                      kubernetes.io/config.mirror: b38112e11521dcc07e79de28b7cb678c\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:21.893585826Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  etcd:\n",
      "    Container ID:  containerd://cd80ce8d1f803e13d99073f257afdfa1f2e46d3ce058de6110768fb2411f0344\n",
      "    Image:         registry.k8s.io/etcd:3.5.15-0\n",
      "    Image ID:      registry.k8s.io/etcd@sha256:a6dc63e6e8cfa0307d7851762fa6b629afb18f28d8aa3fab5a6e91b4af60026a\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      etcd\n",
      "      --advertise-client-urls=https://10.134.132.2:2379\n",
      "      --cert-file=/etc/kubernetes/pki/etcd/server.crt\n",
      "      --client-cert-auth=true\n",
      "      --data-dir=/var/lib/etcd\n",
      "      --experimental-initial-corrupt-check=true\n",
      "      --experimental-watch-progress-notify-interval=5s\n",
      "      --initial-advertise-peer-urls=https://10.134.132.2:2380\n",
      "      --initial-cluster=cpnode=https://10.134.132.2:2380\n",
      "      --key-file=/etc/kubernetes/pki/etcd/server.key\n",
      "      --listen-client-urls=https://127.0.0.1:2379,https://10.134.132.2:2379\n",
      "      --listen-metrics-urls=http://127.0.0.1:2381\n",
      "      --listen-peer-urls=https://10.134.132.2:2380\n",
      "      --name=cpnode\n",
      "      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n",
      "      --peer-client-cert-auth=true\n",
      "      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n",
      "      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n",
      "      --snapshot-count=10000\n",
      "      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "      memory:     100Mi\n",
      "    Liveness:     http-get http://127.0.0.1:2381/livez delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Readiness:    http-get http://127.0.0.1:2381/readyz delay=0s timeout=15s period=1s #success=1 #failure=3\n",
      "    Startup:      http-get http://127.0.0.1:2381/readyz delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/kubernetes/pki/etcd from etcd-certs (rw)\n",
      "      /var/lib/etcd from etcd-data (rw)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  etcd-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/pki/etcd\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  etcd-data:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/etcd\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/etcd:3.5.15-0\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: etcd\n",
      "  Normal  Started  2m48s  kubelet  Started container etcd\n",
      "\n",
      "\n",
      "Name:                 kube-apiserver-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=kube-apiserver\n",
      "                      tier=control-plane\n",
      "Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.134.132.2:6443\n",
      "                      kubernetes.io/config.hash: f807b57d31281f36c12eddb66344ee9e\n",
      "                      kubernetes.io/config.mirror: f807b57d31281f36c12eddb66344ee9e\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:21.893586628Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  kube-apiserver:\n",
      "    Container ID:  containerd://0cbeca4b8c1646c1bbeea6761503f0794ca8674da0523938d5dab7f7910d07ea\n",
      "    Image:         registry.k8s.io/kube-apiserver:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-apiserver@sha256:22c19cc70fe5806d0a2cb28a6b6b33fd34e6f9e50616bdf6d53649bcfafbc277\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      kube-apiserver\n",
      "      --advertise-address=10.134.132.2\n",
      "      --allow-privileged=true\n",
      "      --authorization-mode=Node,RBAC\n",
      "      --client-ca-file=/etc/kubernetes/pki/ca.crt\n",
      "      --enable-admission-plugins=NodeRestriction\n",
      "      --enable-bootstrap-token-auth=true\n",
      "      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n",
      "      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n",
      "      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n",
      "      --etcd-servers=https://127.0.0.1:2379\n",
      "      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n",
      "      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n",
      "      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n",
      "      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n",
      "      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n",
      "      --requestheader-allowed-names=front-proxy-client\n",
      "      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n",
      "      --requestheader-extra-headers-prefix=X-Remote-Extra-\n",
      "      --requestheader-group-headers=X-Remote-Group\n",
      "      --requestheader-username-headers=X-Remote-User\n",
      "      --secure-port=6443\n",
      "      --service-account-issuer=https://kubernetes.default.svc.cluster.local\n",
      "      --service-account-key-file=/etc/kubernetes/pki/sa.pub\n",
      "      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n",
      "      --service-cluster-ip-range=10.96.0.0/12\n",
      "      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n",
      "      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        250m\n",
      "    Liveness:     http-get https://10.134.132.2:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Readiness:    http-get https://10.134.132.2:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3\n",
      "    Startup:      http-get https://10.134.132.2:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/ca-certificates from etc-ca-certificates (ro)\n",
      "      /etc/kubernetes/pki from k8s-certs (ro)\n",
      "      /etc/ssl/certs from ca-certs (ro)\n",
      "      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)\n",
      "      /usr/share/ca-certificates from usr-share-ca-certificates (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  ca-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ssl/certs\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  etc-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  k8s-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/pki\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  usr-local-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/local/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  usr-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/kube-apiserver:v1.31.7\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: kube-apiserver\n",
      "  Normal  Started  2m48s  kubelet  Started container kube-apiserver\n",
      "\n",
      "\n",
      "Name:                 kube-controller-manager-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=kube-controller-manager\n",
      "                      tier=control-plane\n",
      "Annotations:          kubernetes.io/config.hash: aac482c30a24fd0335a50eaba5f1d92b\n",
      "                      kubernetes.io/config.mirror: aac482c30a24fd0335a50eaba5f1d92b\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:14.300503833Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  kube-controller-manager:\n",
      "    Container ID:  containerd://fa8a7922f2652fbd16fa7c5d8e010d628a3a3db30159fef0728c8487bbf55f64\n",
      "    Image:         registry.k8s.io/kube-controller-manager:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-controller-manager@sha256:6abe7a0accecf29db6ebab18a10f844678ffed693d79e2e51a18a6f2b4530cbb\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      kube-controller-manager\n",
      "      --allocate-node-cidrs=true\n",
      "      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf\n",
      "      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf\n",
      "      --bind-address=127.0.0.1\n",
      "      --client-ca-file=/etc/kubernetes/pki/ca.crt\n",
      "      --cluster-cidr=10.134.132.0/24\n",
      "      --cluster-name=kubernetes\n",
      "      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n",
      "      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n",
      "      --controllers=*,bootstrapsigner,tokencleaner\n",
      "      --kubeconfig=/etc/kubernetes/controller-manager.conf\n",
      "      --leader-elect=true\n",
      "      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n",
      "      --root-ca-file=/etc/kubernetes/pki/ca.crt\n",
      "      --service-account-private-key-file=/etc/kubernetes/pki/sa.key\n",
      "      --service-cluster-ip-range=10.96.0.0/12\n",
      "      --use-service-account-credentials=true\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        200m\n",
      "    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/ca-certificates from etc-ca-certificates (ro)\n",
      "      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)\n",
      "      /etc/kubernetes/pki from k8s-certs (ro)\n",
      "      /etc/ssl/certs from ca-certs (ro)\n",
      "      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)\n",
      "      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)\n",
      "      /usr/share/ca-certificates from usr-share-ca-certificates (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  ca-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ssl/certs\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  etc-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  flexvolume-dir:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  k8s-certs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/pki\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  kubeconfig:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/controller-manager.conf\n",
      "    HostPathType:  FileOrCreate\n",
      "  usr-local-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/local/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  usr-share-ca-certificates:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /usr/share/ca-certificates\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/kube-controller-manager:v1.31.7\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: kube-controller-manager\n",
      "  Normal  Started  2m48s  kubelet  Started container kube-controller-manager\n",
      "\n",
      "\n",
      "Name:                 kube-proxy-7wsg4\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      kube-proxy\n",
      "Node:                 wknode1/10.20.4.189\n",
      "Start Time:           Thu, 13 Mar 2025 16:04:48 +0000\n",
      "Labels:               controller-revision-hash=67b77d7946\n",
      "                      k8s-app=kube-proxy\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.4.189\n",
      "IPs:\n",
      "  IP:           10.20.4.189\n",
      "Controlled By:  DaemonSet/kube-proxy\n",
      "Containers:\n",
      "  kube-proxy:\n",
      "    Container ID:  containerd://9be4d1f870bd8d2e097cce587a86d43ff2b802081ebeac2612a16c75314c5e16\n",
      "    Image:         registry.k8s.io/kube-proxy:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-proxy@sha256:e5839270c96c3ad1bea1dce4935126d3281297527f3655408d2970aa4b5cf178\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /usr/local/bin/kube-proxy\n",
      "      --config=/var/lib/kube-proxy/config.conf\n",
      "      --hostname-override=$(NODE_NAME)\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:00 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      NODE_NAME:   (v1:spec.nodeName)\n",
      "    Mounts:\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /var/lib/kube-proxy from kube-proxy (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-trpc7 (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-proxy:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      kube-proxy\n",
      "    Optional:  false\n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  kube-api-access-trpc7:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  77s   default-scheduler  Successfully assigned kube-system/kube-proxy-7wsg4 to wknode1\n",
      "  Normal  Pulling    74s   kubelet            Pulling image \"registry.k8s.io/kube-proxy:v1.31.7\"\n",
      "  Normal  Pulled     66s   kubelet            Successfully pulled image \"registry.k8s.io/kube-proxy:v1.31.7\" in 3.089s (8.826s including waiting). Image size: 30353649 bytes.\n",
      "  Normal  Created    65s   kubelet            Created container: kube-proxy\n",
      "  Normal  Started    65s   kubelet            Started container kube-proxy\n",
      "\n",
      "\n",
      "Name:                 kube-proxy-hlvhr\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      kube-proxy\n",
      "Node:                 wknode2/10.20.4.135\n",
      "Start Time:           Thu, 13 Mar 2025 16:05:42 +0000\n",
      "Labels:               controller-revision-hash=67b77d7946\n",
      "                      k8s-app=kube-proxy\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.4.135\n",
      "IPs:\n",
      "  IP:           10.20.4.135\n",
      "Controlled By:  DaemonSet/kube-proxy\n",
      "Containers:\n",
      "  kube-proxy:\n",
      "    Container ID:  containerd://0ae4b0d2480f7ef38b542256910748707c67a8471cd7954d8534624a5d286d34\n",
      "    Image:         registry.k8s.io/kube-proxy:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-proxy@sha256:e5839270c96c3ad1bea1dce4935126d3281297527f3655408d2970aa4b5cf178\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /usr/local/bin/kube-proxy\n",
      "      --config=/var/lib/kube-proxy/config.conf\n",
      "      --hostname-override=$(NODE_NAME)\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:05:53 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      NODE_NAME:   (v1:spec.nodeName)\n",
      "    Mounts:\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /var/lib/kube-proxy from kube-proxy (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7dwx4 (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-proxy:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      kube-proxy\n",
      "    Optional:  false\n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  kube-api-access-7dwx4:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  24s   default-scheduler  Successfully assigned kube-system/kube-proxy-hlvhr to wknode2\n",
      "  Normal  Pulling    20s   kubelet            Pulling image \"registry.k8s.io/kube-proxy:v1.31.7\"\n",
      "  Normal  Pulled     12s   kubelet            Successfully pulled image \"registry.k8s.io/kube-proxy:v1.31.7\" in 3.099s (8.447s including waiting). Image size: 30353649 bytes.\n",
      "  Normal  Created    12s   kubelet            Created container: kube-proxy\n",
      "  Normal  Started    12s   kubelet            Started container kube-proxy\n",
      "\n",
      "\n",
      "Name:                 kube-proxy-q7vsz\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Service Account:      kube-proxy\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:27 +0000\n",
      "Labels:               controller-revision-hash=67b77d7946\n",
      "                      k8s-app=kube-proxy\n",
      "                      pod-template-generation=1\n",
      "Annotations:          <none>\n",
      "Status:               Running\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  DaemonSet/kube-proxy\n",
      "Containers:\n",
      "  kube-proxy:\n",
      "    Container ID:  containerd://f2474af30aa4f343d6abdb92a2f0e2f698a791a01e8922350f3055a5d61f002c\n",
      "    Image:         registry.k8s.io/kube-proxy:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-proxy@sha256:e5839270c96c3ad1bea1dce4935126d3281297527f3655408d2970aa4b5cf178\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      /usr/local/bin/kube-proxy\n",
      "      --config=/var/lib/kube-proxy/config.conf\n",
      "      --hostname-override=$(NODE_NAME)\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:27 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      NODE_NAME:   (v1:spec.nodeName)\n",
      "    Mounts:\n",
      "      /lib/modules from lib-modules (ro)\n",
      "      /run/xtables.lock from xtables-lock (rw)\n",
      "      /var/lib/kube-proxy from kube-proxy (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-th9mp (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-proxy:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      kube-proxy\n",
      "    Optional:  false\n",
      "  xtables-lock:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /run/xtables.lock\n",
      "    HostPathType:  FileOrCreate\n",
      "  lib-modules:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /lib/modules\n",
      "    HostPathType:  \n",
      "  kube-api-access-th9mp:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              kubernetes.io/os=linux\n",
      "Tolerations:                 op=Exists\n",
      "                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists\n",
      "                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists\n",
      "                             node.kubernetes.io/unschedulable:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason     Age    From               Message\n",
      "  ----    ------     ----   ----               -------\n",
      "  Normal  Scheduled  2m38s  default-scheduler  Successfully assigned kube-system/kube-proxy-q7vsz to cpnode\n",
      "  Normal  Pulled     2m38s  kubelet            Container image \"registry.k8s.io/kube-proxy:v1.31.7\" already present on machine\n",
      "  Normal  Created    2m38s  kubelet            Created container: kube-proxy\n",
      "  Normal  Started    2m38s  kubelet            Started container kube-proxy\n",
      "\n",
      "\n",
      "Name:                 kube-scheduler-cpnode\n",
      "Namespace:            kube-system\n",
      "Priority:             2000001000\n",
      "Priority Class Name:  system-node-critical\n",
      "Node:                 cpnode/10.20.5.188\n",
      "Start Time:           Thu, 13 Mar 2025 16:03:22 +0000\n",
      "Labels:               component=kube-scheduler\n",
      "                      tier=control-plane\n",
      "Annotations:          kubernetes.io/config.hash: 010b3393b5e001f315b370f02ea180dc\n",
      "                      kubernetes.io/config.mirror: 010b3393b5e001f315b370f02ea180dc\n",
      "                      kubernetes.io/config.seen: 2025-03-13T16:03:21.893584684Z\n",
      "                      kubernetes.io/config.source: file\n",
      "Status:               Running\n",
      "SeccompProfile:       RuntimeDefault\n",
      "IP:                   10.20.5.188\n",
      "IPs:\n",
      "  IP:           10.20.5.188\n",
      "Controlled By:  Node/cpnode\n",
      "Containers:\n",
      "  kube-scheduler:\n",
      "    Container ID:  containerd://bca4d8d2bb99a0358e77a3b70f3f1f181ebbad6aa51eac345a5298aca785eef0\n",
      "    Image:         registry.k8s.io/kube-scheduler:v1.31.7\n",
      "    Image ID:      registry.k8s.io/kube-scheduler@sha256:fb80249bcb77ee72b1c9fa5b70bc28a83ed107c9ca71957841ad91db379963bf\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      kube-scheduler\n",
      "      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n",
      "      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n",
      "      --bind-address=127.0.0.1\n",
      "      --kubeconfig=/etc/kubernetes/scheduler.conf\n",
      "      --leader-elect=true\n",
      "    State:          Running\n",
      "      Started:      Thu, 13 Mar 2025 16:03:17 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8\n",
      "    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /etc/kubernetes/scheduler.conf from kubeconfig (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kubeconfig:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /etc/kubernetes/scheduler.conf\n",
      "    HostPathType:  FileOrCreate\n",
      "QoS Class:         Burstable\n",
      "Node-Selectors:    <none>\n",
      "Tolerations:       :NoExecute op=Exists\n",
      "Events:\n",
      "  Type    Reason   Age    From     Message\n",
      "  ----    ------   ----   ----     -------\n",
      "  Normal  Pulled   2m48s  kubelet  Container image \"registry.k8s.io/kube-scheduler:v1.31.7\" already present on machine\n",
      "  Normal  Created  2m48s  kubelet  Created container: kube-scheduler\n",
      "  Normal  Started  2m48s  kubelet  Started container kube-scheduler\n",
      "\n",
      "\n",
      "Getting logs for kube-apiserver...\n",
      "kube-apiserver-cpnodeI0313 16:03:19.525885       1 cache.go:39] Caches are synced for RemoteAvailability controller\n",
      "I0313 16:03:19.526489       1 handler_discovery.go:450] Starting ResourceDiscoveryManager\n",
      "I0313 16:03:19.526756       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller\n",
      "I0313 16:03:19.527160       1 cache.go:39] Caches are synced for LocalAvailability controller\n",
      "I0313 16:03:19.527315       1 apf_controller.go:382] Running API Priority and Fairness config worker\n",
      "I0313 16:03:19.527389       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process\n",
      "I0313 16:03:19.531100       1 shared_informer.go:320] Caches are synced for crd-autoregister\n",
      "I0313 16:03:19.531995       1 aggregator.go:171] initial CRD sync complete...\n",
      "I0313 16:03:19.532071       1 autoregister_controller.go:144] Starting autoregister controller\n",
      "I0313 16:03:19.532149       1 cache.go:32] Waiting for caches to sync for autoregister controller\n",
      "I0313 16:03:19.532220       1 cache.go:39] Caches are synced for autoregister controller\n",
      "I0313 16:03:19.535138       1 shared_informer.go:320] Caches are synced for node_authorizer\n",
      "I0313 16:03:19.557676       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]\n",
      "I0313 16:03:19.557689       1 policy_source.go:224] refreshing policies\n",
      "E0313 16:03:19.582105       1 controller.go:148] \"Unhandled Error\" err=\"while syncing ConfigMap \\\"kube-system/kube-apiserver-legacy-service-account-token-tracking\\\", err: namespaces \\\"kube-system\\\" not found\" logger=\"UnhandledError\"\n",
      "E0313 16:03:19.593931       1 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"namespaces \\\"kube-system\\\" not found\" interval=\"200ms\"\n",
      "I0313 16:03:19.632652       1 controller.go:615] quota admission added evaluator for: namespaces\n",
      "I0313 16:03:19.796216       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io\n",
      "I0313 16:03:20.430129       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000\n",
      "I0313 16:03:20.434952       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000\n",
      "I0313 16:03:20.435379       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.\n",
      "I0313 16:03:20.754073       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io\n",
      "I0313 16:03:20.783545       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io\n",
      "I0313 16:03:20.840975       1 alloc.go:330] \"allocated clusterIPs\" service=\"default/kubernetes\" clusterIPs={\"IPv4\":\"10.96.0.1\"}\n",
      "W0313 16:03:20.849331       1 lease.go:265] Resetting endpoints for master service \"kubernetes\" to [10.134.132.2]\n",
      "I0313 16:03:20.850368       1 controller.go:615] quota admission added evaluator for: endpoints\n",
      "I0313 16:03:20.853153       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io\n",
      "I0313 16:03:21.738648       1 controller.go:615] quota admission added evaluator for: serviceaccounts\n",
      "I0313 16:03:21.879452       1 controller.go:615] quota admission added evaluator for: deployments.apps\n",
      "I0313 16:03:21.914334       1 alloc.go:330] \"allocated clusterIPs\" service=\"kube-system/kube-dns\" clusterIPs={\"IPv4\":\"10.96.0.10\"}\n",
      "I0313 16:03:21.933068       1 controller.go:615] quota admission added evaluator for: daemonsets.apps\n",
      "I0313 16:03:23.907124       1 controller.go:615] quota admission added evaluator for: poddisruptionbudgets.policy\n",
      "I0313 16:03:23.958848       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:23.974650       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:23.991945       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.009490       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.021428       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.095935       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.137002       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.145477       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.151153       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.158433       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.163030       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.172923       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.180655       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.192368       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.219975       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.313468       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:27.283056       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps\n",
      "I0313 16:03:27.395761       1 controller.go:615] quota admission added evaluator for: replicasets.apps\n",
      "Logs from kube-apiserver (kube-apiserver-cpnode):\n",
      "I0313 16:03:19.525885       1 cache.go:39] Caches are synced for RemoteAvailability controller\n",
      "I0313 16:03:19.526489       1 handler_discovery.go:450] Starting ResourceDiscoveryManager\n",
      "I0313 16:03:19.526756       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller\n",
      "I0313 16:03:19.527160       1 cache.go:39] Caches are synced for LocalAvailability controller\n",
      "I0313 16:03:19.527315       1 apf_controller.go:382] Running API Priority and Fairness config worker\n",
      "I0313 16:03:19.527389       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process\n",
      "I0313 16:03:19.531100       1 shared_informer.go:320] Caches are synced for crd-autoregister\n",
      "I0313 16:03:19.531995       1 aggregator.go:171] initial CRD sync complete...\n",
      "I0313 16:03:19.532071       1 autoregister_controller.go:144] Starting autoregister controller\n",
      "I0313 16:03:19.532149       1 cache.go:32] Waiting for caches to sync for autoregister controller\n",
      "I0313 16:03:19.532220       1 cache.go:39] Caches are synced for autoregister controller\n",
      "I0313 16:03:19.535138       1 shared_informer.go:320] Caches are synced for node_authorizer\n",
      "I0313 16:03:19.557676       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]\n",
      "I0313 16:03:19.557689       1 policy_source.go:224] refreshing policies\n",
      "E0313 16:03:19.582105       1 controller.go:148] \"Unhandled Error\" err=\"while syncing ConfigMap \\\"kube-system/kube-apiserver-legacy-service-account-token-tracking\\\", err: namespaces \\\"kube-system\\\" not found\" logger=\"UnhandledError\"\n",
      "E0313 16:03:19.593931       1 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"namespaces \\\"kube-system\\\" not found\" interval=\"200ms\"\n",
      "I0313 16:03:19.632652       1 controller.go:615] quota admission added evaluator for: namespaces\n",
      "I0313 16:03:19.796216       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io\n",
      "I0313 16:03:20.430129       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000\n",
      "I0313 16:03:20.434952       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000\n",
      "I0313 16:03:20.435379       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.\n",
      "I0313 16:03:20.754073       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io\n",
      "I0313 16:03:20.783545       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io\n",
      "I0313 16:03:20.840975       1 alloc.go:330] \"allocated clusterIPs\" service=\"default/kubernetes\" clusterIPs={\"IPv4\":\"10.96.0.1\"}\n",
      "W0313 16:03:20.849331       1 lease.go:265] Resetting endpoints for master service \"kubernetes\" to [10.134.132.2]\n",
      "I0313 16:03:20.850368       1 controller.go:615] quota admission added evaluator for: endpoints\n",
      "I0313 16:03:20.853153       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io\n",
      "I0313 16:03:21.738648       1 controller.go:615] quota admission added evaluator for: serviceaccounts\n",
      "I0313 16:03:21.879452       1 controller.go:615] quota admission added evaluator for: deployments.apps\n",
      "I0313 16:03:21.914334       1 alloc.go:330] \"allocated clusterIPs\" service=\"kube-system/kube-dns\" clusterIPs={\"IPv4\":\"10.96.0.10\"}\n",
      "I0313 16:03:21.933068       1 controller.go:615] quota admission added evaluator for: daemonsets.apps\n",
      "I0313 16:03:23.907124       1 controller.go:615] quota admission added evaluator for: poddisruptionbudgets.policy\n",
      "I0313 16:03:23.958848       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:23.974650       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:23.991945       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.009490       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.021428       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.095935       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.137002       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.145477       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.151153       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.158433       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.163030       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.172923       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.180655       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.192368       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.219975       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:24.313468       1 handler.go:286] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager\n",
      "I0313 16:03:27.283056       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps\n",
      "I0313 16:03:27.395761       1 controller.go:615] quota admission added evaluator for: replicasets.apps\n",
      "\n",
      "\n",
      "Getting logs for kube-controller-manager...\n",
      "kube-controller-manager-cpnodeI0313 16:03:46.287620       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/coredns-7c65d6cfc9\" duration=\"143.709s\"\n",
      "I0313 16:03:46.902088       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/coredns-7c65d6cfc9\" duration=\"7.275106ms\"\n",
      "I0313 16:03:46.903011       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/coredns-7c65d6cfc9\" duration=\"33.924s\"\n",
      "I0313 16:03:50.220270       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/calico-kube-controllers-6879d4fcdc\" duration=\"40.806s\"\n",
      "I0313 16:03:50.280923       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/calico-kube-controllers-6879d4fcdc\" duration=\"11.351148ms\"\n",
      "I0313 16:03:50.282666       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/calico-kube-controllers-6879d4fcdc\" duration=\"1.716472ms\"\n",
      "I0313 16:03:53.581673       1 range_allocator.go:247] \"Successfully synced\" logger=\"node-ipam-controller\" key=\"cpnode\"\n",
      "E0313 16:04:48.049245       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:04:48.049289       1 actual_state_of_world.go:541] \"Failed to update statusUpdateNeeded field in actual state of world\" logger=\"persistentvolume-attach-detach-controller\" err=\"Failed to set statusUpdateNeeded to needed true, because nodeName=\\\"wknode1\\\" does not exist\"\n",
      "E0313 16:04:48.055304       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.065735       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.066138       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.085795       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.166030       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.178963       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.286772       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.326614       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:49.607193       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:04:51.544438       1 node_lifecycle_controller.go:884] \"Missing timestamp for Node. Assuming now as a timestamp\" logger=\"node-lifecycle-controller\" node=\"wknode1\"\n",
      "E0313 16:04:51.643533       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:52.167346       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:02.407460       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:19.279308       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:22.888138       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:05:29.418385       1 topologycache.go:237] \"Can't get CPU or zone information for node\" logger=\"endpointslice-controller\" node=\"wknode1\"\n",
      "E0313 16:05:29.418735       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:29.430496       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:31.556181       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:35.900674       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:35.924232       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:36.068789       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:40.987600       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:05:40.987632       1 actual_state_of_world.go:541] \"Failed to update statusUpdateNeeded field in actual state of world\" logger=\"persistentvolume-attach-detach-controller\" err=\"Failed to set statusUpdateNeeded to needed true, because nodeName=\\\"wknode2\\\" does not exist\"\n",
      "I0313 16:05:40.987953       1 topologycache.go:237] \"Can't get CPU or zone information for node\" logger=\"endpointslice-controller\" node=\"wknode1\"\n",
      "E0313 16:05:40.993720       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.004289       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.015881       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.024454       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.105301       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.117153       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.168073       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.265789       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:05:41.556239       1 node_lifecycle_controller.go:884] \"Missing timestamp for Node. Assuming now as a timestamp\" logger=\"node-lifecycle-controller\" node=\"wknode2\"\n",
      "E0313 16:05:41.585951       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:42.546110       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:47.666225       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:50.533217       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:51.470282       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:57.906515       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:06:12.111844       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "Logs from kube-controller-manager (kube-controller-manager-cpnode):\n",
      "I0313 16:03:46.287620       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/coredns-7c65d6cfc9\" duration=\"143.709s\"\n",
      "I0313 16:03:46.902088       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/coredns-7c65d6cfc9\" duration=\"7.275106ms\"\n",
      "I0313 16:03:46.903011       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/coredns-7c65d6cfc9\" duration=\"33.924s\"\n",
      "I0313 16:03:50.220270       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/calico-kube-controllers-6879d4fcdc\" duration=\"40.806s\"\n",
      "I0313 16:03:50.280923       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/calico-kube-controllers-6879d4fcdc\" duration=\"11.351148ms\"\n",
      "I0313 16:03:50.282666       1 replica_set.go:679] \"Finished syncing\" logger=\"replicaset-controller\" kind=\"ReplicaSet\" key=\"kube-system/calico-kube-controllers-6879d4fcdc\" duration=\"1.716472ms\"\n",
      "I0313 16:03:53.581673       1 range_allocator.go:247] \"Successfully synced\" logger=\"node-ipam-controller\" key=\"cpnode\"\n",
      "E0313 16:04:48.049245       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:04:48.049289       1 actual_state_of_world.go:541] \"Failed to update statusUpdateNeeded field in actual state of world\" logger=\"persistentvolume-attach-detach-controller\" err=\"Failed to set statusUpdateNeeded to needed true, because nodeName=\\\"wknode1\\\" does not exist\"\n",
      "E0313 16:04:48.055304       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.065735       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.066138       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.085795       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.166030       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.178963       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.286772       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:48.326614       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:49.607193       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:04:51.544438       1 node_lifecycle_controller.go:884] \"Missing timestamp for Node. Assuming now as a timestamp\" logger=\"node-lifecycle-controller\" node=\"wknode1\"\n",
      "E0313 16:04:51.643533       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:04:52.167346       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:02.407460       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:19.279308       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:22.888138       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:05:29.418385       1 topologycache.go:237] \"Can't get CPU or zone information for node\" logger=\"endpointslice-controller\" node=\"wknode1\"\n",
      "E0313 16:05:29.418735       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:29.430496       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:31.556181       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:35.900674       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:35.924232       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:36.068789       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:40.987600       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:05:40.987632       1 actual_state_of_world.go:541] \"Failed to update statusUpdateNeeded field in actual state of world\" logger=\"persistentvolume-attach-detach-controller\" err=\"Failed to set statusUpdateNeeded to needed true, because nodeName=\\\"wknode2\\\" does not exist\"\n",
      "I0313 16:05:40.987953       1 topologycache.go:237] \"Can't get CPU or zone information for node\" logger=\"endpointslice-controller\" node=\"wknode1\"\n",
      "E0313 16:05:40.993720       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.004289       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.015881       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.024454       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.105301       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.117153       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.168073       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:41.265789       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "I0313 16:05:41.556239       1 node_lifecycle_controller.go:884] \"Missing timestamp for Node. Assuming now as a timestamp\" logger=\"node-lifecycle-controller\" node=\"wknode2\"\n",
      "E0313 16:05:41.585951       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:42.546110       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:47.666225       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:50.533217       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode1': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:51.470282       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:05:57.906515       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "E0313 16:06:12.111844       1 range_allocator.go:252] \"Unhandled Error\" err=\"error syncing 'wknode2': failed to allocate cidr from cluster cidr at idx:0: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range, requeuing\" logger=\"UnhandledError\"\n",
      "\n",
      "\n",
      "Getting logs for kube-scheduler...\n",
      "kube-scheduler-cpnodeI0313 16:03:18.639393       1 serving.go:386] Generated self-signed cert in-memory\n",
      "W0313 16:03:19.740074       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'\n",
      "W0313 16:03:19.745714       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n",
      "W0313 16:03:19.745907       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.\n",
      "W0313 16:03:19.746020       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false\n",
      "I0313 16:03:19.767763       1 server.go:167] \"Starting Kubernetes Scheduler\" version=\"v1.31.7\"\n",
      "I0313 16:03:19.768043       1 server.go:169] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\n",
      "I0313 16:03:19.774086       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259\n",
      "I0313 16:03:19.775451       1 configmap_cafile_content.go:205] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\"\n",
      "I0313 16:03:19.775559       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\n",
      "I0313 16:03:19.775660       1 tlsconfig.go:243] \"Starting DynamicServingCertificateController\"\n",
      "W0313 16:03:19.778255       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.779364       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csidrivers\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.781257       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.781997       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"services\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.785984       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.786108       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"namespaces\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.786323       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.786426       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"nodes\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.786552       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope\n",
      "E0313 16:03:19.786655       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"replicasets\\\" in API group \\\"apps\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.786788       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.786894       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"persistentvolumeclaims\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787045       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.787166       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"storageclasses\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787330       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.787429       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csinodes\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787561       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.787656       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csistoragecapacities\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787885       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.788523       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"replicationcontrollers\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787914       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope\n",
      "E0313 16:03:19.788714       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"poddisruptionbudgets\\\" in API group \\\"policy\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787977       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope\n",
      "E0313 16:03:19.788909       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"statefulsets\\\" in API group \\\"apps\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.788059       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.789094       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"persistentvolumes\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.788323       1 reflector.go:561] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n",
      "E0313 16:03:19.789276       1 reflector.go:158] \"Unhandled Error\" err=\"runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \\\"extension-apiserver-authentication\\\" is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"configmaps\\\" in API group \\\"\\\" in the namespace \\\"kube-system\\\"\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.788396       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.789466       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:20.606206       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:20.606378       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csinodes\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:20.619714       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:20.619735       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csistoragecapacities\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:20.648869       1 reflector.go:561] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n",
      "E0313 16:03:20.648907       1 reflector.go:158] \"Unhandled Error\" err=\"runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \\\"extension-apiserver-authentication\\\" is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"configmaps\\\" in API group \\\"\\\" in the namespace \\\"kube-system\\\"\" logger=\"UnhandledError\"\n",
      "I0313 16:03:23.275678       1 leaderelection.go:254] attempting to acquire leader lease kube-system/kube-scheduler...\n",
      "I0313 16:03:23.280036       1 leaderelection.go:268] successfully acquired lease kube-system/kube-scheduler\n",
      "I0313 16:03:23.476197       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\n",
      "Logs from kube-scheduler (kube-scheduler-cpnode):\n",
      "I0313 16:03:18.639393       1 serving.go:386] Generated self-signed cert in-memory\n",
      "W0313 16:03:19.740074       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'\n",
      "W0313 16:03:19.745714       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n",
      "W0313 16:03:19.745907       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.\n",
      "W0313 16:03:19.746020       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false\n",
      "I0313 16:03:19.767763       1 server.go:167] \"Starting Kubernetes Scheduler\" version=\"v1.31.7\"\n",
      "I0313 16:03:19.768043       1 server.go:169] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\n",
      "I0313 16:03:19.774086       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259\n",
      "I0313 16:03:19.775451       1 configmap_cafile_content.go:205] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\"\n",
      "I0313 16:03:19.775559       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\n",
      "I0313 16:03:19.775660       1 tlsconfig.go:243] \"Starting DynamicServingCertificateController\"\n",
      "W0313 16:03:19.778255       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.779364       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csidrivers\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.781257       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.781997       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"services\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.785984       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.786108       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"namespaces\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.786323       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.786426       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"nodes\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.786552       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope\n",
      "E0313 16:03:19.786655       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"replicasets\\\" in API group \\\"apps\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.786788       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.786894       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"persistentvolumeclaims\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787045       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.787166       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"storageclasses\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787330       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.787429       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csinodes\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787561       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:19.787656       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csistoragecapacities\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787885       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.788523       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"replicationcontrollers\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787914       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope\n",
      "E0313 16:03:19.788714       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"poddisruptionbudgets\\\" in API group \\\"policy\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.787977       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope\n",
      "E0313 16:03:19.788909       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"statefulsets\\\" in API group \\\"apps\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.788059       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.789094       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"persistentvolumes\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.788323       1 reflector.go:561] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n",
      "E0313 16:03:19.789276       1 reflector.go:158] \"Unhandled Error\" err=\"runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \\\"extension-apiserver-authentication\\\" is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"configmaps\\\" in API group \\\"\\\" in the namespace \\\"kube-system\\\"\" logger=\"UnhandledError\"\n",
      "W0313 16:03:19.788396       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope\n",
      "E0313 16:03:19.789466       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:20.606206       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:20.606378       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csinodes\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:20.619714       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope\n",
      "E0313 16:03:20.619735       1 reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"csistoragecapacities\\\" in API group \\\"storage.k8s.io\\\" at the cluster scope\" logger=\"UnhandledError\"\n",
      "W0313 16:03:20.648869       1 reflector.go:561] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n",
      "E0313 16:03:20.648907       1 reflector.go:158] \"Unhandled Error\" err=\"runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \\\"extension-apiserver-authentication\\\" is forbidden: User \\\"system:kube-scheduler\\\" cannot list resource \\\"configmaps\\\" in API group \\\"\\\" in the namespace \\\"kube-system\\\"\" logger=\"UnhandledError\"\n",
      "I0313 16:03:23.275678       1 leaderelection.go:254] attempting to acquire leader lease kube-system/kube-scheduler...\n",
      "I0313 16:03:23.280036       1 leaderelection.go:268] successfully acquired lease kube-system/kube-scheduler\n",
      "I0313 16:03:23.476197       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\n",
      "\n",
      "\n",
      "Getting logs for etcd...\n",
      "etcd-cpnode{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.456230Z\",\"caller\":\"etcdserver/backend.go:81\",\"msg\":\"opened backend db\",\"path\":\"/var/lib/etcd/member/snap/db\",\"took\":\"2.401298ms\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.461655Z\",\"caller\":\"etcdserver/raft.go:495\",\"msg\":\"starting local member\",\"local-member-id\":\"5672bc4f43e9d656\",\"cluster-id\":\"3b12cef16026bdb2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465738Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 switched to configuration voters=()\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465784Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became follower at term 0\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465808Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"newRaft 5672bc4f43e9d656 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465825Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became follower at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465858Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 switched to configuration voters=(6229248283234653782)\"}\n",
      "{\"level\":\"warn\",\"ts\":\"2025-03-13T16:03:17.472214Z\",\"caller\":\"auth/store.go:1241\",\"msg\":\"simple token is not cryptographically signed\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.473253Z\",\"caller\":\"mvcc/kvstore.go:418\",\"msg\":\"kvstore restored\",\"current-rev\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.474100Z\",\"caller\":\"etcdserver/quota.go:94\",\"msg\":\"enabled backend quota with default value\",\"quota-name\":\"v3-applier\",\"quota-size-bytes\":2147483648,\"quota-size\":\"2.1 GB\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.475690Z\",\"caller\":\"etcdserver/server.go:867\",\"msg\":\"starting etcd server\",\"local-member-id\":\"5672bc4f43e9d656\",\"local-server-version\":\"3.5.15\",\"cluster-version\":\"to_be_decided\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.476345Z\",\"caller\":\"v3rpc/health.go:61\",\"msg\":\"grpc service status changed\",\"service\":\"\",\"status\":\"SERVING\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.484669Z\",\"caller\":\"embed/etcd.go:728\",\"msg\":\"starting with client TLS\",\"tls-info\":\"cert = /etc/kubernetes/pki/etcd/server.crt, key = /etc/kubernetes/pki/etcd/server.key, client-cert=, client-key=, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = true, crl-file = \",\"cipher-suites\":[]}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.488267Z\",\"caller\":\"embed/etcd.go:279\",\"msg\":\"now serving peer/client/metrics\",\"local-member-id\":\"5672bc4f43e9d656\",\"initial-advertise-peer-urls\":[\"https://10.134.132.2:2380\"],\"listen-peer-urls\":[\"https://10.134.132.2:2380\"],\"advertise-client-urls\":[\"https://10.134.132.2:2379\"],\"listen-client-urls\":[\"https://10.134.132.2:2379\",\"https://127.0.0.1:2379\"],\"listen-metrics-urls\":[\"http://127.0.0.1:2381\"]}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.488292Z\",\"caller\":\"embed/etcd.go:870\",\"msg\":\"serving metrics\",\"address\":\"http://127.0.0.1:2381\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.490016Z\",\"caller\":\"etcdserver/server.go:751\",\"msg\":\"started as single-node; fast-forwarding election ticks\",\"local-member-id\":\"5672bc4f43e9d656\",\"forward-ticks\":9,\"forward-duration\":\"900ms\",\"election-ticks\":10,\"election-timeout\":\"1s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493779Z\",\"caller\":\"fileutil/purge.go:50\",\"msg\":\"started to purge file\",\"dir\":\"/var/lib/etcd/member/snap\",\"suffix\":\"snap.db\",\"max\":5,\"interval\":\"30s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493822Z\",\"caller\":\"fileutil/purge.go:50\",\"msg\":\"started to purge file\",\"dir\":\"/var/lib/etcd/member/snap\",\"suffix\":\"snap\",\"max\":5,\"interval\":\"30s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493965Z\",\"caller\":\"embed/etcd.go:599\",\"msg\":\"serving peer traffic\",\"address\":\"10.134.132.2:2380\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493991Z\",\"caller\":\"embed/etcd.go:571\",\"msg\":\"cmux::serve\",\"address\":\"10.134.132.2:2380\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493829Z\",\"caller\":\"fileutil/purge.go:50\",\"msg\":\"started to purge file\",\"dir\":\"/var/lib/etcd/member/wal\",\"suffix\":\"wal\",\"max\":5,\"interval\":\"30s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.494589Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 switched to configuration voters=(6229248283234653782)\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.494899Z\",\"caller\":\"membership/cluster.go:421\",\"msg\":\"added member\",\"cluster-id\":\"3b12cef16026bdb2\",\"local-member-id\":\"5672bc4f43e9d656\",\"added-peer-id\":\"5672bc4f43e9d656\",\"added-peer-peer-urls\":[\"https://10.134.132.2:2380\"]}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969686Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 is starting a new election at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969730Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became pre-candidate at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969760Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 received MsgPreVoteResp from 5672bc4f43e9d656 at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969773Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became candidate at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969788Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 received MsgVoteResp from 5672bc4f43e9d656 at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969798Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became leader at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969811Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"raft.node: 5672bc4f43e9d656 elected leader 5672bc4f43e9d656 at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972323Z\",\"caller\":\"etcdserver/server.go:2118\",\"msg\":\"published local member to cluster through raft\",\"local-member-id\":\"5672bc4f43e9d656\",\"local-member-attributes\":\"{Name:cpnode ClientURLs:[https://10.134.132.2:2379]}\",\"request-path\":\"/0/members/5672bc4f43e9d656/attributes\",\"cluster-id\":\"3b12cef16026bdb2\",\"publish-timeout\":\"7s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972357Z\",\"caller\":\"embed/serve.go:103\",\"msg\":\"ready to serve client requests\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972383Z\",\"caller\":\"etcdserver/server.go:2629\",\"msg\":\"setting up initial cluster version using v2 API\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.977681Z\",\"caller\":\"etcdmain/main.go:44\",\"msg\":\"notifying init daemon\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.977756Z\",\"caller\":\"etcdmain/main.go:50\",\"msg\":\"successfully notified init daemon\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978234Z\",\"caller\":\"v3rpc/health.go:61\",\"msg\":\"grpc service status changed\",\"service\":\"\",\"status\":\"SERVING\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978310Z\",\"caller\":\"membership/cluster.go:584\",\"msg\":\"set initial cluster version\",\"cluster-id\":\"3b12cef16026bdb2\",\"local-member-id\":\"5672bc4f43e9d656\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978383Z\",\"caller\":\"api/capability.go:75\",\"msg\":\"enabled capabilities for version\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972413Z\",\"caller\":\"embed/serve.go:103\",\"msg\":\"ready to serve client requests\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978562Z\",\"caller\":\"etcdserver/server.go:2653\",\"msg\":\"cluster version is updated\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978996Z\",\"caller\":\"embed/serve.go:250\",\"msg\":\"serving client traffic securely\",\"traffic\":\"grpc+http\",\"address\":\"10.134.132.2:2379\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.979033Z\",\"caller\":\"v3rpc/health.go:61\",\"msg\":\"grpc service status changed\",\"service\":\"\",\"status\":\"SERVING\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.979699Z\",\"caller\":\"embed/serve.go:250\",\"msg\":\"serving client traffic securely\",\"traffic\":\"grpc+http\",\"address\":\"127.0.0.1:2379\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.276195Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[555311430] transaction\",\"detail\":\"{read_only:false; response_revision:560; number_of_response:1; }\",\"duration\":\"150.252138ms\",\"start\":\"2025-03-13T16:03:43.125934Z\",\"end\":\"2025-03-13T16:03:43.276186Z\",\"steps\":[\"trace[555311430] 'process raft request'  (duration: 150.143063ms)\"],\"step_count\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.276261Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[519077195] linearizableReadLoop\",\"detail\":\"{readStateIndex:574; appliedIndex:573; }\",\"duration\":\"137.147698ms\",\"start\":\"2025-03-13T16:03:43.139010Z\",\"end\":\"2025-03-13T16:03:43.276158Z\",\"steps\":[\"trace[519077195] 'read index received'  (duration: 137.036901ms)\",\"trace[519077195] 'applied index is now lower than readState.Index'  (duration: 110.346s)\"],\"step_count\":2}\n",
      "{\"level\":\"warn\",\"ts\":\"2025-03-13T16:03:43.276454Z\",\"caller\":\"etcdserver/util.go:170\",\"msg\":\"apply request took too long\",\"took\":\"137.430319ms\",\"expected-duration\":\"100ms\",\"prefix\":\"read-only range \",\"request\":\"key:\\\"/registry/pods/kube-system/calico-node-d8djr\\\" \",\"response\":\"range_response_count:1 size:14118\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.276596Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[839858332] range\",\"detail\":\"{range_begin:/registry/pods/kube-system/calico-node-d8djr; range_end:; response_count:1; response_revision:560; }\",\"duration\":\"137.579769ms\",\"start\":\"2025-03-13T16:03:43.139008Z\",\"end\":\"2025-03-13T16:03:43.276588Z\",\"steps\":[\"trace[839858332] 'agreement among raft nodes before linearized reading'  (duration: 137.266051ms)\"],\"step_count\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.428738Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[70406338] transaction\",\"detail\":\"{read_only:false; response_revision:561; number_of_response:1; }\",\"duration\":\"148.182624ms\",\"start\":\"2025-03-13T16:03:43.280539Z\",\"end\":\"2025-03-13T16:03:43.428721Z\",\"steps\":[\"trace[70406338] 'process raft request'  (duration: 104.151367ms)\",\"trace[70406338] 'compare'  (duration: 43.917372ms)\"],\"step_count\":2}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.460484Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[433349143] transaction\",\"detail\":\"{read_only:false; response_revision:562; number_of_response:1; }\",\"duration\":\"175.354942ms\",\"start\":\"2025-03-13T16:03:43.285119Z\",\"end\":\"2025-03-13T16:03:43.460474Z\",\"steps\":[\"trace[433349143] 'process raft request'  (duration: 174.176149ms)\"],\"step_count\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.460737Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[532973498] transaction\",\"detail\":\"{read_only:false; response_revision:563; number_of_response:1; }\",\"duration\":\"100.13143ms\",\"start\":\"2025-03-13T16:03:43.360599Z\",\"end\":\"2025-03-13T16:03:43.460731Z\",\"steps\":[\"trace[532973498] 'process raft request'  (duration: 98.871013ms)\"],\"step_count\":1}\n",
      "Logs from etcd (etcd-cpnode):\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.456230Z\",\"caller\":\"etcdserver/backend.go:81\",\"msg\":\"opened backend db\",\"path\":\"/var/lib/etcd/member/snap/db\",\"took\":\"2.401298ms\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.461655Z\",\"caller\":\"etcdserver/raft.go:495\",\"msg\":\"starting local member\",\"local-member-id\":\"5672bc4f43e9d656\",\"cluster-id\":\"3b12cef16026bdb2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465738Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 switched to configuration voters=()\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465784Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became follower at term 0\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465808Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"newRaft 5672bc4f43e9d656 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465825Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became follower at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.465858Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 switched to configuration voters=(6229248283234653782)\"}\n",
      "{\"level\":\"warn\",\"ts\":\"2025-03-13T16:03:17.472214Z\",\"caller\":\"auth/store.go:1241\",\"msg\":\"simple token is not cryptographically signed\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.473253Z\",\"caller\":\"mvcc/kvstore.go:418\",\"msg\":\"kvstore restored\",\"current-rev\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.474100Z\",\"caller\":\"etcdserver/quota.go:94\",\"msg\":\"enabled backend quota with default value\",\"quota-name\":\"v3-applier\",\"quota-size-bytes\":2147483648,\"quota-size\":\"2.1 GB\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.475690Z\",\"caller\":\"etcdserver/server.go:867\",\"msg\":\"starting etcd server\",\"local-member-id\":\"5672bc4f43e9d656\",\"local-server-version\":\"3.5.15\",\"cluster-version\":\"to_be_decided\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.476345Z\",\"caller\":\"v3rpc/health.go:61\",\"msg\":\"grpc service status changed\",\"service\":\"\",\"status\":\"SERVING\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.484669Z\",\"caller\":\"embed/etcd.go:728\",\"msg\":\"starting with client TLS\",\"tls-info\":\"cert = /etc/kubernetes/pki/etcd/server.crt, key = /etc/kubernetes/pki/etcd/server.key, client-cert=, client-key=, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = true, crl-file = \",\"cipher-suites\":[]}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.488267Z\",\"caller\":\"embed/etcd.go:279\",\"msg\":\"now serving peer/client/metrics\",\"local-member-id\":\"5672bc4f43e9d656\",\"initial-advertise-peer-urls\":[\"https://10.134.132.2:2380\"],\"listen-peer-urls\":[\"https://10.134.132.2:2380\"],\"advertise-client-urls\":[\"https://10.134.132.2:2379\"],\"listen-client-urls\":[\"https://10.134.132.2:2379\",\"https://127.0.0.1:2379\"],\"listen-metrics-urls\":[\"http://127.0.0.1:2381\"]}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.488292Z\",\"caller\":\"embed/etcd.go:870\",\"msg\":\"serving metrics\",\"address\":\"http://127.0.0.1:2381\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.490016Z\",\"caller\":\"etcdserver/server.go:751\",\"msg\":\"started as single-node; fast-forwarding election ticks\",\"local-member-id\":\"5672bc4f43e9d656\",\"forward-ticks\":9,\"forward-duration\":\"900ms\",\"election-ticks\":10,\"election-timeout\":\"1s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493779Z\",\"caller\":\"fileutil/purge.go:50\",\"msg\":\"started to purge file\",\"dir\":\"/var/lib/etcd/member/snap\",\"suffix\":\"snap.db\",\"max\":5,\"interval\":\"30s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493822Z\",\"caller\":\"fileutil/purge.go:50\",\"msg\":\"started to purge file\",\"dir\":\"/var/lib/etcd/member/snap\",\"suffix\":\"snap\",\"max\":5,\"interval\":\"30s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493965Z\",\"caller\":\"embed/etcd.go:599\",\"msg\":\"serving peer traffic\",\"address\":\"10.134.132.2:2380\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493991Z\",\"caller\":\"embed/etcd.go:571\",\"msg\":\"cmux::serve\",\"address\":\"10.134.132.2:2380\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.493829Z\",\"caller\":\"fileutil/purge.go:50\",\"msg\":\"started to purge file\",\"dir\":\"/var/lib/etcd/member/wal\",\"suffix\":\"wal\",\"max\":5,\"interval\":\"30s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.494589Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 switched to configuration voters=(6229248283234653782)\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.494899Z\",\"caller\":\"membership/cluster.go:421\",\"msg\":\"added member\",\"cluster-id\":\"3b12cef16026bdb2\",\"local-member-id\":\"5672bc4f43e9d656\",\"added-peer-id\":\"5672bc4f43e9d656\",\"added-peer-peer-urls\":[\"https://10.134.132.2:2380\"]}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969686Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 is starting a new election at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969730Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became pre-candidate at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969760Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 received MsgPreVoteResp from 5672bc4f43e9d656 at term 1\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969773Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became candidate at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969788Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 received MsgVoteResp from 5672bc4f43e9d656 at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969798Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"5672bc4f43e9d656 became leader at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.969811Z\",\"logger\":\"raft\",\"caller\":\"etcdserver/zap_raft.go:77\",\"msg\":\"raft.node: 5672bc4f43e9d656 elected leader 5672bc4f43e9d656 at term 2\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972323Z\",\"caller\":\"etcdserver/server.go:2118\",\"msg\":\"published local member to cluster through raft\",\"local-member-id\":\"5672bc4f43e9d656\",\"local-member-attributes\":\"{Name:cpnode ClientURLs:[https://10.134.132.2:2379]}\",\"request-path\":\"/0/members/5672bc4f43e9d656/attributes\",\"cluster-id\":\"3b12cef16026bdb2\",\"publish-timeout\":\"7s\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972357Z\",\"caller\":\"embed/serve.go:103\",\"msg\":\"ready to serve client requests\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972383Z\",\"caller\":\"etcdserver/server.go:2629\",\"msg\":\"setting up initial cluster version using v2 API\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.977681Z\",\"caller\":\"etcdmain/main.go:44\",\"msg\":\"notifying init daemon\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.977756Z\",\"caller\":\"etcdmain/main.go:50\",\"msg\":\"successfully notified init daemon\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978234Z\",\"caller\":\"v3rpc/health.go:61\",\"msg\":\"grpc service status changed\",\"service\":\"\",\"status\":\"SERVING\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978310Z\",\"caller\":\"membership/cluster.go:584\",\"msg\":\"set initial cluster version\",\"cluster-id\":\"3b12cef16026bdb2\",\"local-member-id\":\"5672bc4f43e9d656\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978383Z\",\"caller\":\"api/capability.go:75\",\"msg\":\"enabled capabilities for version\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.972413Z\",\"caller\":\"embed/serve.go:103\",\"msg\":\"ready to serve client requests\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978562Z\",\"caller\":\"etcdserver/server.go:2653\",\"msg\":\"cluster version is updated\",\"cluster-version\":\"3.5\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.978996Z\",\"caller\":\"embed/serve.go:250\",\"msg\":\"serving client traffic securely\",\"traffic\":\"grpc+http\",\"address\":\"10.134.132.2:2379\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.979033Z\",\"caller\":\"v3rpc/health.go:61\",\"msg\":\"grpc service status changed\",\"service\":\"\",\"status\":\"SERVING\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:17.979699Z\",\"caller\":\"embed/serve.go:250\",\"msg\":\"serving client traffic securely\",\"traffic\":\"grpc+http\",\"address\":\"127.0.0.1:2379\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.276195Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[555311430] transaction\",\"detail\":\"{read_only:false; response_revision:560; number_of_response:1; }\",\"duration\":\"150.252138ms\",\"start\":\"2025-03-13T16:03:43.125934Z\",\"end\":\"2025-03-13T16:03:43.276186Z\",\"steps\":[\"trace[555311430] 'process raft request'  (duration: 150.143063ms)\"],\"step_count\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.276261Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[519077195] linearizableReadLoop\",\"detail\":\"{readStateIndex:574; appliedIndex:573; }\",\"duration\":\"137.147698ms\",\"start\":\"2025-03-13T16:03:43.139010Z\",\"end\":\"2025-03-13T16:03:43.276158Z\",\"steps\":[\"trace[519077195] 'read index received'  (duration: 137.036901ms)\",\"trace[519077195] 'applied index is now lower than readState.Index'  (duration: 110.346s)\"],\"step_count\":2}\n",
      "{\"level\":\"warn\",\"ts\":\"2025-03-13T16:03:43.276454Z\",\"caller\":\"etcdserver/util.go:170\",\"msg\":\"apply request took too long\",\"took\":\"137.430319ms\",\"expected-duration\":\"100ms\",\"prefix\":\"read-only range \",\"request\":\"key:\\\"/registry/pods/kube-system/calico-node-d8djr\\\" \",\"response\":\"range_response_count:1 size:14118\"}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.276596Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[839858332] range\",\"detail\":\"{range_begin:/registry/pods/kube-system/calico-node-d8djr; range_end:; response_count:1; response_revision:560; }\",\"duration\":\"137.579769ms\",\"start\":\"2025-03-13T16:03:43.139008Z\",\"end\":\"2025-03-13T16:03:43.276588Z\",\"steps\":[\"trace[839858332] 'agreement among raft nodes before linearized reading'  (duration: 137.266051ms)\"],\"step_count\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.428738Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[70406338] transaction\",\"detail\":\"{read_only:false; response_revision:561; number_of_response:1; }\",\"duration\":\"148.182624ms\",\"start\":\"2025-03-13T16:03:43.280539Z\",\"end\":\"2025-03-13T16:03:43.428721Z\",\"steps\":[\"trace[70406338] 'process raft request'  (duration: 104.151367ms)\",\"trace[70406338] 'compare'  (duration: 43.917372ms)\"],\"step_count\":2}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.460484Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[433349143] transaction\",\"detail\":\"{read_only:false; response_revision:562; number_of_response:1; }\",\"duration\":\"175.354942ms\",\"start\":\"2025-03-13T16:03:43.285119Z\",\"end\":\"2025-03-13T16:03:43.460474Z\",\"steps\":[\"trace[433349143] 'process raft request'  (duration: 174.176149ms)\"],\"step_count\":1}\n",
      "{\"level\":\"info\",\"ts\":\"2025-03-13T16:03:43.460737Z\",\"caller\":\"traceutil/trace.go:171\",\"msg\":\"trace[532973498] transaction\",\"detail\":\"{read_only:false; response_revision:563; number_of_response:1; }\",\"duration\":\"100.13143ms\",\"start\":\"2025-03-13T16:03:43.360599Z\",\"end\":\"2025-03-13T16:03:43.460731Z\",\"steps\":[\"trace[532973498] 'process raft request'  (duration: 98.871013ms)\"],\"step_count\":1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get nodes\n",
    "    print(\"Getting nodes...\")\n",
    "    stdout, stderr = cpnode.execute(\"kubectl get nodes\")\n",
    "    print(f\"Nodes:\\n{stdout}\")\n",
    "    if stderr:\n",
    "        print(f\"Stderr: {stderr}\")\n",
    "\n",
    "    # Get all resources across namespaces\n",
    "    print(\"Getting all kubernetes resources...\")\n",
    "    stdout, stderr = cpnode.execute(\"kubectl get all --all-namespaces\")\n",
    "    print(f\"All resources:\\n{stdout}\")\n",
    "    if stderr:\n",
    "        print(f\"Stderr: {stderr}\")\n",
    "    \n",
    "    # Get all pods with more details\n",
    "    print(\"\\nGetting detailed pods status across all namespaces...\")\n",
    "    stdout, stderr = cpnode.execute(\"kubectl get pods -A -o wide\")\n",
    "    print(f\"Detailed pods status:\\n{stdout}\")\n",
    "    if stderr:\n",
    "        print(f\"Stderr: {stderr}\")\n",
    "        \n",
    "    # Describe kube-system namespace pods\n",
    "    print(\"\\nDescribing kube-system pods...\")\n",
    "    stdout, stderr = cpnode.execute(\"kubectl describe pods -n kube-system\")\n",
    "    print(f\"Kube-system pods details:\\n{stdout}\")\n",
    "    if stderr:\n",
    "        print(f\"Stderr: {stderr}\")\n",
    "    \n",
    "    # Get logs from specific system pods\n",
    "    system_components = ['kube-apiserver', 'kube-controller-manager', 'kube-scheduler', 'etcd']\n",
    "    \n",
    "    for component in system_components:\n",
    "        print(f\"\\nGetting logs for {component}...\")\n",
    "        try:\n",
    "            # First get the pod name\n",
    "            cmd = f\"kubectl get pods -n kube-system -l component={component} -o jsonpath='{{.items[0].metadata.name}}'\"\n",
    "            stdout, stderr = cpnode.execute(cmd)\n",
    "            if stdout:\n",
    "                pod_name = stdout.strip()\n",
    "                # Then get the logs\n",
    "                stdout, stderr = cpnode.execute(f\"kubectl logs -n kube-system {pod_name} --tail=50\")\n",
    "                print(f\"Logs from {component} ({pod_name}):\\n{stdout}\")\n",
    "            else:\n",
    "                print(f\"No pod found for component {component}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting logs for {component}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exception while monitoring kubernetes cluster: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node has successfully joined the cluster.\n",
    "\n",
    "# Deploying a hello world application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we pull a hello world image and create a \"deployment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/kubernetes-bootcamp created\n",
      "stdout: deployment.apps/kubernetes-bootcamp created\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stdout, stderr = cpnode.execute(\"kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --replicas=9\")\n",
    "    print(f\"stdout: {stdout}\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do some status commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\n",
      "default       kubernetes-bootcamp-68cfbdbb99-w2pk4       1/1     Running   0          14s\n",
      "kube-system   calico-kube-controllers-6879d4fcdc-rrp2b   1/1     Running   0          6m40s\n",
      "kube-system   calico-node-kjxnc                          1/1     Running   0          3m44s\n",
      "kube-system   calico-node-xrzxv                          1/1     Running   0          6m40s\n",
      "kube-system   coredns-7c65d6cfc9-lsqw4                   1/1     Running   0          6m40s\n",
      "kube-system   coredns-7c65d6cfc9-tqzzx                   1/1     Running   0          6m40s\n",
      "kube-system   etcd-node1                                 1/1     Running   0          6m47s\n",
      "kube-system   kube-apiserver-node1                       1/1     Running   0          6m47s\n",
      "kube-system   kube-controller-manager-node1              1/1     Running   0          6m47s\n",
      "kube-system   kube-proxy-472xw                           1/1     Running   0          3m44s\n",
      "kube-system   kube-proxy-f4ldr                           1/1     Running   0          6m40s\n",
      "kube-system   kube-scheduler-node1                       1/1     Running   0          6m47s\n",
      "stdout: NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\n",
      "default       kubernetes-bootcamp-68cfbdbb99-w2pk4       1/1     Running   0          14s\n",
      "kube-system   calico-kube-controllers-6879d4fcdc-rrp2b   1/1     Running   0          6m40s\n",
      "kube-system   calico-node-kjxnc                          1/1     Running   0          3m44s\n",
      "kube-system   calico-node-xrzxv                          1/1     Running   0          6m40s\n",
      "kube-system   coredns-7c65d6cfc9-lsqw4                   1/1     Running   0          6m40s\n",
      "kube-system   coredns-7c65d6cfc9-tqzzx                   1/1     Running   0          6m40s\n",
      "kube-system   etcd-node1                                 1/1     Running   0          6m47s\n",
      "kube-system   kube-apiserver-node1                       1/1     Running   0          6m47s\n",
      "kube-system   kube-controller-manager-node1              1/1     Running   0          6m47s\n",
      "kube-system   kube-proxy-472xw                           1/1     Running   0          3m44s\n",
      "kube-system   kube-proxy-f4ldr                           1/1     Running   0          6m40s\n",
      "kube-system   kube-scheduler-node1                       1/1     Running   0          6m47s\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _, stderr = cpnode.execute(\"kubectl get pods\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the correct pod name and run the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             kubernetes-bootcamp-68cfbdbb99-d67sb\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             node3/10.20.5.91\n",
      "Start Time:       Tue, 12 Nov 2024 03:47:12 +0000\n",
      "Labels:           app=kubernetes-bootcamp\n",
      "                  pod-template-hash=68cfbdbb99\n",
      "Annotations:      cni.projectcalico.org/containerID: 06ee87410189c92c674d921c8a11167abab926f81d6f74da7c115f909272d113\n",
      "                  cni.projectcalico.org/podIP: 10.146.2.113/32\n",
      "                  cni.projectcalico.org/podIPs: 10.146.2.113/32\n",
      "Status:           Running\n",
      "IP:               10.146.2.113\n",
      "IPs:\n",
      "  IP:           10.146.2.113\n",
      "Controlled By:  ReplicaSet/kubernetes-bootcamp-68cfbdbb99\n",
      "Containers:\n",
      "  kubernetes-bootcamp:\n",
      "    Container ID:   containerd://5c233800d4a748ec92341a274e4d2e158872d10d18e214ac97d67c12a27119f3\n",
      "    Image:          gcr.io/google-samples/kubernetes-bootcamp:v1\n",
      "    Image ID:       gcr.io/google-samples/kubernetes-bootcamp@sha256:0d6b8ee63bb57c5f5b6156f446b3bc3b3c143d233037f3a2f00e279c8fcc64af\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Tue, 12 Nov 2024 03:47:37 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qcrg4 (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-api-access-qcrg4:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason       Age   From               Message\n",
      "  ----     ------       ----  ----               -------\n",
      "  Normal   Scheduled    45s   default-scheduler  Successfully assigned default/kubernetes-bootcamp-68cfbdbb99-d67sb to node3\n",
      "  Warning  FailedMount  44s   kubelet            MountVolume.SetUp failed for volume \"kube-api-access-qcrg4\" : failed to sync configmap cache: timed out waiting for the condition\n",
      "  Normal   Pulling      43s   kubelet            Pulling image \"gcr.io/google-samples/kubernetes-bootcamp:v1\"\n",
      "  Normal   Pulled       20s   kubelet            Successfully pulled image \"gcr.io/google-samples/kubernetes-bootcamp:v1\" in 23.343s (23.343s including waiting). Image size: 83642968 bytes.\n",
      "  Normal   Created      20s   kubelet            Created container kubernetes-bootcamp\n",
      "  Normal   Started      20s   kubelet            Started container kubernetes-bootcamp\n",
      "stdout: Name:             kubernetes-bootcamp-68cfbdbb99-d67sb\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             node3/10.20.5.91\n",
      "Start Time:       Tue, 12 Nov 2024 03:47:12 +0000\n",
      "Labels:           app=kubernetes-bootcamp\n",
      "                  pod-template-hash=68cfbdbb99\n",
      "Annotations:      cni.projectcalico.org/containerID: 06ee87410189c92c674d921c8a11167abab926f81d6f74da7c115f909272d113\n",
      "                  cni.projectcalico.org/podIP: 10.146.2.113/32\n",
      "                  cni.projectcalico.org/podIPs: 10.146.2.113/32\n",
      "Status:           Running\n",
      "IP:               10.146.2.113\n",
      "IPs:\n",
      "  IP:           10.146.2.113\n",
      "Controlled By:  ReplicaSet/kubernetes-bootcamp-68cfbdbb99\n",
      "Containers:\n",
      "  kubernetes-bootcamp:\n",
      "    Container ID:   containerd://5c233800d4a748ec92341a274e4d2e158872d10d18e214ac97d67c12a27119f3\n",
      "    Image:          gcr.io/google-samples/kubernetes-bootcamp:v1\n",
      "    Image ID:       gcr.io/google-samples/kubernetes-bootcamp@sha256:0d6b8ee63bb57c5f5b6156f446b3bc3b3c143d233037f3a2f00e279c8fcc64af\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Tue, 12 Nov 2024 03:47:37 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qcrg4 (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  kube-api-access-qcrg4:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason       Age   From               Message\n",
      "  ----     ------       ----  ----               -------\n",
      "  Normal   Scheduled    45s   default-scheduler  Successfully assigned default/kubernetes-bootcamp-68cfbdbb99-d67sb to node3\n",
      "  Warning  FailedMount  44s   kubelet            MountVolume.SetUp failed for volume \"kube-api-access-qcrg4\" : failed to sync configmap cache: timed out waiting for the condition\n",
      "  Normal   Pulling      43s   kubelet            Pulling image \"gcr.io/google-samples/kubernetes-bootcamp:v1\"\n",
      "  Normal   Pulled       20s   kubelet            Successfully pulled image \"gcr.io/google-samples/kubernetes-bootcamp:v1\" in 23.343s (23.343s including waiting). Image size: 83642968 bytes.\n",
      "  Normal   Created      20s   kubelet            Created container kubernetes-bootcamp\n",
      "  Normal   Started      20s   kubelet            Started container kubernetes-bootcamp\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    _, stderr = cpnode.execute(\"kubectl describe pod kubernetes-bootcamp-68cfbdbb99-d67sb\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next thing we need to do is to create what is called a \"service\".\n",
    "\n",
    "We are going to use it to expose the deployment to the outside, through a port, which is 8080. Like this:\n",
    "\n",
    "Note that the service itself will still need to be exposed. There's another \"expose\" step that we need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/kubernetes-bootcamp exposed\n",
      "stdout: service/kubernetes-bootcamp exposed\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _, stderr = cpnode.execute('kubectl expose deployment/kubernetes-bootcamp --type=\"ClusterIP\" --port 8080')\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check if the service was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\n",
      "kubernetes-bootcamp   ClusterIP   10.111.180.18   <none>        8080/TCP   11s\n",
      "stdout: NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\n",
      "kubernetes-bootcamp   ClusterIP   10.111.180.18   <none>        8080/TCP   11s\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _, stderr = cpnode.execute(\"kubectl get service kubernetes-bootcamp\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we need to run a port forwarding command in order to expose the service to the outside.\n",
    "\n",
    "Modify the --address flag. Use the \"CLUSTER-IP\" that is output form the command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _, stderr = cpnode.execute(\"kubectl port-forward --address 10.111.180.18 service/kubernetes-bootcamp 8080:8080 > /dev/null 2>&1 &\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now our application should finally be visible. Let's test the deployment on the control plane machine itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-68cfbdbb99-bdrtr | v=1\n",
      "\u001b[31m   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    84    0    84    0     0  14000      0 --:--:-- --:--:-- --:--:-- 14000\n",
      " \u001b[0mstdout: Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-68cfbdbb99-bdrtr | v=1\n",
      "\n",
      "stderr:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    84    0    84    0     0  14000      0 --:--:-- --:--:-- --:--:-- 14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _, stderr = cpnode.execute(\"curl 10.111.180.18:8080\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2SAR Perf application in kubernetes test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the perf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/e2sar-perf created\n",
      "persistentvolume/logs-pv created\n",
      "persistentvolumeclaim/logs-pvc created\n",
      "service/e2sar-receiver-svc created\n",
      "networkpolicy.networking.k8s.io/e2sar-network-policy created\n",
      "job.batch/e2sar-receiver created\n",
      "job.batch/e2sar-sender created\n",
      "job.batch/e2sar-log-viewer created\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "# Run the perf test\n",
    "# upload the yaml file to the control plane\n",
    "file_attributes = cpnode.upload_file(local_file_path=\"/workspaces/local-fabric/FabricPortal_tests/e2sar-perf-test/e2sar-headless-fabric.yaml\", remote_file_path=\"e2sar-headless-fabric.yaml\")\n",
    "\n",
    "# Apply the yaml file\n",
    "try:\n",
    "    _, stderr = cpnode.execute(\"kubectl apply -f e2sar-headless-fabric.yaml\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the perf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender node: wknode1\n",
      "Receiver node: wknode2\n",
      "--- sender--- sender--- sender--- sender--- sender--- sender--- sender--- sender--- sender--- sender\n",
      "E2SAR Version: 0.1.4\n",
      "Control plane                OFF\n",
      "Event rate reporting in Sync ON\n",
      "Using usecs as event numbers ON\n",
      "*** Make sure the URI reflects proper data address, other parts are ignored.\n",
      "Sending bit rate is 0.1 Gbps\n",
      "Event size is 512 bytes or 4096 bits\n",
      "Event rate is 24414.1 Hz\n",
      "Inter-event sleep time is 40 microseconds\n",
      "Sending 500 event buffers\n",
      "Using MTU 512\n",
      "WARNING: Fewer frames than expected have been sent (978 of 1000), sender is not keeping up with the requested send rate.\n",
      "Completed, 978 frames sent, 0 errors\n",
      "Stopping threads\n",
      "Sender logs:\n",
      "\n",
      "--- receiver--- receiver--- receiver--- receiver--- receiver--- receiver--- receiver--- receiver--- receiver--- receiver\n",
      "E2SAR Version: 0.1.4\n",
      "Control plane will be OFF\n",
      "Using Unassigned Threads\n",
      "Will run for 300 sec\n",
      "*** Make sure the URI reflects proper data address, other parts are ignored.\n",
      "Receiving on ports 19522:19522\n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 0\n",
      "\tEvents Mangled: 0\n",
      "\tEvents Lost: 0\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 5\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Stats:\n",
      "\tEvents Received: 85\n",
      "\tEvents Mangled: 1\n",
      "\tEvents Lost: 6\n",
      "\tData Errors: 0\n",
      "\tgRPC Errors: 0\n",
      "\tEvents lost so far: <0:4321> <52:4321> <41:4321> <54:4321> <100:4321> <497:4321> \n",
      "Completed\n",
      "Stopping threads\n",
      "Deregistering worker\n",
      "Receiver logs:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To monitor sender logs directly on the sender node\n",
    "# Define sender and receiver nodes from the slice\n",
    "try:\n",
    "    # Assuming we have nodes with specific roles or labels\n",
    "    # You might need to adjust this based on your actual node naming/labeling\n",
    "    sender_node = slice.get_node(name=\"wknode1\")  # or any specific node name for sender\n",
    "    receiver_node = slice.get_node(name=\"wknode2\")  # or any specific node name for receiver\n",
    "    \n",
    "    # If nodes don't have specific names, you could use indices\n",
    "    # sender_node = slice.get_nodes()[1]  # First worker node\n",
    "    # receiver_node = slice.get_nodes()[2]  # Second worker node\n",
    "    \n",
    "    print(f\"Sender node: {sender_node.get_name()}\")\n",
    "    print(f\"Receiver node: {receiver_node.get_name()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error defining sender/receiver nodes: {e}\")\n",
    "\n",
    "print(\"--- sender\"*10)\n",
    "\n",
    "try:\n",
    "    _, stderr = sender_node.execute(\"cat /tmp/e2sar-logs/sender/sender.log\")\n",
    "    print(f\"Sender logs:\")\n",
    "    print(stderr)\n",
    "except Exception as e:\n",
    "    print(f\"Exception when accessing sender logs: {e}\")\n",
    "\n",
    "print(\"--- receiver\"*10)\n",
    "\n",
    "# To monitor receiver logs directly on the receiver node\n",
    "try:\n",
    "    _, stderr = receiver_node.execute(\"cat /tmp/e2sar-logs/receiver/receiver.log\")\n",
    "    print(f\"Receiver logs:\")\n",
    "    print(stderr)\n",
    "except Exception as e:\n",
    "    print(f\"Exception when accessing receiver logs: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the perf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace \"e2sar-perf\" deleted\n",
      "persistentvolume \"logs-pv\" deleted\n",
      "persistentvolumeclaim \"logs-pvc\" deleted\n",
      "service \"e2sar-receiver-svc\" deleted\n",
      "networkpolicy.networking.k8s.io \"e2sar-network-policy\" deleted\n",
      "job.batch \"e2sar-receiver\" deleted\n",
      "job.batch \"e2sar-sender\" deleted\n",
      "job.batch \"e2sar-log-viewer\" deleted\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "# Clean up the perf test\n",
    "try:\n",
    "    _, stderr = cpnode.execute(\"kubectl delete -f e2sar-headless-fabric.yaml\")\n",
    "    print(f\"stderr: {stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue: communication between cp and kubelet port on workers.\n",
    "The control plane node cannot communicate with kubelet on port 10250 on worker nodes, despite being able to ping the worker node IP addresses. This prevents certain kubectl commands from functioning properly, particularly those that require direct kubelet communication, such as: \"kubectl exec -it <pod-running-on-worker-node>\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Delete Slice\n",
    "\n",
    "Please delete your slicd when you are done with your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    slice = fablib.get_slice(slice_name)\n",
    "    slice.delete()\n",
    "except Exception as e:\n",
    "    print(f\"Fail: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
